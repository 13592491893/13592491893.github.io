{"meta":{"title":"Hy","subtitle":"Hyの博客","description":"博客因兴趣诞生,在不断完善中","author":"Hy","url":"https://13592491893.github.io","root":"/"},"pages":[{"title":"关于本站","date":"2020-04-19T12:58:56.000Z","updated":"2021-12-29T03:19:25.627Z","comments":false,"path":"about/index.html","permalink":"https://13592491893.github.io/about/index.html","excerpt":"","text":"Hy“ 认识自我、超越自我 “ 那么你离成功就又进了一步！ 当你在项目中感觉所要学习的人和事越来越多时，说明你在 成长 。 当你感觉要责怪的人和事越来越少时，说明你在 成熟 。 当你在项目中不断获得了友谊和朋友时，说明你将取得项目的 成功 。 联系我QQ：459173919（嘀嘀~，麻烦让一下，我要开车了！） 座右铭：既然选择了远方 便只顾风雨兼程"},{"title":"archives","date":"2019-10-25T00:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"archives/index.html","permalink":"https://13592491893.github.io/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-11-24T15:12:19.000Z","updated":"2021-12-29T03:19:25.627Z","comments":false,"path":"categories/index.html","permalink":"https://13592491893.github.io/categories/index.html","excerpt":"","text":""},{"title":"留言板","date":"2020-10-31T10:11:28.000Z","updated":"2021-12-29T03:19:25.627Z","comments":false,"path":"comments/index.html","permalink":"https://13592491893.github.io/comments/index.html","excerpt":"","text":"new Artitalk({ appId: 'o2lydQokojD1IP8EGhex24WU-MdYXbMMI', appKey: '8ykSyPm9CwzCDdR5dbid1AWT' }) ## 1.ttt 2.sss"},{"title":"友情链接","date":"2018-06-07T22:17:49.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"link/index.html","permalink":"https://13592491893.github.io/link/index.html","excerpt":"","text":""},{"title":"我的歌单","date":"2019-05-17T16:14:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"music/index.html","permalink":"https://13592491893.github.io/music/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-11-24T15:14:39.000Z","updated":"2021-12-29T03:19:25.627Z","comments":false,"path":"tags/index.html","permalink":"https://13592491893.github.io/tags/index.html","excerpt":"","text":""},{"title":"相册","date":"2021-12-29T03:19:25.591Z","updated":"2021-12-29T03:19:25.591Z","comments":false,"path":"List/gallery/index.html","permalink":"https://13592491893.github.io/List/gallery/index.html","excerpt":"","text":"壁纸 收藏的一些壁纸 OH MY GIRL 关于OH MY GIRL的图片"},{"title":"","date":"2019-08-10T16:41:10.000Z","updated":"2021-12-29T03:19:25.591Z","comments":false,"path":"List/movies/index.html","permalink":"https://13592491893.github.io/List/movies/index.html","excerpt":"","text":"励志视频"},{"title":"Music-BBOX","date":"2020-04-23T12:58:56.000Z","updated":"2021-12-29T03:19:25.591Z","comments":false,"path":"List/music/index.html","permalink":"https://13592491893.github.io/List/music/index.html","excerpt":"","text":""},{"title":"","date":"2020-11-24T16:05:01.000Z","updated":"2021-12-29T03:19:25.591Z","comments":false,"path":"List/gallery/ohmygirl/index.html","permalink":"https://13592491893.github.io/List/gallery/ohmygirl/index.html","excerpt":"","text":""},{"title":"","date":"2021-12-29T03:19:25.591Z","updated":"2021-12-29T03:19:25.591Z","comments":false,"path":"List/gallery/wallpaper/index.html","permalink":"https://13592491893.github.io/List/gallery/wallpaper/index.html","excerpt":"","text":""}],"posts":[{"title":"MySQL优化","slug":"Mysql优化","date":"2021-12-28T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/2c9a5c07.html","link":"","permalink":"https://13592491893.github.io/posts/2c9a5c07.html","excerpt":"","text":"1.MySQL环境1.1.环境安装# 查看Linux服务器上是否安装过MySQL rpm -qa | grep -i mysql # 查询出所有mysql依赖包 # 1、拉取镜像 docker pull mysql:5.7 # 2、创建实例并启动 docker run -p 3306:3306 --name mysql \\ -v /root/mysql/log:/var/log/mysql \\ -v /root/mysql/data:/var/lib/mysql \\ -v /root/mysql/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=333 \\ -d mysql:5.7 # 3、mysql配置 /root/mysql/conf/my.conf [client] #mysqlde utf8字符集默认为3位的，不支持emoji表情及部分不常见的汉字，故推荐使用utf8mb4 default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] #设置client连接mysql时的字符集,防止乱码 init_connect='SET collation_connection = utf8_general_ci' init_connect='SET NAMES utf8' #数据库默认字符集 character-set-server=utf8 #数据库字符集对应一些排序等规则，注意要和character-set-server对应 collation-server=utf8_general_ci # 跳过mysql程序起动时的字符参数设置 ，使用服务器端字符集设置 skip-character-set-client-handshake # 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ skip-name-resolve # 4、重启mysql容器 docker restart mysql # 5、进入到mysql容器 docker exec -it mysql /bin/bash # 6、查看修改的配置文件 cat /etc/mysql/my.conf 1.2.安装位置Docker容器就是一个小型的Linux环境，进入到MySQL容器中。 docker exec -it mysql /bin/bash Linux环境下MySQL的安装目录。 路径 解释 /var/lib/mysql MySQL数据库文件存放位置 /usr/share/mysql 错误消息和字符集文件配置 /usr/bin 客户端程序和脚本 /etc/init.d/mysql 启停脚本相关 1.3.修改字符集# 1、进入到mysql数据库并查看字符集 # show variables like 'character%'; # show variables like '%char%'; mysql&gt; show variables like 'character%'; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+ 8 rows in set (0.00 sec) mysql&gt; show variables like '%char%'; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+ 8 rows in set (0.01 sec) MySQL5.7配置文件位置是/etc/my.cnf或者/etc/mysql/my.cnf，如果字符集不是utf-8直接进入配置文件修改即可。 [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] # 设置client连接mysql时的字符集,防止乱码 init_connect='SET NAMES utf8' init_connect='SET collation_connection = utf8_general_ci' # 数据库默认字符集 character-set-server=utf8 #数据库字符集对应一些排序等规则，注意要和character-set-server对应 collation-server=utf8_general_ci # 跳过mysql程序起动时的字符参数设置 ，使用服务器端字符集设置 skip-character-set-client-handshake # 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ skip-name-resolve 注意：安装MySQL完毕之后，第一件事就是修改字符集编码。 1.4.配置文件MySQL配置文件讲解：https://www.cnblogs.com/gaoyuechen/p/10273102.html 1、二进制日志log-bin：主从复制。 # my,cnf # 开启mysql binlog功能 log-bin=mysql-bin 2、错误日志log-error：默认是关闭的，记录严重的警告和错误信息，每次启动和关闭的详细信息等。 # my,cnf # 数据库错误日志文件 log-error = error.log 3、查询日志log：默认关闭，记录查询的sql语句，如果开启会降低MySQL整体的性能，因为记录日志需要消耗系统资源。 # my,cnf # 慢查询sql日志设置 slow_query_log = 1 slow_query_log_file = slow.log 4、数据文件。 frm文件：存放表结构。 myd文件：存放表数据。 myi文件：存放表索引。 # mysql5.7 使用.frm文件来存储表结构 # 使用 .ibd文件来存储表索引和表数据 -rw-r----- 1 mysql mysql 8988 Jun 25 09:31 pms_category.frm -rw-r----- 1 mysql mysql 245760 Jul 21 10:01 pms_category.ibd MySQL5.7的Innodb存储引擎可将所有数据存放于ibdata*的共享表空间，也可将每张表存放于独立的.ibd文件的独立表空间。共享表空间以及独立表空间都是针对数据的存储方式而言的。 共享表空间: 某一个数据库的所有的表数据，索引文件全部放在一个文件中，默认这个共享表空间的文件路径在data目录下。 默认的文件名为:ibdata1 初始化为10M。 独立表空间: 每一个表都将会生成以独立的文件方式来进行存储，每一个表都有一个.frm表描述文件，还有一个.ibd文件。 其中这个文件包括了单独一个表的数据内容以及索引内容，默认情况下它的存储位置也是在表的位置之中。在配置文件my.cnf中设置： innodb_file_per_table。 2.MySQL逻辑架构 Connectors：指的是不同语言中与SQL的交互。 Connection Pool：管理缓冲用户连接，线程处理等需要缓存的需求。MySQL数据库的连接层。 Management Serveices &amp; Utilities：系统管理和控制工具。备份、安全、复制、集群等等。。 SQL Interface：接受用户的SQL命令，并且返回用户需要查询的结果。 Parser：SQL语句解析器。 Optimizer：查询优化器，SQL语句在查询之前会使用查询优化器对查询进行优化。就是优化客户端请求query，根据客户端请求的 query 语句，和数据库中的一些统计信息，在一系列算法的基础上进行分析，得出一个最优的策略，告诉后面的程序如何取得这个 query 语句的结果。For Example： select uid,name from user where gender = 1;这个select 查询先根据where 语句进行选取，而不是先将表全部查询出来以后再进行gender过滤；然后根据uid和name进行属性投影，而不是将属性全部取出以后再进行过滤。最后将这两个查询条件联接起来生成最终查询结果。 Caches &amp; Buffers：查询缓存。 Pluggable Storage Engines：存储引擎接口。MySQL区别于其他数据库的最重要的特点就是其插件式的表存储引擎(注意：存储引擎是基于表的，而不是数据库)。 File System：数据落地到磁盘上，就是文件的存储。 MySQL数据库和其他数据库相比，MySQL有点与众不同，主要体现在存储引擎的架构上，插件式的存储引擎架构将查询处理和其他的系统任务以及数据的存储提取相分离。这种架构可以根据业务的需求和实际需求选择合适的存储引擎。 逻辑架构分层 连接层：最上层是一些客户端和连接服务，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcp/ip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 服务层：MySQL的核心服务功能层，该层是MySQL的核心，包括查询缓存，解析器，解析树，预处理器，查询优化器。主要进行查询解析、分析、查询缓存、内置函数、存储过程、触发器、视图等，select操作会先检查是否命中查询缓存，命中则直接返回缓存数据，否则解析查询并创建对应的解析树。 引擎层：存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。 存储层：数据存储层，主要是将数据存储在运行于裸设备的文件系统之上，并完成与存储引擎的交互。 3.存储引擎show engines;命令查看MySQL5.7支持的存储引擎。 mysql&gt; show engines; show variables like 'default_storage_engine%';查看当前数据库正在使用的存储引擎。 mysql&gt; show variables like 'default_storage_engine%'; +------------------------+--------+ | Variable_name | Value | +------------------------+--------+ | default_storage_engine | InnoDB | +------------------------+--------+ 1 row in set (0.01 sec) InnoDB和MyISAM对比 对比项 MyISAM InnoDB 主外键 不支持 支持 事务 不支持 支持 行表锁 表锁，即使操作一条记录也会锁住整张表，不适合高并发操作 行锁，操作时只锁某一行，不对其他行有影响，适合高并发操作 缓存 只缓存索引，不缓存真实数据 不仅缓存索引还要缓存真实数据，対内存要求较高，而且内存大小対性能有决定性影响 表空间 小 大 关注点 性能 事务 默认安装 Y Y 4.SQL性能下降的原因 查询语句写的差。 索引失效：索引建了，但是没有用上。 关联 查询太多join（设计缺陷或者不得已的需求）。 服务器调优以及各个参数的设置（缓冲、线程数等）。 5.SQL执行顺序select # 5 ... from # 1 ... where # 2 .... group by # 3 ... having # 4 ... order by # 6 ... limit # 7 [offset] 6.七种JOIN理论 /* 1 */ SELECT &lt;select_list&gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key; /* 2 */ SELECT &lt;select_list&gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key; /* 3 */ SELECT &lt;select_list&gt; FROM TableA A INNER JOIN TableB B ON A.Key = B.Key; /* 4 */ SELECT &lt;select_list&gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key WHERE B.Key IS NULL; /* 5 */ SELECT &lt;select_list&gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key WHERE A.Key IS NULL; /* 6 */ SELECT &lt;select_list&gt; FROM TableA A FULL OUTER JOIN TableB B ON A.Key = B.Key; /* MySQL不支持FULL OUTER JOIN这种语法 可以改成 1+2 */ SELECT &lt;select_list&gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key UNION SELECT &lt;select_list&gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key; /* 7 */ SELECT &lt;select_list&gt; FROM TableA A FULL OUTER JOIN TableB B ON A.Key = B.Key WHERE A.Key IS NULL OR B.Key IS NULL; /* MySQL不支持FULL OUTER JOIN这种语法 可以改成 4+5 */ SELECT &lt;select_list&gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key WHERE B.Key IS NULL; UNION SELECT &lt;select_list&gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key WHERE A.Key IS NULL; 7.索引7.1.索引简介 索引是什么？ MySQL官方对索引的定义为：索引（INDEX）是帮助MySQL高效获取数据的数据结果。 从而可以获得索引的本质：索引是排好序的快速查找数据结构。 索引的目的在于提高查询效率，可以类比字典的目录。如果要查mysql这个这个单词，我们肯定要先定位到m字母，然后从上往下找y字母，再找剩下的sql。如果没有索引，那么可能需要a---z，这样全字典扫描，如果我想找Java开头的单词呢？如果我想找Oracle开头的单词呢？？？ 重点：索引会影响到MySQL查找(WHERE的查询条件)和排序(ORDER BY)两大功能！ 除了数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这些数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。 # Linux下查看磁盘空间命令 df -h [root@Ringo ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/vda1 40G 16G 23G 41% / devtmpfs 911M 0 911M 0% /dev tmpfs 920M 0 920M 0% /dev/shm tmpfs 920M 480K 920M 1% /run tmpfs 920M 0 920M 0% /sys/fs/cgroup overlay 40G 16G 23G 41% 我们平时所说的索引，如果没有特别指明，都是指B树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引，次要索引，覆盖索引，复合索引，前缀索引，唯一索引默认都是使用B+树索引，统称索引。当然，除了B+树这种数据结构的索引之外，还有哈希索引（Hash Index）等。 索引的优势和劣势 优势： 查找：类似大学图书馆的书目索引，提高数据检索的效率，降低数据库的IO成本。 排序：通过索引対数据进行排序，降低数据排序的成本，降低了CPU的消耗。 劣势： 实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的。 虽然索引大大提高了查询速度，但是同时会降低表的更新速度，例如对表频繁的进行INSERT、UPDATE和DELETE。因为更新表的时候，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加的索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。 索引只是提高效率的一个因素，如果MySQL有大数据量的表，就需要花时间研究建立最优秀的索引。 7.2.MySQL索引分类索引分类： 单值索引：一个索引只包含单个列，一个表可以有多个单列索引。 唯一索引：索引列的值必须唯一，但是允许空值。 复合索引：一个索引包含多个字段。 建议：一张表建的索引最好不要超过5个！ /* 基本语法 */ /* 1、创建索引 [UNIQUE]可以省略*/ /* 如果只写一个字段就是单值索引，写多个字段就是复合索引 */ CREATE [UNIQUE] INDEX indexName ON tabName(columnName(length)); /* 2、删除索引 */ DROP INDEX [indexName] ON tabName; /* 3、查看索引 */ /* 加上\\G就可以以列的形式查看了 不加\\G就是以表的形式查看 */ SHOW INDEX FROM tabName \\G; 使用ALTER命令来为数据表添加索引 /* 1、该语句添加一个主键，这意味着索引值必须是唯一的，并且不能为NULL */ ALTER TABLE tabName ADD PRIMARY KEY(column_list); /* 2、该语句创建索引的键值必须是唯一的(除了NULL之外，NULL可能会出现多次) */ ALTER TABLE tabName ADD UNIQUE indexName(column_list); /* 3、该语句创建普通索引，索引值可以出现多次 */ ALTER TABLE tabName ADD INDEX indexName(column_list); /* 4、该语句指定了索引为FULLTEXT，用于全文检索 */ ALTER TABLE tabName ADD FULLTEXT indexName(column_list); 7.3MySQL索引数据结构索引数据结构： BTree索引。 Hash索引。 Full-text全文索引。 R-Tree索引。 BTree索引检索原理： 7.4.哪些情况需要建索引 主键自动建立主键索引（唯一 + 非空）。 频繁作为查询条件的字段应该创建索引。 查询中与其他表关联的字段，外键关系建立索引。 查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度。 查询中统计或者分组字段（group by也和索引有关）。 7.5.那些情况不要建索引 记录太少的表。 经常增删改的表。 频繁更新的字段不适合创建索引。 Where条件里用不到的字段不创建索引。 假如一个表有10万行记录，有一个字段A只有true和false两种值，并且每个值的分布概率大约为50%，那么对A字段建索引一般不会提高数据库的查询速度。索引的选择性是指索引列中不同值的数目与表中记录数的比。如果一个表中有2000条记录，表索引列有1980个不同的值，那么这个索引的选择性就是1980/2000=0.99。一个索引的选择性越接近于1，这个索引的效率就越高。 8.性能分析8.1.EXPLAIN简介 EXPLAIN是什么？ EXPLAIN：SQL的执行计划，使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理SQL语句的。 EXPLAIN怎么使用？ 语法：explain + SQL。 mysql&gt; explain select * from pms_category \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1425 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) EXPLAIN能干嘛？ 可以查看以下信息： id：表的读取顺序。 select_type：数据读取操作的操作类型。 possible_keys：哪些索引可以使用。 key：哪些索引被实际使用。 ref：表之间的引用。 rows：每张表有多少行被优化器查询。 8.2.EXPLAIN字段 id id：表的读取和加载顺序。 值有以下三种情况： id相同，执行顺序由上至下。 id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行。 id相同不同，同时存在。永远是id大的优先级最高，id相等的时候顺序执行。 select_type select_type：数据查询的类型，主要是用于区别，普通查询、联合查询、子查询等的复杂查询。 SIMPLE：简单的SELECT查询，查询中不包含子查询或者UNION 。 PRIMARY：查询中如果包含任何复杂的子部分，最外层查询则被标记为PRIMARY。 SUBQUERY：在SELECT或者WHERE子句中包含了子查询。 DERIVED：在FROM子句中包含的子查询被标记为DERIVED(衍生)，MySQL会递归执行这些子查询，把结果放在临时表中。 UNION：如果第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM子句的子查询中，外层SELECT将被标记为DERIVED。 UNION RESULT：从UNION表获取结果的SELECT。 type type：访问类型排列。 从最好到最差依次是：system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;ALL。除了ALL没有用到索引，其他级别都用到索引了。 一般来说，得保证查询至少达到range级别，最好达到ref。 system：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现，这个也可以忽略不计。 const：表示通过索引一次就找到了，const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL就能将该查询转化为一个常量。 eq_ref：唯一性索引扫描，读取本表中和关联表表中的每行组合成的一行，查出来只有一条记录。除 了 system 和 const 类型之外, 这是最好的联接类型。 ref：非唯一性索引扫描，返回本表和关联表某个值匹配的所有行，查出来有多条记录。 range：只检索给定范围的行，一般就是在WHERE语句中出现了BETWEEN、&lt; &gt;、in等的查询。这种范围扫描索引比全表扫描要好，因为它只需要开始于索引树的某一点，而结束于另一点，不用扫描全部索引。 index：Full Index Scan，全索引扫描，index和ALL的区别为index类型只遍历索引树。也就是说虽然ALL和index都是读全表，但是index是从索引中读的，ALL是从磁盘中读取的。 ALL：Full Table Scan，没有用到索引，全表扫描。 possible_keys 和 key possible_keys：显示可能应用在这张表中的索引，一个或者多个。查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。 key：实际使用的索引。如果为NULL，则没有使用索引。查询中如果使用了覆盖索引，则该索引仅仅出现在key列表中。 key_len key_len：表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。在不损失精度的情况下，长度越短越好。 key_len计算规则：https://blog.csdn.net/qq_34930488/article/details/102931490 mysql&gt; desc pms_category; +---------------+------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +---------------+------------+------+-----+---------+----------------+ | cat_id | bigint(20) | NO | PRI | NULL | auto_increment | | name | char(50) | YES | | NULL | | | parent_cid | bigint(20) | YES | | NULL | | | cat_level | int(11) | YES | | NULL | | | show_status | tinyint(4) | YES | | NULL | | | sort | int(11) | YES | | NULL | | | icon | char(255) | YES | | NULL | | | product_unit | char(50) | YES | | NULL | | | product_count | int(11) | YES | | NULL | | +---------------+------------+------+-----+---------+----------------+ 9 rows in set (0.00 sec) mysql&gt; explain select cat_id from pms_category where cat_id between 10 and 20 \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: range possible_keys: PRIMARY key: PRIMARY # 用到了主键索引，通过查看表结构知道，cat_id是bigint类型，占用8个字节 key_len: 8 # 这里只用到了cat_id主键索引，所以长度就是8！ ref: NULL rows: 11 filtered: 100.00 Extra: Using where; Using index 1 row in set, 1 warning (0.00 sec) ref ref：显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值。 rows rows：根据表统计信息及索引选用情况，大致估算出找到所需的记录需要读取的行数。 Extra Extra：包含不适合在其他列中显示但十分重要的额外信息。 Using filesort：说明MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成的排序操作成为”文件内排序”。 # 排序没有使用索引 mysql&gt; explain select name from pms_category where name='Tangs' order by cat_level \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: ref possible_keys: idx_name_parentCid_catLevel key: idx_name_parentCid_catLevel key_len: 201 ref: const rows: 1 filtered: 100.00 Extra: Using where; Using index; Using filesort 1 row in set, 1 warning (0.00 sec) #~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # 排序使用到了索引 mysql&gt; explain select name from pms_category where name='Tangs' order by parent_cid,cat_level\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: ref possible_keys: idx_name_parentCid_catLevel key: idx_name_parentCid_catLevel key_len: 201 ref: const rows: 1 filtered: 100.00 Extra: Using where; Using index 1 row in set, 1 warning (0.00 sec) Using temporary：使用了临时表保存中间结果，MySQL在対查询结果排序时使用了临时表。常见于排序order by和分组查询group by。临时表対系统性能损耗很大。 Using index：表示相应的SELECT操作中使用了覆盖索引，避免访问了表的数据行，效率不错！如果同时出现Using where，表示索引被用来执行索引键值的查找；如果没有同时出现Using where，表明索引用来读取数据而非执行查找动作。 # 覆盖索引 # 就是select的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。 # 注意：如果要使用覆盖索引，一定不能写SELECT *，要写出具体的字段。 mysql&gt; explain select cat_id from pms_category \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: index possible_keys: NULL key: PRIMARY key_len: 8 ref: NULL rows: 1425 filtered: 100.00 Extra: Using index # select的数据列只用从索引中就能够取得，不必从数据表中读取 1 row in set, 1 warning (0.00 sec) Using where：表明使用了WHERE过滤。 Using join buffer：使用了连接缓存。 impossible where：WHERE子句的值总是false，不能用来获取任何元组。 mysql&gt; explain select name from pms_category where name = 'zs' and name = 'ls'\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: NULL partitions: NULL type: NULL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: NULL filtered: NULL Extra: Impossible WHERE # 不可能字段同时查到两个名字 1 row in set, 1 warning (0.00 sec) 9.索引分析9.1.单表索引分析 数据准备 DROP TABLE IF EXISTS `article`; CREATE TABLE IF NOT EXISTS `article`( `id` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT '主键', `author_id` INT(10) UNSIGNED NOT NULL COMMENT '作者id', `category_id` INT(10) UNSIGNED NOT NULL COMMENT '分类id', `views` INT(10) UNSIGNED NOT NULL COMMENT '被查看的次数', `comments` INT(10) UNSIGNED NOT NULL COMMENT '回帖的备注', `title` VARCHAR(255) NOT NULL COMMENT '标题', `content` VARCHAR(255) NOT NULL COMMENT '正文内容' ) COMMENT '文章'; INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(1,1,1,1,'1','1'); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(2,2,2,2,'2','2'); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(3,3,3,3,'3','3'); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(1,1,3,3,'3','3'); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(1,1,4,4,'4','4'); 案例：查询category_id为1且comments大于1的情况下，views最多的article_id。 1、编写SQL语句并查看SQL执行计划。 # 1、sql语句 SELECT id,author_id FROM article WHERE category_id = 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1; # 2、sql执行计划 mysql&gt; EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: article partitions: NULL type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 5 filtered: 20.00 Extra: Using where; Using filesort # 产生了文件内排序，需要优化SQL 1 row in set, 1 warning (0.00 sec) 2、创建索引idx_article_ccv。 CREATE INDEX idx_article_ccv ON article(category_id,comments,views); 3、查看当前索引。 4、查看现在SQL语句的执行计划。 我们发现，创建符合索引idx_article_ccv之后，虽然解决了全表扫描的问题，但是在order by排序的时候没有用到索引，MySQL居然还是用的Using filesort，为什么？ 5、我们试试把SQL修改为SELECT id,author_id FROM article WHERE category_id = 1 AND comments = 1 ORDER BY views DESC LIMIT 1;看看SQL的执行计划。 推论：当comments &gt; 1的时候order by排序views字段索引就用不上，但是当comments = 1的时候order by排序views字段索引就可以用上！！！所以，范围之后的索引会失效。 6、我们现在知道范围之后的索引会失效，原来的索引idx_article_ccv最后一个字段views会失效，那么我们如果删除这个索引，创建idx_article_cv索引呢？？？？ /* 创建索引 idx_article_cv */ CREATE INDEX idx_article_cv ON article(category_id,views); 查看当前的索引 7、当前索引是idx_article_cv，来看一下SQL执行计划。 9.2.两表索引分析 数据准备 DROP TABLE IF EXISTS `class`; DROP TABLE IF EXISTS `book`; CREATE TABLE IF NOT EXISTS `class`( `id` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT '主键', `card` INT(10) UNSIGNED NOT NULL COMMENT '分类' ) COMMENT '商品类别'; CREATE TABLE IF NOT EXISTS `book`( `bookid` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT '主键', `card` INT(10) UNSIGNED NOT NULL COMMENT '分类' ) COMMENT '书籍'; 两表连接查询的SQL执行计划 1、不创建索引的情况下，SQL的执行计划。 book和class两张表都是没有使用索引，全表扫描，那么如果进行优化，索引是创建在book表还是创建在class表呢？下面进行大胆的尝试！ 2、左表(book表)创建索引。 创建索引idx_book_card /* 在book表创建索引 */ CREATE INDEX idx_book_card ON book(card); 在book表中有idx_book_card索引的情况下，查看SQL执行计划 3、删除book表的索引，右表(class表)创建索引。 创建索引idx_class_card /* 在class表创建索引 */ CREATE INDEX idx_class_card ON class(card); 在class表中有idx_class_card索引的情况下，查看SQL执行计划 由此可见，左连接将索引创建在右表上更合适，右连接将索引创建在左表上更合适。 9.3.三张表索引分析 数据准备 DROP TABLE IF EXISTS `phone`; CREATE TABLE IF NOT EXISTS `phone`( `phone_id` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT '主键', `card` INT(10) UNSIGNED NOT NULL COMMENT '分类' ) COMMENT '手机'; 三表连接查询SQL优化 1、不加任何索引，查看SQL执行计划。 2、根据两表查询优化的经验，左连接需要在右表上添加索引，所以尝试在book表和phone表上添加索引。 /* 在book表创建索引 */ CREATE INDEX idx_book_card ON book(card); /* 在phone表上创建索引 */ CREATE INDEX idx_phone_card ON phone(card); 再次执行SQL的执行计划 9.4.结论JOIN语句的优化： 尽可能减少JOIN语句中的NestedLoop（嵌套循环）的总次数：永远都是小的结果集驱动大的结果集。 优先优化NestedLoop的内层循环。 保证JOIN语句中被驱动表上JOIN条件字段已经被索引。 当无法保证被驱动表的JOIN条件字段被索引且内存资源充足的前提下，不要太吝惜Join Buffer 的设置。 10.索引失效 数据准备 CREATE TABLE `staffs`( `id` INT(10) PRIMARY KEY AUTO_INCREMENT, `name` VARCHAR(24) NOT NULL DEFAULT '' COMMENT '姓名', `age` INT(10) NOT NULL DEFAULT 0 COMMENT '年龄', `pos` VARCHAR(20) NOT NULL DEFAULT '' COMMENT '职位', `add_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间' )COMMENT '员工记录表'; INSERT INTO `staffs`(`name`,`age`,`pos`) VALUES('Ringo', 18, 'manager'); INSERT INTO `staffs`(`name`,`age`,`pos`) VALUES('张三', 20, 'dev'); INSERT INTO `staffs`(`name`,`age`,`pos`) VALUES('李四', 21, 'dev'); /* 创建索引 */ CREATE INDEX idx_staffs_name_age_pos ON `staffs`(`name`,`age`,`pos`); 10.1.索引失效的情况 全值匹配我最爱。 最佳左前缀法则。 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描。 索引中范围条件右边的字段会全部失效。 尽量使用覆盖索引（只访问索引的查询，索引列和查询列一致），减少SELECT *。 MySQL在使用!=或者&lt;&gt;的时候无法使用索引会导致全表扫描。 is null、is not null也无法使用索引。 like以通配符开头%abc索引失效会变成全表扫描（使用覆盖索引就不会全表扫描了）。 字符串不加单引号索引失效。 少用or，用它来连接时会索引失效。 10.2.最佳左前缀法则 案例 /* 用到了idx_staffs_name_age_pos索引中的name字段 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 'Ringo'; /* 用到了idx_staffs_name_age_pos索引中的name, age字段 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 'Ringo' AND `age` = 18; /* 用到了idx_staffs_name_age_pos索引中的name，age，pos字段 这是属于全值匹配的情况！！！*/ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 'Ringo' AND `age` = 18 AND `pos` = 'manager'; /* 索引没用上，ALL全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `age` = 18 AND `pos` = 'manager'; /* 索引没用上，ALL全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `pos` = 'manager'; /* 用到了idx_staffs_name_age_pos索引中的name字段，pos字段索引失效 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 'Ringo' AND `pos` = 'manager'; 概念 最佳左前缀法则：如果索引是多字段的复合索引，要遵守最佳左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的字段。 口诀：带头大哥不能死，中间兄弟不能断。 10.3.索引列上不计算 案例 # 现在要查询`name` = 'Ringo'的记录下面有两种方式来查询！ # 1、直接使用 字段 = 值的方式来计算 mysql&gt; SELECT * FROM `staffs` WHERE `name` = 'Ringo'; +----+-------+-----+---------+---------------------+ | id | name | age | pos | add_time | +----+-------+-----+---------+---------------------+ | 1 | Ringo | 18 | manager | 2020-08-03 08:30:39 | +----+-------+-----+---------+---------------------+ 1 row in set (0.00 sec) # 2、使用MySQL内置的函数 mysql&gt; SELECT * FROM `staffs` WHERE LEFT(`name`, 5) = 'Ringo'; +----+-------+-----+---------+---------------------+ | id | name | age | pos | add_time | +----+-------+-----+---------+---------------------+ | 1 | Ringo | 18 | manager | 2020-08-03 08:30:39 | +----+-------+-----+---------+---------------------+ 1 row in set (0.00 sec) 我们发现以上两条SQL的执行结果都是一样的，但是执行效率有没有差距呢？？？ 通过分析两条SQL的执行计划来分析性能。 由此可见，在索引列上进行计算，会使索引失效。 口诀：索引列上不计算。 10.4.范围之后全失效 案例 /* 用到了idx_staffs_name_age_pos索引中的name，age，pos字段 这是属于全值匹配的情况！！！*/ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 'Ringo' AND `age` = 18 AND `pos` = 'manager'; /* 用到了idx_staffs_name_age_pos索引中的name，age字段，pos字段索引失效 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = '张三' AND `age` &gt; 18 AND `pos` = 'dev'; 查看上述SQL的执行计划 由此可知，查询范围的字段使用到了索引，但是范围之后的索引字段会失效。 口诀：范围之后全失效。 10.5.覆盖索引尽量用在写SQL的不要使用SELECT *，用什么字段就查询什么字段。 /* 没有用到覆盖索引 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 'Ringo' AND `age` = 18 AND `pos` = 'manager'; /* 用到了覆盖索引 */ EXPLAIN SELECT `name`, `age`, `pos` FROM `staffs` WHERE `name` = 'Ringo' AND `age` = 18 AND `pos` = 'manager'; **口诀：查询一定不用***。 10.6.不等有时会失效/* 会使用到覆盖索引 */ EXPLAIN SELECT `name`, `age`, `pos` FROM `staffs` WHERE `name` != 'Ringo'; /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` != 'Ringo'; 10.7.like百分加右边/* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` LIKE '%ing%'; /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` LIKE '%ing'; /* 使用索引范围查询 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` LIKE 'Rin%'; 口诀：like百分加右边。 如果一定要使用%like，而且还要保证索引不失效，那么使用覆盖索引来编写SQL。 /* 使用到了覆盖索引 */ EXPLAIN SELECT `id` FROM `staffs` WHERE `name` LIKE '%in%'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `name` FROM `staffs` WHERE `name` LIKE '%in%'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `age` FROM `staffs` WHERE `name` LIKE '%in%'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `pos` FROM `staffs` WHERE `name` LIKE '%in%'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `name` LIKE '%in%'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `age` FROM `staffs` WHERE `name` LIKE '%in%'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`,`name`, `age`, `pos` FROM `staffs` WHERE `name` LIKE '%in'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `pos` LIKE '%na'; /* 索引失效 全表扫描 */ EXPLAIN SELECT `name`, `age`, `pos`, `add_time` FROM `staffs` WHERE `name` LIKE '%in'; 口诀：覆盖索引保两边。 10.8.字符要加单引号/* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `name` = 'Ringo'; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `name` = 2000; /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 2000; 这里name = 2000在MySQL中会发生强制类型转换，将数字转成字符串。 口诀：字符要加单引号。 10.9.索引相关题目假设index(a,b,c) Where语句 索引是否被使用 where a = 3 Y，使用到a where a = 3 and b = 5 Y，使用到a，b where a = 3 and b = 5 Y，使用到a，b，c where b = 3 或者 where b = 3 and c = 4 或者 where c = 4 N，没有用到a字段 where a = 3 and c = 5 使用到a，但是没有用到c，因为b断了 where a = 3 and b &gt; 4 and c = 5 使用到a，b，但是没有用到c，因为c在范围之后 where a = 3 and b like ‘kk%’ and c = 4 Y，a，b，c都用到 where a = 3 and b like ‘%kk’ and c = 4 只用到a where a = 3 and b like ‘%kk%’ and c = 4 只用到a where a = 3 and b like ‘k%kk%’ and c = 4 Y，a，b，c都用到 10.10.面试题分析 数据准备 /* 创建表 */ CREATE TABLE `test03`( `id` INT PRIMARY KEY NOT NULL AUTO_INCREMENT, `c1` CHAR(10), `c2` CHAR(10), `c3` CHAR(10), `c4` CHAR(10), `c5` CHAR(10) ); /* 插入数据 */ INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES('a1','a2','a3','a4','a5'); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES('b1','b22','b3','b4','b5'); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES('c1','c2','c3','c4','c5'); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES('d1','d2','d3','d4','d5'); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES('e1','e2','e3','e4','e5'); /* 创建复合索引 */ CREATE INDEX idx_test03_c1234 ON `test03`(`c1`,`c2`,`c3`,`c4`); 题目 /* 最好索引怎么创建的，就怎么用，按照顺序使用，避免让MySQL再自己去翻译一次 */ /* 1.全值匹配 用到索引c1 c2 c3 c4全字段 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c3` = 'a3' AND `c4` = 'a4'; /* 2.用到索引c1 c2 c3 c4全字段 MySQL的查询优化器会优化SQL语句的顺序*/ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c4` = 'a4' AND `c3` = 'a3'; /* 3.用到索引c1 c2 c3 c4全字段 MySQL的查询优化器会优化SQL语句的顺序*/ EXPLAIN SELECT * FROM `test03` WHERE `c4` = 'a4' AND `c3` = 'a3' AND `c2` = 'a2' AND `c1` = 'a1'; /* 4.用到索引c1 c2 c3字段，c4字段失效，范围之后全失效 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c3` &gt; 'a3' AND `c4` = 'a4'; /* 5.用到索引c1 c2 c3 c4全字段 MySQL的查询优化器会优化SQL语句的顺序*/ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c4` &gt; 'a4' AND `c3` = 'a3'; /* 6.用到了索引c1 c2 c3三个字段, c1和c2两个字段用于查找, c3字段用于排序了但是没有统计到key_len中，c4字段失效 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c4` = 'a4' ORDER BY `c3`; /* 7.用到了索引c1 c2 c3三个字段，c1和c2两个字段用于查找, c3字段用于排序了但是没有统计到key_len中*/ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' ORDER BY `c3`; /* 8.用到了索引c1 c2两个字段，c4失效，c1和c2两个字段用于查找，c4字段排序产生了Using filesort说明排序没有用到c4字段 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' ORDER BY `c4`; /* 9.用到了索引c1 c2 c3三个字段，c1用于查找，c2和c3用于排序 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c5` = 'a5' ORDER BY `c2`, `c3`; /* 10.用到了c1一个字段，c1用于查找，c3和c2两个字段索引失效，产生了Using filesort */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c5` = 'a5' ORDER BY `c3`, `c2`; /* 11.用到了c1 c2 c3三个字段，c1 c2用于查找，c2 c3用于排序 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' ORDER BY c2, c3; /* 12.用到了c1 c2 c3三个字段，c1 c2用于查找，c2 c3用于排序 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c5` = 'a5' ORDER BY c2, c3; /* 13.用到了c1 c2 c3三个字段，c1 c2用于查找，c2 c3用于排序 没有产生Using filesort 因为之前c2这个字段已经确定了是'a2'了，这是一个常量，再去ORDER BY c3,c2 这时候c2已经不用排序了！ 所以没有产生Using filesort 和(10)进行对比学习！ */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c2` = 'a2' AND `c5` = 'a5' ORDER BY c3, c2; /* GROUP BY 表面上是叫做分组，但是分组之前必定排序。 */ /* 14.用到c1 c2 c3三个字段，c1用于查找，c2 c3用于排序，c4失效 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c4` = 'a4' GROUP BY `c2`,`c3`; /* 15.用到c1这一个字段，c4失效，c2和c3排序失效产生了Using filesort */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = 'a1' AND `c4` = 'a4' GROUP BY `c3`,`c2`; GROUP BY基本上都需要进行排序，索引优化几乎和ORDER BY一致，但是GROUP BY会有临时表的产生。 10.11.总结索引优化的一般性建议： 对于单值索引，尽量选择针对当前query过滤性更好的索引。 在选择复合索引的时候，当前query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择复合索引的时候，尽量选择可以能够包含当前query中的where子句中更多字段的索引。 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的。 口诀： 带头大哥不能死。 中间兄弟不能断。 索引列上不计算。 范围之后全失效。 覆盖索引尽量用。 不等有时会失效。 like百分加右边。 字符要加单引号。 一般SQL少用or。 11.分析慢SQL的步骤分析： 1、观察，至少跑1天，看看生产的慢SQL情况。 2、开启慢查询日志，设置阈值，比如超过5秒钟的就是慢SQL，并将它抓取出来。 3、explain + 慢SQL分析。 4、show Profile。 5、运维经理 OR DBA，进行MySQL数据库服务器的参数调优。 总结（大纲）： 1、慢查询的开启并捕获。 2、explain + 慢SQL分析。 3、show Profile查询SQL在MySQL数据库中的执行细节和生命周期情况。 4、MySQL数据库服务器的参数调优。 12.查询优化12.1.小表驱动大表 优化原则：对于MySQL数据库而言，永远都是小表驱动大表。 /** * 举个例子：可以使用嵌套的for循环来理解小表驱动大表。 * 以下两个循环结果都是一样的，但是对于MySQL来说不一样， * 第一种可以理解为，和MySQL建立5次连接每次查询1000次。 * 第一种可以理解为，和MySQL建立1000次连接每次查询5次。 */ for(int i = 1; i &lt;= 5; i ++){ for(int j = 1; j &lt;= 1000; j++){ } } // ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ for(int i = 1; i &lt;= 1000; i ++){ for(int j = 1; j &lt;= 5; j++){ } } IN和EXISTS /* 优化原则：小表驱动大表，即小的数据集驱动大的数据集 */ /* IN适合B表比A表数据小的情况*/ SELECT * FROM `A` WHERE `id` IN (SELECT `id` FROM `B`) /* EXISTS适合B表比A表数据大的情况 */ SELECT * FROM `A` WHERE EXISTS (SELECT 1 FROM `B` WHERE `B`.id = `A`.id); EXISTS： 语法：SELECT....FROM tab WHERE EXISTS(subquery);该语法可以理解为： 该语法可以理解为：将主查询的数据，放到子查询中做条件验证，根据验证结果（true或是false）来决定主查询的数据结果是否得以保留。 提示： EXISTS(subquery)子查询只返回true或者false，因此子查询中的SELECT *可以是SELECT 1 OR SELECT X，它们并没有区别。 EXISTS(subquery)子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比，如果担心效率问题，可进行实际检验以确定是否有效率问题。 EXISTS(subquery)子查询往往也可以用条件表达式，其他子查询或者JOIN替代，何种最优需要具体问题具体分析。 12.2.ORDER BY优化 数据准备 CREATE TABLE `talA`( `age` INT, `birth` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ); INSERT INTO `talA`(`age`) VALUES(18); INSERT INTO `talA`(`age`) VALUES(19); INSERT INTO `talA`(`age`) VALUES(20); INSERT INTO `talA`(`age`) VALUES(21); INSERT INTO `talA`(`age`) VALUES(22); INSERT INTO `talA`(`age`) VALUES(23); INSERT INTO `talA`(`age`) VALUES(24); INSERT INTO `talA`(`age`) VALUES(25); /* 创建索引 */ CREATE INDEX idx_talA_age_birth ON `talA`(`age`, `birth`); 案例 /* 1.使用索引进行排序了 不会产生Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` &gt; 20 ORDER BY `age`; /* 2.使用索引进行排序了 不会产生Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` &gt; 20 ORDER BY `age`,`birth`; /* 3.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` &gt; 20 ORDER BY `birth`; /* 4.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` &gt; 20 ORDER BY `birth`,`age`; /* 5.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` ORDER BY `birth`; /* 6.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `birth` &gt; '2020-08-04 07:42:21' ORDER BY `birth`; /* 7.使用索引进行排序了 不会产生Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `birth` &gt; '2020-08-04 07:42:21' ORDER BY `age`; /* 8.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` ORDER BY `age` ASC, `birth` DESC; ORDER BY子句，尽量使用索引排序，避免使用Using filesort排序。 MySQL支持两种方式的排序，FileSort和Index，Index的效率高，它指MySQL扫描索引本身完成排序。FileSort方式效率较低。 ORDER BY满足两情况，会使用Index方式排序： ORDER BY语句使用索引最左前列。 使用WHERE子句与ORDER BY子句条件列组合满足索引最左前列。 结论：尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀原则。 如果不在索引列上，File Sort有两种算法：MySQL就要启动双路排序算法和单路排序算法 1、双路排序算法：MySQL4.1之前使用双路排序，字面意思就是两次扫描磁盘，最终得到数据，读取行指针和ORDER BY列，対他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出。一句话，从磁盘取排序字段，在buffer中进行排序，再从磁盘取其他字段。 取一批数据，要对磁盘进行两次扫描，众所周知，IO是很耗时的，所以在MySQL4.1之后，出现了改进的算法，就是单路排序算法。 2、单路排序算法：从磁盘读取查询需要的所有列，按照ORDER BY列在buffer対它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了。 由于单路排序算法是后出的，总体而言效率好过双路排序算法。 但是单路排序算法有问题：如果SortBuffer缓冲区太小，导致从磁盘中读取所有的列不能完全保存在SortBuffer缓冲区中，这时候单路复用算法就会出现问题，反而性能不如双路复用算法。 单路复用算法的优化策略： 增大sort_buffer_size参数的设置。 增大max_length_for_sort_data参数的设置。 提高ORDER BY排序的速度： ORDER BY时使用SELECT *是大忌，查什么字段就写什么字段，这点非常重要。在这里的影响是： 当查询的字段大小总和小于max_length_for_sort_data而且排序字段不是TEXT|BLOB类型时，会使用单路排序算法，否则使用多路排序算法。 两种排序算法的数据都有可能超出sort_buffer缓冲区的容量，超出之后，会创建tmp临时文件进行合并排序，导致多次IO，但是单路排序算法的风险会更大一些，所以要增大sort_buffer_size参数的设置。 尝试提高sort_buffer_size：不管使用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的。 尝试提高max_length_for_sort_data：提高这个参数，会增加用单路排序算法的概率。但是如果设置的太高，数据总容量sort_buffer_size的概率就增大，明显症状是高的磁盘IO活动和低的处理器使用率。 12.3.GORUP BY优化 GROUP BY实质是先排序后进行分组，遵照索引建的最佳左前缀。 当无法使用索引列时，会使用Using filesort进行排序，增大max_length_for_sort_data参数的设置和增大sort_buffer_size参数的设置，会提高性能。 WHERE执行顺序高于HAVING，能写在WHERE限定条件里的就不要写在HAVING中了。 12.4.总结为排序使用索引 MySQL两种排序方式：Using filesort和Index扫描有序索引排序。 MySQL能为排序与查询使用相同的索引，创建的索引既可以用于排序也可以用于查询。 /* 创建a b c三个字段的索引 */ idx_table_a_b_c(a, b, c) /* 1.ORDER BY 能使用索引最左前缀 */ ORDER BY a; ORDER BY a, b; ORDER BY a, b, c; ORDER BY a DESC, b DESC, c DESC; /* 2.如果WHERE子句中使用索引的最左前缀定义为常量，则ORDER BY能使用索引 */ WHERE a = 'Ringo' ORDER BY b, c; WHERE a = 'Ringo' AND b = 'Tangs' ORDER BY c; WHERE a = 'Ringo' AND b &gt; 2000 ORDER BY b, c; /* 3.不能使用索引进行排序 */ ORDER BY a ASC, b DESC, c DESC; /* 排序不一致 */ WHERE g = const ORDER BY b, c; /* 丢失a字段索引 */ WHERE a = const ORDER BY c; /* 丢失b字段索引 */ WHERE a = const ORDER BY a, d; /* d字段不是索引的一部分 */ WHERE a IN (...) ORDER BY b, c; /* 对于排序来说，多个相等条件(a=1 or a=2)也是范围查询 */ 13.慢查询日志13.1.基本介绍 慢查询日志是什么？ MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阈值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。 long_query_time的默认值为10，意思是运行10秒以上的语句。 由慢查询日志来查看哪些SQL超出了我们的最大忍耐时间值，比如一条SQL执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒钟的SQL，结合之前explain进行全面分析。 特别说明 默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。 当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件。 查看慢查询日志是否开以及如何开启 查看慢查询日志是否开启：SHOW VARIABLES LIKE '%slow_query_log%';。 开启慢查询日志：SET GLOBAL slow_query_log = 1;。使用该方法开启MySQL的慢查询日志只对当前数据库生效，如果MySQL重启后会失效。 # 1、查看慢查询日志是否开启 mysql&gt; SHOW VARIABLES LIKE '%slow_query_log%'; +---------------------+--------------------------------------+ | Variable_name | Value | +---------------------+--------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /var/lib/mysql/1dcb5644392c-slow.log | +---------------------+--------------------------------------+ 2 rows in set (0.01 sec) # 2、开启慢查询日志 mysql&gt; SET GLOBAL slow_query_log = 1; Query OK, 0 rows affected (0.00 sec) 如果要使慢查询日志永久开启，需要修改my.cnf文件，在[mysqld]下增加修改参数。 # my.cnf [mysqld] # 1.这个是开启慢查询。注意ON需要大写 slow_query_log=ON # 2.这个是存储慢查询的日志文件。这个文件不存在的话，需要自己创建 slow_query_log_file=/var/lib/mysql/slow.log 开启了慢查询日志后，什么样的SQL才会被记录到慢查询日志里面呢？ 这个是由参数long_query_time控制的，默认情况下long_query_time的值为10秒。 MySQL中查看long_query_time的时间：SHOW VARIABLES LIKE 'long_query_time%';。 # 查看long_query_time 默认是10秒 # 只有SQL的执行时间&gt;10才会被记录 mysql&gt; SHOW VARIABLES LIKE 'long_query_time%'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) 修改long_query_time的时间，需要在my.cnf修改配置文件 [mysqld] # 这个是设置慢查询的时间，我设置的为1秒 long_query_time=1 查新慢查询日志的总记录条数：SHOW GLOBAL STATUS LIKE '%Slow_queries%';。 mysql&gt; SHOW GLOBAL STATUS LIKE '%Slow_queries%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Slow_queries | 3 | +---------------+-------+ 1 row in set (0.00 sec) 13.2.日志分析工具日志分析工具mysqldumpslow：在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow。 # 1、mysqldumpslow --help 来查看mysqldumpslow的帮助信息 root@1dcb5644392c:/usr/bin# mysqldumpslow --help Usage: mysqldumpslow [ OPTS... ] [ LOGS... ] Parse and summarize the MySQL slow query log. Options are --verbose verbose --debug debug --help write this text to standard output -v verbose -d debug -s ORDER what to sort by (al, at, ar, c, l, r, t), 'at' is default # 按照何种方式排序 al: average lock time # 平均锁定时间 ar: average rows sent # 平均返回记录数 at: average query time # 平均查询时间 c: count # 访问次数 l: lock time # 锁定时间 r: rows sent # 返回记录 t: query time # 查询时间 -r reverse the sort order (largest last instead of first) -t NUM just show the top n queries # 返回前面多少条记录 -a don't abstract all numbers to N and strings to 'S' -n NUM abstract numbers with at least n digits within names -g PATTERN grep: only consider stmts that include this string -h HOSTNAME hostname of db server for *-slow.log filename (can be wildcard), default is '*', i.e. match all -i NAME name of server instance (if using mysql.server startup script) -l don't subtract lock time from total time # 2、 案例 # 2.1、得到返回记录集最多的10个SQL mysqldumpslow -s r -t 10 /var/lib/mysql/slow.log # 2.2、得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /var/lib/mysql/slow.log # 2.3、得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g \"left join\" /var/lib/mysql/slow.log # 2.4、另外建议使用这些命令时结合|和more使用，否则出现爆屏的情况 mysqldumpslow -s r -t 10 /var/lib/mysql/slow.log | more 14.批量插入数据脚本14.1.环境准备 1、建表SQL。 /* 1.dept表 */ CREATE TABLE `dept` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `deptno` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '部门id', `dname` varchar(20) NOT NULL DEFAULT '' COMMENT '部门名字', `loc` varchar(13) NOT NULL DEFAULT '' COMMENT '部门地址', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='部门表' /* 2.emp表 */ CREATE TABLE `emp` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `empno` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '员工编号', `ename` varchar(20) NOT NULL DEFAULT '' COMMENT '员工名字', `job` varchar(9) NOT NULL DEFAULT '' COMMENT '职位', `mgr` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '上级编号', `hiredata` date NOT NULL COMMENT '入职时间', `sal` decimal(7,2) NOT NULL COMMENT '薪水', `comm` decimal(7,2) NOT NULL COMMENT '分红', `deptno` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '部门id', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='员工表' 2、由于开启过慢查询日志，开启了bin-log，我们就必须为function指定一个参数，否则使用函数会报错。 # 在mysql中设置 # log_bin_trust_function_creators 默认是关闭的 需要手动开启 mysql&gt; SHOW VARIABLES LIKE 'log_bin_trust_function_creators'; +---------------------------------+-------+ | Variable_name | Value | +---------------------------------+-------+ | log_bin_trust_function_creators | OFF | +---------------------------------+-------+ 1 row in set (0.00 sec) mysql&gt; SET GLOBAL log_bin_trust_function_creators=1; Query OK, 0 rows affected (0.00 sec) 上述修改方式MySQL重启后会失败，在my.cnf配置文件下修改永久有效。 [mysqld] log_bin_trust_function_creators=ON 14.2.创建函数# 1、函数：随机产生字符串 DELIMITER $$ CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwsyzABCDEFGHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END $$ # 2、函数：随机产生部门编号 DELIMITER $$ CREATE FUNCTION rand_num() RETURNS INT(5) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(100 + RAND() * 10); RETURN i; END $$ 14.3.创建存储过程# 1、函数：向dept表批量插入 DELIMITER $$ CREATE PROCEDURE insert_dept(IN START INT(10),IN max_num INT(10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO dept(deptno,dname,loc) VALUES((START + i),rand_string(10),rand_string(8)); UNTIL i = max_num END REPEAT; COMMIT; END $$ # 2、函数：向emp表批量插入 DELIMITER $$ CREATE PROCEDURE insert_emp(IN START INT(10),IN max_num INT(10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO emp(empno,ename,job,mgr,hiredata,sal,comm,deptno) VALUES((START + i),rand_string(6),'SALESMAN',0001,CURDATE(),2000,400,rand_num()); UNTIL i = max_num END REPEAT; COMMIT; END $$ 14.4.调用存储过程# 1、调用存储过程向dept表插入10个部门。 DELIMITER ; CALL insert_dept(100,10); # 2、调用存储过程向emp表插入50万条数据。 DELIMITER ; CALL insert_emp(100001,500000); 15.Show Profile Show Profile是什么？ Show Profile：MySQL提供可以用来分析当前会话中语句执行的资源消耗情况。可以用于SQL的调优的测量。默认情况下，参数处于关闭状态，并保存最近15次的运行结果。 分析步骤 1、是否支持，看看当前的MySQL版本是否支持。 # 查看Show Profile功能是否开启 mysql&gt; SHOW VARIABLES LIKE 'profiling'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | profiling | OFF | +---------------+-------+ 1 row in set (0.00 sec) 2、开启Show Profile功能，默认是关闭的，使用前需要开启。 # 开启Show Profile功能 mysql&gt; SET profiling=ON; Query OK, 0 rows affected, 1 warning (0.00 sec) 3、运行SQL SELECT * FROM `emp` GROUP BY `id`%10 LIMIT 150000; SELECT * FROM `emp` GROUP BY `id`%20 ORDER BY 5; 4、查看结果，执行SHOW PROFILES; Duration：持续时间。 mysql&gt; SHOW PROFILES; +----------+------------+---------------------------------------------------+ | Query_ID | Duration | Query | +----------+------------+---------------------------------------------------+ | 1 | 0.00156100 | SHOW VARIABLES LIKE 'profiling' | | 2 | 0.56296725 | SELECT * FROM `emp` GROUP BY `id`%10 LIMIT 150000 | | 3 | 0.52105825 | SELECT * FROM `emp` GROUP BY `id`%10 LIMIT 150000 | | 4 | 0.51279775 | SELECT * FROM `emp` GROUP BY `id`%20 ORDER BY 5 | +----------+------------+---------------------------------------------------+ 4 rows in set, 1 warning (0.00 sec) 5、诊断SQL，SHOW PROFILE cpu,block io FOR QUERY Query_ID; # 这里的3是第四步中的Query_ID。 # 可以在SHOW PROFILE中看到一条SQL中完整的生命周期。 mysql&gt; SHOW PROFILE cpu,block io FOR QUERY 3; +----------------------+----------+----------+------------+--------------+---------------+ | Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out | +----------------------+----------+----------+------------+--------------+---------------+ | starting | 0.000097 | 0.000090 | 0.000002 | 0 | 0 | | checking permissions | 0.000010 | 0.000009 | 0.000000 | 0 | 0 | | Opening tables | 0.000039 | 0.000058 | 0.000000 | 0 | 0 | | init | 0.000046 | 0.000046 | 0.000000 | 0 | 0 | | System lock | 0.000011 | 0.000000 | 0.000000 | 0 | 0 | | optimizing | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | | statistics | 0.000023 | 0.000037 | 0.000000 | 0 | 0 | | preparing | 0.000014 | 0.000000 | 0.000000 | 0 | 0 | | Creating tmp table | 0.000041 | 0.000053 | 0.000000 | 0 | 0 | | Sorting result | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | | executing | 0.000003 | 0.000000 | 0.000000 | 0 | 0 | | Sending data | 0.520620 | 0.516267 | 0.000000 | 0 | 0 | | Creating sort index | 0.000060 | 0.000051 | 0.000000 | 0 | 0 | | end | 0.000006 | 0.000000 | 0.000000 | 0 | 0 | | query end | 0.000011 | 0.000000 | 0.000000 | 0 | 0 | | removing tmp table | 0.000006 | 0.000000 | 0.000000 | 0 | 0 | | query end | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | | closing tables | 0.000009 | 0.000000 | 0.000000 | 0 | 0 | | freeing items | 0.000032 | 0.000064 | 0.000000 | 0 | 0 | | cleaning up | 0.000019 | 0.000000 | 0.000000 | 0 | 0 | +----------------------+----------+----------+------------+--------------+---------------+ 20 rows in set, 1 warning (0.00 sec) Show Profile查询参数备注： ALL：显示所有的开销信息。 BLOCK IO：显示块IO相关开销（通用）。 CONTEXT SWITCHES：上下文切换相关开销。 CPU：显示CPU相关开销信息（通用）。 IPC：显示发送和接收相关开销信息。 MEMORY：显示内存相关开销信息。 PAGE FAULTS：显示页面错误相关开销信息。 SOURCE：显示和Source_function。 SWAPS：显示交换次数相关开销的信息。 6、Show Profile查询列表，日常开发需要注意的结论： converting HEAP to MyISAM：查询结果太大，内存都不够用了，往磁盘上搬了。 Creating tmp table：创建临时表（拷贝数据到临时表，用完再删除），非常耗费数据库性能。 Copying to tmp table on disk：把内存中的临时表复制到磁盘，危险！！！ locked：死锁。 16.表锁(偏读)表锁特点： 表锁偏向MyISAM存储引擎，开销小，加锁快，无死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 16.1.环境准备# 1、创建表 CREATE TABLE `mylock`( `id` INT NOT NULL PRIMARY KEY AUTO_INCREMENT, `name` VARCHAR(20) )ENGINE=MYISAM DEFAULT CHARSET=utf8 COMMENT='测试表锁'; # 2、插入数据 INSERT INTO `mylock`(`name`) VALUES('ZhangSan'); INSERT INTO `mylock`(`name`) VALUES('LiSi'); INSERT INTO `mylock`(`name`) VALUES('WangWu'); INSERT INTO `mylock`(`name`) VALUES('ZhaoLiu'); 16.2.锁表的命令 1、查看数据库表锁的命令。 # 查看数据库表锁的命令 SHOW OPEN TABLES; 2、给mylock表上读锁，给book表上写锁。 # 给mylock表上读锁，给book表上写锁 LOCK TABLE `mylock` READ, `book` WRITE; # 查看当前表的状态 mysql&gt; SHOW OPEN TABLES; +--------------------+------------------------------------------------------+--------+-------------+ | Database | Table | In_use | Name_locked | +--------------------+------------------------------------------------------+--------+-------------+ | sql_analysis | book | 1 | 0 | | sql_analysis | mylock | 1 | 0 | +--------------------+------------------------------------------------------+--------+-------------+ 3、释放表锁。 # 释放给表添加的锁 UNLOCK TABLES; # 查看当前表的状态 mysql&gt; SHOW OPEN TABLES; +--------------------+------------------------------------------------------+--------+-------------+ | Database | Table | In_use | Name_locked | +--------------------+------------------------------------------------------+--------+-------------+ | sql_analysis | book | 0 | 0 | | sql_analysis | mylock | 0 | 0 | +--------------------+------------------------------------------------------+--------+-------------+ 16.3.读锁案例 1、打开两个会话，SESSION1为mylock表添加读锁。 # 为mylock表添加读锁 LOCK TABLE `mylock` READ; 2、打开两个会话，SESSION1是否可以读自己锁的表？是否可以修改自己锁的表？是否可以读其他的表？那么SESSION2呢？ # SESSION1 # 问题1：SESSION1为mylock表加了读锁，可以读mylock表！ mysql&gt; SELECT * FROM `mylock`; +----+----------+ | id | name | +----+----------+ | 1 | ZhangSan | | 2 | LiSi | | 3 | WangWu | | 4 | ZhaoLiu | +----+----------+ 4 rows in set (0.00 sec) # 问题2：SESSION1为mylock表加了读锁，不可以修改mylock表！ mysql&gt; UPDATE `mylock` SET `name` = 'abc' WHERE `id` = 1; ERROR 1099 (HY000): Table 'mylock' was locked with a READ lock and can't be updated # 问题3：SESSION1为mylock表加了读锁，不可以读其他的表！ mysql&gt; SELECT * FROM `book`; ERROR 1100 (HY000): Table 'book' was not locked with LOCK TABLES # SESSION2 # 问题1：SESSION1为mylock表加了读锁，SESSION2可以读mylock表！ mysql&gt; SELECT * FROM `mylock`; +----+----------+ | id | name | +----+----------+ | 1 | ZhangSan | | 2 | LiSi | | 3 | WangWu | | 4 | ZhaoLiu | +----+----------+ 4 rows in set (0.00 sec) # 问题2：SESSION1为mylock表加了读锁，SESSION2修改mylock表会被阻塞，需要等待SESSION1释放mylock表！ mysql&gt; UPDATE `mylock` SET `name` = 'abc' WHERE `id` = 1; ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted # 问题3：SESSION1为mylock表加了读锁，SESSION2可以读其他表！ mysql&gt; SELECT * FROM `book`; +--------+------+ | bookid | card | +--------+------+ | 1 | 1 | | 7 | 4 | | 8 | 4 | | 9 | 5 | | 5 | 6 | | 17 | 6 | | 15 | 8 | +--------+------+ 24 rows in set (0.00 sec) 16.4.写锁案例 1、打开两个会话，SESSION1为mylock表添加写锁。 # 为mylock表添加写锁 LOCK TABLE `mylock` WRITE; 2、打开两个会话，SESSION1是否可以读自己锁的表？是否可以修改自己锁的表？是否可以读其他的表？那么SESSION2呢？ # SESSION1 # 问题1：SESSION1为mylock表加了写锁，可以读mylock的表！ mysql&gt; SELECT * FROM `mylock`; +----+----------+ | id | name | +----+----------+ | 1 | ZhangSan | | 2 | LiSi | | 3 | WangWu | | 4 | ZhaoLiu | +----+----------+ 4 rows in set (0.00 sec) # 问题2：SESSION1为mylock表加了写锁，可以修改mylock表! mysql&gt; UPDATE `mylock` SET `name` = 'abc' WHERE `id` = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # 问题3：SESSION1为mylock表加了写锁，不能读其他表! mysql&gt; SELECT * FROM `book`; ERROR 1100 (HY000): Table 'book' was not locked with LOCK TABLES # SESSION2 # 问题1：SESSION1为mylock表加了写锁，SESSION2读mylock表会阻塞，等待SESSION1释放！ mysql&gt; SELECT * FROM `mylock`; ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted # 问题2：SESSION1为mylock表加了写锁，SESSION2读mylock表会阻塞，等待SESSION1释放！ mysql&gt; UPDATE `mylock` SET `name` = 'abc' WHERE `id` = 1; ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted # 问题3：SESSION1为mylock表加了写锁，SESSION2可以读其他表！ mysql&gt; SELECT * FROM `book`; +--------+------+ | bookid | card | +--------+------+ | 1 | 1 | | 7 | 4 | | 8 | 4 | | 9 | 5 | | 5 | 6 | | 17 | 6 | | 15 | 8 | +--------+------+ 24 rows in set (0.00 sec) 16.5.案例结论MyISAM引擎在执行查询语句SELECT之前，会自动给涉及到的所有表加读锁，在执行增删改之前，会自动给涉及的表加写锁。 MySQL的表级锁有两种模式： 表共享读锁（Table Read Lock）。 表独占写锁（Table Write Lock）。 対MyISAM表进行操作，会有以下情况： 対MyISAM表的读操作（加读锁），不会阻塞其他线程対同一表的读操作，但是会阻塞其他线程対同一表的写操作。只有当读锁释放之后，才会执行其他线程的写操作。 対MyISAM表的写操作（加写锁），会阻塞其他线程対同一表的读和写操作，只有当写锁释放之后，才会执行其他线程的读写操作。 16.6.表锁分析mysql&gt; SHOW STATUS LIKE 'table%'; +----------------------------+-------+ | Variable_name | Value | +----------------------------+-------+ | Table_locks_immediate | 173 | | Table_locks_waited | 0 | | Table_open_cache_hits | 5 | | Table_open_cache_misses | 8 | | Table_open_cache_overflows | 0 | +----------------------------+-------+ 5 rows in set (0.00 sec) 可以通过Table_locks_immediate和Table_locks_waited状态变量来分析系统上的表锁定。具体说明如下： Table_locks_immediate：产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1。 Table_locks_waited：出现表级锁定争用而发生等待的次数（不能立即获取锁的次数，每等待一次锁值加1），此值高则说明存在较严重的表级锁争用情况。 此外，MyISAM的读写锁调度是写优先，这也是MyISAM不适合作为主表的引擎。因为写锁后，其他线程不能进行任何操作，大量的写操作会使查询很难得到锁，从而造成永远阻塞。 17.行锁(偏写)行锁特点： 偏向InnoDB存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。 InnoDB存储引擎和MyISAM存储引擎最大不同有两点：一是支持事务，二是采用行锁。 事务的ACID： Atomicity [ˌætəˈmɪsəti] 。 Consistency [kənˈsɪstənsi] 。 Isolation [ˌaɪsəˈleɪʃn]。 Durability [ˌdjʊərəˈbɪlɪti] 。 17.1.环境准备# 建表语句 CREATE TABLE `test_innodb_lock`( `a` INT, `b` VARCHAR(16) )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT='测试行锁'; # 插入数据 INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(1, 'b2'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(2, '3'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(3, '4000'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(4, '5000'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(5, '6000'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(6, '7000'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(7, '8000'); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(8, '9000'); # 创建索引 CREATE INDEX idx_test_a ON `test_innodb_lock`(a); CREATE INDEX idx_test_b ON `test_innodb_lock`(b); 17.2.行锁案例 1、开启手动提交 打开SESSION1和SESSION2两个会话，都开启手动提交。 # 开启MySQL数据库的手动提交 mysql&gt; SET autocommit=0; Query OK, 0 rows affected (0.00 sec) 2、读几知所写 # SESSION1 # SESSION1対test_innodb_lock表做写操作，但是没有commit。 # 执行修改SQL之后，查询一下test_innodb_lock表，发现数据被修改了。 mysql&gt; UPDATE `test_innodb_lock` SET `b` = '88' WHERE `a` = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; SELECT * FROM `test_innodb_lock`; +------+------+ | a | b | +------+------+ | 1 | 88 | | 2 | 3 | | 3 | 4000 | | 4 | 5000 | | 5 | 6000 | | 6 | 7000 | | 7 | 8000 | | 8 | 9000 | +------+------+ 8 rows in set (0.00 sec) # SESSION2 # SESSION2这时候来查询test_innodb_lock表。 # 发现SESSION2是读不到SESSION1未提交的数据的。 mysql&gt; SELECT * FROM `test_innodb_lock`; +------+------+ | a | b | +------+------+ | 1 | b2 | | 2 | 3 | | 3 | 4000 | | 4 | 5000 | | 5 | 6000 | | 6 | 7000 | | 7 | 8000 | | 8 | 9000 | +------+------+ 8 rows in set (0.00 se 3、行锁两个SESSION同时対一条记录进行写操作 # SESSION1 対test_innodb_lock表的`a`=1这一行进行写操作，但是没有commit mysql&gt; UPDATE `test_innodb_lock` SET `b` = '99' WHERE `a` = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # SESSION2 也对test_innodb_lock表的`a`=1这一行进行写操作，但是发现阻塞了！！！ # 等SESSION1执行commit语句之后，SESSION2的SQL就会执行了 mysql&gt; UPDATE `test_innodb_lock` SET `b` = 'asdasd' WHERE `a` = 1; ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 4、行锁两个SESSION同时对不同记录进行写操作 # SESSION1 対test_innodb_lock表的`a`=6这一行进行写操作，但是没有commit mysql&gt; UPDATE `test_innodb_lock` SET `b` = '8976' WHERE `a` = 6; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # SESSION2 対test_innodb_lock表的`a`=4这一行进行写操作，没有阻塞！！！ # SESSION1和SESSION2同时对不同的行进行写操作互不影响 mysql&gt; UPDATE `test_innodb_lock` SET `b` = 'Ringo' WHERE `a` = 4; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 17.3.索引失效行锁变表锁# SESSION1 执行SQL语句，没有执行commit。 # 由于`b`字段是字符串，但是没有加单引号导致索引失效 mysql&gt; UPDATE `test_innodb_lock` SET `a` = 888 WHERE `b` = 8000; Query OK, 1 row affected, 1 warning (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 1 # SESSION2 和SESSION1操作的并不是同一行，但是也被阻塞了？？？ # 由于SESSION1执行的SQL索引失效，导致行锁升级为表锁。 mysql&gt; UPDATE `test_innodb_lock` SET `b` = '1314' WHERE `a` = 1; ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 17.4.间隙锁的危害 什么是间隙锁？ 当我们用范围条件而不是相等条件检索数据，并请求共享或者排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁，对于键值在条件范文内但并不存在的记录，叫做”间隙(GAP)”。 InnoDB也会对这个”间隙”加锁，这种锁的机制就是所谓的”间隙锁”。 间隙锁的危害 因为Query执行过程中通过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值不存在。 间隙锁有一个比较致命的缺点，就是当锁定一个范围的键值后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会対性能造成很大的危害。 17.5.如何锁定一行 SELECT .....FOR UPDATE在锁定某一行后，其他写操作会被阻塞，直到锁定的行被COMMIT。 mysql InnoDB引擎默认的修改数据语句，update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果加排他锁可以使用select …for update语句，加共享锁可以使用select … lock in share mode语句。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select …from…查询数据，因为普通查询没有任何锁机制。 17.6.案例结论InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势了。 但是，InnoDB的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。 17.7.行锁分析mysql&gt; SHOW STATUS LIKE 'innodb_row_lock%'; +-------------------------------+--------+ | Variable_name | Value | +-------------------------------+--------+ | Innodb_row_lock_current_waits | 0 | | Innodb_row_lock_time | 124150 | | Innodb_row_lock_time_avg | 31037 | | Innodb_row_lock_time_max | 51004 | | Innodb_row_lock_waits | 4 | +-------------------------------+--------+ 5 rows in set (0.00 sec) 対各个状态量的说明如下： Innodb_row_lock_current_waits：当前正在等待锁定的数量。 Innodb_row_lock_time：从系统启动到现在锁定总时间长度（重要）。 Innodb_row_lock_time_avg：每次等待所花的平均时间（重要）。 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花的时间。 Innodb_row_lock_waits：系统启动后到现在总共等待的次数（重要）。 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化策略。 18.主从复制18.1.复制基本原理 MySQL复制过程分为三步： Master将改变记录到二进制日志(Binary Log)。这些记录过程叫做二进制日志事件，Binary Log Events； Slave将Master的Binary Log Events拷贝到它的中继日志(Replay Log); Slave重做中继日志中的事件，将改变应用到自己的数据库中。MySQL复制是异步且串行化的。 18.2.复制基本原则 每个Slave只有一个Master。 每个Slave只能有一个唯一的服务器ID。 每个Master可以有多个Salve。 18.3.一主一从配置 1、基本要求：Master和Slave的MySQL服务器版本一致且后台以服务运行。 # 创建mysql-slave1实例 docker run -p 3307:3306 --name mysql-slave1 \\ -v /root/mysql-slave1/log:/var/log/mysql \\ -v /root/mysql-slave1/data:/var/lib/mysql \\ -v /root/mysql-slave1/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=333 \\ -d mysql:5.7 2、主从配置都是配在[mysqld]节点下，都是小写 # Master配置 [mysqld] server-id=1 # 必须 log-bin=/var/lib/mysql/mysql-bin # 必须 read-only=0 binlog-ignore-db=mysql # Slave配置 [mysqld] server-id=2 # 必须 log-bin=/var/lib/mysql/mysql-bin 3、Master配置 # 1、GRANT REPLICATION SLAVE ON *.* TO 'username'@'从机IP地址' IDENTIFIED BY 'password'; mysql&gt; GRANT REPLICATION SLAVE ON *.* TO 'zhangsan'@'172.18.0.3' IDENTIFIED BY '123456'; Query OK, 0 rows affected, 1 warning (0.01 sec) # 2、刷新命令 mysql&gt; FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) # 3、记录下File和Position # 每次配从机的时候都要SHOW MASTER STATUS;查看最新的File和Position mysql&gt; SHOW MASTER STATUS; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000001 | 602 | | mysql | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 4、Slave从机配置 CHANGE MASTER TO MASTER_HOST='172.18.0.4', MASTER_USER='zhangsan', MASTER_PASSWORD='123456', MASTER_LOG_FILE='mysql-bin.File的编号', MASTER_LOG_POS=Position的最新值; # 1、使用用户名密码登录进Master mysql&gt; CHANGE MASTER TO MASTER_HOST='172.18.0.4', -&gt; MASTER_USER='zhangsan', -&gt; MASTER_PASSWORD='123456', -&gt; MASTER_LOG_FILE='mysql-bin.000001', -&gt; MASTER_LOG_POS=602; Query OK, 0 rows affected, 2 warnings (0.02 sec) # 2、开启Slave从机的复制 mysql&gt; START SLAVE; Query OK, 0 rows affected (0.00 sec) # 3、查看Slave状态 # Slave_IO_Running 和 Slave_SQL_Running 必须同时为Yes 说明主从复制配置成功！ mysql&gt; SHOW SLAVE STATUS\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event # Slave待命状态 Master_Host: 172.18.0.4 Master_User: zhangsan Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 602 Relay_Log_File: b030ad25d5fe-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 602 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: bd047557-b20c-11ea-9961-0242ac120002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 5、测试主从复制 # Master创建数据库 mysql&gt; create database test_replication; Query OK, 1 row affected (0.01 sec) # Slave查询数据库 mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test_replication | +--------------------+ 5 rows in set (0.00 sec) 6、停止主从复制功能 # 1、停止Slave mysql&gt; STOP SLAVE; Query OK, 0 rows affected (0.00 sec) # 2、重新配置主从 # MASTER_LOG_FILE 和 MASTER_LOG_POS一定要根据最新的数据来配 mysql&gt; CHANGE MASTER TO MASTER_HOST='172.18.0.4', -&gt; MASTER_USER='zhangsan', -&gt; MASTER_PASSWORD='123456', -&gt; MASTER_LOG_FILE='mysql-bin.000001', -&gt; MASTER_LOG_POS=797; Query OK, 0 rows affected, 2 warnings (0.01 sec) mysql&gt; START SLAVE; Query OK, 0 rows affected (0.00 sec) mysql&gt; SHOW SLAVE STATUS\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.18.0.4 Master_User: zhangsan Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 797 Relay_Log_File: b030ad25d5fe-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 797 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: bd047557-b20c-11ea-9961-0242ac120002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 19.MySql事务19.1.事务是什么？事务是一组SQL语句，要么全部执行成功，要么全部执行失败。通常一个事务对应一个完整的业务(例如银行账户转账业务，该业务就是一个最小的工作单元)事务的提交：COMMIT事务的回滚：ROLLBACK事务的关闭：CLOSE默认情况下一个SQL语句为一个事务。举例说明： 这是两张表 user和user1现在我们需要让丽颖给刘昊然转十块钱。 public static void main(String[] args) { Connection connection = null; PreparedStatement preparedStatement = null; try { Class.forName(\"com.mysql.jdbc.Driver\"); String url = \"jdbc:mysql://localhost:3306/shiwu\"; connection = DriverManager.getConnection(url,\"root\",\"132990\"); // // 禁止jdbc自动提交事务 // connection.setAutoCommit(false); preparedStatement = connection.prepareStatement(\"update user set money = money-? where id= ?\"); preparedStatement.setInt(1,10); preparedStatement.setInt(2,1); preparedStatement.executeUpdate(); String str = null; if(str.equals(\"\")){ } preparedStatement = connection.prepareStatement(\"update user1 set money = money+? where id = ?\"); preparedStatement.setInt(1,10); preparedStatement.setInt(2,1); preparedStatement.executeUpdate(); // // 提交事务 // connection.commit(); } catch (Exception e) { e.printStackTrace(); // // 回滚事务 // try { // connection.rollback(); // } catch (SQLException e1) { // e1.printStackTrace(); // } }finally { try { preparedStatement.close(); connection.close(); } catch (SQLException e) { e.printStackTrace(); } } } 我们可以观察出两个SQL语句中间会报空指针异常，这个时候我们来 看一下运行结果。 所以这个时候我们应该禁止jdbc自动提交事务: connection.setAutoCommit(false); 然后再两条SQl语句执行完之后提交事务 connection.commit(); 如果有异常则回滚事务 catch (Exception e) { e.printStackTrace(); // 回滚事务 try { connection.rollback(); } catch (SQLException e1) { e1.printStackTrace(); } } 19.2.事务的ACID特性 原子性 事务是最小单元，不可再分，要么全部执行成功，要么全部失败回滚 一致性 一致性是指事务必须使数据库从一个一致的状态变到另外一个一致的状态，也就是执行事务之前和之后的状态都必须处于一致的状态。不一致性包含三点：脏读，不可重复读，幻读 隔离性 隔离性是指当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离 持久性 DBMS（数据库管理系统）对数据的修改是永久性的。 19.3.事务的四个隔离级别脏读：一个事务处理过程里读取了另一个未提交的事务中的数据可重复读：一个事务在它运行期间，两次查找相同的表，出现了不同的数据幻读：在一个事务中读取到了别的事务插入的数据，导致前后不一致：A事务读取了B事务已经提交的新增数据。和不可重复读的区别，这里是新增，不可重复读是更改（或删除）。这两种情况对策是不一样的，对于不可重复读，只需要采取行级锁防止该记录数据被更改或删除，然而对于幻读必须加表级锁，防止在这个表中新增一条数据。 读未提交：读未提交，即能够读取到没有被提交的数据，所以很明显这个级别的隔离机制无法解决脏读、不可重复读、幻读中的任何一种 读已提交：读已提交，即能够读到那些已经提交的数据，自然能够防止脏读，但是无法限制不可重复读和幻读 可重复读：可重复读，读取了一条数据，这个事务不结束，别的事务就不可以改这条记录，这样就解决了脏读、不可重复读的问题， 串行化：串行化，多个事务时，只有运行完一个事务之后，才能运行其他事务。 我们来演示一下：SELECT @@AUTOCOMMIT; 查看MySQL是否自动提交事务 0表示手动提交事务 1表示自动提交事务SET AUTOCOMMIT = 0; 设置事务提交方式 0表示手动提交事务 1表示自动提交事务 SELECT @@TX_ISOLATION;查询事务的隔离级别 未提交读的隔离级别 默认为可重复读。我们来将它改成未提交读。 脏读 我们可以看到左边的事务还未提交，右边查到了左边没有的事务。 可提交读的隔离级别 我们可以看到在可提交读隔离级别上解决了脏读 可重复读 一个事务在它运行期间，两次查找相同的表，出现了不同的数据 可重复读隔离级别 可重复读： 我么可以看到这个隔离级别解决了可重复读 幻读 我们可以看到这个并没有出现幻读。因为高版本MySQL Server做的一些优化，在高版本的MySQL用户手册里面，说在可重复读这个级别下，也会对幻读进行一定的防止，但是不能保证绝对不出现幻读。因为串行化一般是不会用到的，效率太低，MySQL默认工作在第三级别，可重复读情况下，所以对这个隔离级别做了优化。 19.4.锁机制MySQL大致可归纳为以下3种锁： 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般表锁：对ＭyISAM表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；对ＭyISAM表的写操作，则会阻塞其他用户对同一表的读和写请求；读读：可以允许读写：不允许写写：不允许InnoDB采用行锁InnoDB实现了以下两种类型的行锁。 共享锁（s）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（Ｘ）：允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。InnoDB行锁是通过索引上的索引项来实现的。InnoDB这种行锁实现特点意味者：只有通过索引条件检索数据，InnoDB才会使用行级锁，否则，InnoDB将使用表锁！对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁；对于普通SELECT语句，InnoDB不会加任何锁。间隙锁（Next-Key锁）当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。比如会给between and 中间所有存在或者不存在数据加锁。可以防止幻读。MVCC（无锁实现） 多版本并发控制机制 –》给每一个事务维护一个数据最初的快照1，未提交读：读读/读写：事务不做任何隔离操作写写：获取记录的排他锁，不能同时进行，除非一个事务 提交或回滚2，已提交读：（其他事务提交或者回滚，它会立即读到）读读：事务读的是事务最初的快照 mvcc机制读写：读的是快照数据，写的也是快照数据 mvcc机制写写：获取记录的排他锁，不能同时进行，除非一个事务 提交或回滚3，可重复读 (jdbc默认隔离级别)读读：事务读的是快照数据 mvcc机制读写：读的是快照数据，写的也是快照数据（除非当前事务提交或回滚，否则访问的都是快照数据） mvcc机制写写：获取记录的排他锁，不能同时进行，除非一个事务 提交或回滚4，串行化读读 ：共享锁多个事务可以同时获取读写 : 共享锁和排它锁写写 ： 排它锁和排它锁 19.5.mysql的for update19.5.1.使用场景如果遇到存在高并发并且对于数据的准确性很有要求的场景，是需要了解和使用for update的。 比如涉及到金钱、库存等。一般这些操作都是很长一串并且是开启事务的。如果库存刚开始读的时候是1，而立马另一个进程进行了update将库存更新为0了，而事务还没有结束，会将错的数据一直执行下去，就会有问题。所以需要for upate 进行数据加锁防止高并发时候数据出错。原则：一锁二判三更新 19.5.2.使用方法select * from table where xxx for update 19.5.3.锁表InnoDB默认是行级别的锁，当有明确指定的主键时候，是行级锁。否则是表级别。 例子: 假设表foods ，存在有id跟name、status三个字段，id是主键，status有索引。 例1: (明确指定主键，并且有此记录，行级锁)SELECT * FROM foods WHERE id=1 FOR UPDATE;SELECT * FROM foods WHERE id=1 and name=‘咖啡色的羊驼’ FOR UPDATE; 例2: (明确指定主键/索引，若查无此记录，无锁)SELECT * FROM foods WHERE id=-1 FOR UPDATE; 例3: (无主键/索引，表级锁)SELECT * FROM foods WHERE name=‘咖啡色的羊驼’ FOR UPDATE; 例4: (主键/索引不明确，表级锁)SELECT * FROM foods WHERE id&lt;&gt;‘3’ FOR UPDATE;SELECT * FROM foods WHERE id LIKE ‘3’ FOR UPDATE; for update的注意点1.for update 仅适用于InnoDB，并且必须开启事务，在begin与commit之间才生效。 2.要测试for update的锁表情况，可以利用MySQL的Command Mode，开启二个视窗来做测试。 当开启一个事务进行for update的时候，另一个事务也有for update的时候会一直等着，直到第一个事务结束吗？答：会的。除非第一个事务commit或者rollback或者断开连接，第二个事务会立马拿到锁进行后面操作。 如果没查到记录会锁表吗？答：会的。表级锁时，不管是否查询到记录，都会锁定表。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://13592491893.github.io/categories/mysql/"}],"tags":[{"name":"MySQL优化","slug":"MySQL优化","permalink":"https://13592491893.github.io/tags/MySQL%E4%BC%98%E5%8C%96/"},{"name":"教程","slug":"教程","permalink":"https://13592491893.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"优化","slug":"优化","permalink":"https://13592491893.github.io/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"Docker基础篇之快速上手","slug":"Docker基础篇之快速上手","date":"2021-12-19T16:00:00.000Z","updated":"2021-12-29T03:19:25.623Z","comments":true,"path":"posts/b75e281c.html","link":"","permalink":"https://13592491893.github.io/posts/b75e281c.html","excerpt":"","text":"Docker基础篇之快速上手第一章 Docker简介是什么？问题：为什么会有 docker 的出现一款产 品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发+运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装?也就是说，安装的时候，把原始环境-模-样地复制过来。开发人员利用Docker可以消除协作编码时“在我的机器上可正常工作”的问题。 之前在服务器配置一个应用的运行环境，要安装各种软件，就拿尚硅谷电商项目的环境来说吧，Java/TomcatMySQL/JDBC驱动包等。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在Windows上安装的这些环境，到了Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等java为例)。而为了让这程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker镜 像的设计，使得Docker得以打过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运.作。 docker理念Docker是基于Go语言实现的云开源项目。Docker的主要目标是“Build, Ship[ and Run Any App,Anywhere“，也就是通过对应用组件的封装、分发、部署、运行等生命期的管理，使用户的APP (可以是一个WEB应用或数据库应用等等)及其运行环境能够做到“一次封装，到处运行”。 Linux容器技术的出现就解决了这样一 一个问题，而Docker就是在它的基础上发展过来的。将应用运行在Docker容器上面，而Docker容器在任何操作系统上都是一-致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作 一句话解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术 能干嘛之前的虚拟机技术虚拟机**(virtual machine)**就是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种作系统，比如在Windows系统里面运行Linux系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统- -模-样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。 虚拟机的缺点: 1、资源占用多 2、冗余步骤多 3、启动慢 容器虚拟化技术由于前面虛拟机存在这些缺点，Linux 发展出了另一种虚拟化技术: Linux 容器(Linux Containers,缩为LXC)。 Linux容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。. 比较了Docker和传统虚拟化方式的不同之处: 1、传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程; 2、而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机为轻便。 3、每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。 开发/运维(DevOps)一次构建、随处运行， 更快速的应用交付和部署​ 传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测试验证时间。 更便捷的升级和扩缩容​ 随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成-块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。 更简单的系统运维​ 应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度–致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。 更高效的计算资源利用​ Docker是内核级虚拟化，其不像传统的虚拟化技术一样 需要额外的Hypervisor支持，所以在-台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。 企业级新浪 美团 蘑菇街 去哪下1、官网docker官网： https://www.docker.com/ docker中文网站: https://www.docker-cn.com/ 2、仓库Docker Hub官网：https://hub.docker.com/ 第二章 Docker安装前提说明CentOS Docker安装Docker支持以下的CentOS版本:CentOS 7 (64-bit)CentOS 6.5 (64-bit)或更高的版本 前提条件目前，CentOS 仅发行版本中的内核支持Docker。Docker运行在CentOS 7.上，要求系统为64位、系统内核版本为3.10以上。Docker运行在CentOS-6.5或更高的版本的CentOS上，要求系统为64位、系统内核版本为2.6.32-431或者更高版本。 Docker 的基本组成docker架构图 镜像( image )Docker镜像(lmage)就是-一个只读的模板。镜像可以用来创建Docker容器，个镜像可以创建很多容器 容器( container)Docker利用容器(Container) 独立运行的一个或一组应用。容器是用镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简 易版的Linux环境(包括root用户权限、进程空间、用户空间和网络空间等)和运行在其中的应用程序。容器的定义和镜像几乎一模一样，也是一堆层的统一视角， 唯- -区别在于容器的最上面那-层是可读可写的。 仓库( repository)仓库(Repository) 是集中存放镜像文件的场所。仓库(Repository)和仓库注册服务器(Registry) 是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多镜像，每个镜像有不同的标签(tag) 。 仓库分为公开仓库(Public) 和私有仓库(Private) 两种形式。最大的公开仓库是Docker Hub(ttps://hub. docker.com/)存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等 小总结 ()需要正确的理解仓储/镜像/容器这几个概念: Docker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一-个可交付的运行环境，这个打好的运行环境就似乎image镜像文件。只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模板。Docker根据image文件生成容器的实例。同一个image文件，可以生成多个同时运行的容器实例。 image文件生成的容器实例，本身也是一一个文件，称为镜像文件。 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一-个对应的运行实例，也就是我们的容器至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。| 安装步骤Centos6.8安装Docker1、yum install -y epel-release 2、yum install -y docker-io 3、安装后的配置文件： etc/sysconfig/docker 4、启动 Docker后台服务: service docker start 5、docker version 验证 Centos7.0安装Dockerhttps://docs.docker.com/engine/install/centos/ centos8 安装docker解决:Failed to start docker.service: Unit docker.service not found. 1. CentOS 8 中安装 docker 和 Podman 冲突 解决步骤： 1) 查看是否安装 Podman rpm -q podman 2) 删除Podman dnf remove podman 重装docker: 分别执行如下命令: sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io sudo yum install docker-ce docker-ce-cli 启动docker: sudo systemctl start docker 查看版本 docker -v 开机自启 systemctl enable docker 设置淘宝镜像 https://173i80ok.mirror.aliyuncs.com sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF' { \"registry-mirrors\": [\"https://173i80ok.mirror.aliyuncs.com\"] } EOF sudo systemctl daemon-reload sudo systemctl restart docker 永远的helloworld阿里云镜像加速是什么 ​ https://promotion.aliyun.com/ntms/act/kubernetes.html 注册一个属于自己的阿里云账户( 可复用淘宝账号) 获得加速器地址连接 ​ 登录阿里云开发者平台 ​ 获取加速器地址 配置本机Docker运行镜像加速器 ​ 鉴于国内网络问题，后续拉取Docker镜像十分缓慢，我们可以需要配置加速器来解决，我使用的是阿里云的本人自己账号的镜像地址(需要自己注册有一个属于你自己的): ht:po/. mirror aliyuncns .com vim /etc/sysconfig/docker将获得的自己账户下的阿里云加速地址配置进other_ args-=”–registry-mirror=https://你自 己的账号加速信息.mirror .aliyuncs.com 重新启动 Docker 后台服务：service docker restart Linux系统下配置完加速器需要检查是否生效 网易云加速基本上同上述阿里云 启动Docker后台容器(测试运行 hello-world ) ​ docker run hello world run干了什么 底层原理Docker是怎样工作的Docker是一个Client-Server结构的系统，Docker守 护进程运行在主机上，然后通过Socket连 接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器，是一个运行时环境，就是我们前面说到的集装箱。 为什么Docker比较比vm快1、docker有着比虚拟机更少的抽象层。由亍docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。2、docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个 容器时,docker不需要和虚拟机一样 重新加载- - 个操作系统内核仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建–个虚拟机时,虚拟机软件需要加载GuestOS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一-个docker容器只需要几秒钟。 第三章 Docker常用命令帮助命令docker Version docker info docker --help 自己查看官网解释，高手都是自己练出来的，百度上只不过是翻译了下，加了点例子 镜像命令docker images 列出本机上的镜像OPTIONS 说明：-a 列出本地所有的镜像(含中间映射层) -q 只显示镜像ID --digests 显示镜像的摘要信息 --no-trunc 显示完整的镜像信息 docker search 某个XXX镜像的名字​ 网站 www.dockerhub.com ​ docker search [OPTIONS] 镜像名字 ​ OPTIONS 说明 --no-trun 显示完整的镜像描述 -s 列出收藏数不小于指定值的镜像 --automated 只列出 automated build类型的镜像 docker pull 某个镜像的名字​ 下载镜像 ​ docker pull 镜像名字[:TAG] docker rmi 某个XXX镜像的名字ID​ 删除镜像 ​ 删除单个 docker rm -f 镜像ID ​ 删除多个 docker rm -f 镜像名1:TAG 镜像名2:TAG ​ 删除多个 docker rmi -f ${docker images -qa} 容器命令有镜像才能创建容器，这是根本前提(下载一个Centos镜像演示) ​ docker pull centos 新建并启动容器​ docker run [OPTIONS] IMAGE [COMMAND][ARG] ​ OPTIONS 说明 OPTIONS说明(常用) :有些是一个减号，有些是两个减号 --name=\"容器新名字\":为容器指定一个名称; -d:后台运行容器，并返回容器ID， 也即启动守护式容器; -i:以交互模式运行容器，通常与-t同时使用; -t:为容器重新分配一个伪输入终端，通常与-i同时使用; -P:随机端口映射; -p:指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 列出当前所有正在运行的容器​ docker ps [OPTIONS] OPTIONS说明(常用) : -a :列出当前所有正在运行的容器+历史上运行过的 -|:显示最近创建的容器。 -n:显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 --no-trunc :不截断输出。 退出容器两种退出方式 ​ exit 容器停止退出 ​ ctrl+P+Q 容器不停止退出 启动容器docker start 容器ID或容器签名 重启容器docker restart 容器ID或容器签名 停止容器docker stop 容器ID或容器签名 强制停止容器docker kill 容器ID或容器签名 删除已停止的容器docker rm 容器ID -f ​ 一次性删除多个容器 ​ docker rm -f $(docker ps -a -q) ​ docker ps -a -q | xargs docker rm 重要启动守护式容器#使用镜像centos:latest以后台模式启动一个容器docker run -d centos 问题:然后docker ps -a进行查看,会发现容器已经退出很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程.容器运行的命令如果不是那些一直挂起的命令 (比如运行top，tail) ，就是会自动退出的。这个是docker的机制问题,比如你的web容器，我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如service nginx start但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用,这样的容器后台启动后，会立即自杀因为他觉得他没事可做了.所以，最佳的解决方案是将你要运行的程序以前台进程的形式运行 查看容器日志docker logs -f -t –tail 容器ID ​ -t 是加入时间戳 ​ -f 跟随最新的日志打印 ​ –tail 数字显示最后多少条 查看容器内的进程docker top 容器ID 查看容器内部细节docker inspect 容器ID 进入正在运行的容器并以命令行交互docker exec -it 容器ID bashShell 重新进入docker attach 容器ID 上述两个区别 attach 直接进入容器启动命令的终端，不会启动新的进程 exec 实在容器中打开新的终端，并且可以穷的那个新的进程 从容器内拷贝文件到主机上docker cp 容器ID:容器内路径 目的主机路径 小总结 第 四 章 Docker 镜像是什么镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS(联合文件系统)UnionFS (状节又件示统)UnionFS (联合文件系统) : Union文件系统(UnionFS)是一一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修作为一 次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a singlevirtualfilesystem)。Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像(没有父镜像)可以制作各种具.体的应用镜像。 特性:一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理Docker镜像加载原理: docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 botfs(boot file system)**主要包含bootloader和kernel**, bootloader主 要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一-层与我们典型的Linux/Unix系统是- - -样的，包含boot加载器和内核。当boot加载完成之 后整个内核就都在内存中了，此时内存的使用权己由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system)，在bootfs之 上。 包含的就是典型Linux系统中的**/dev, /proc, /bin, /etc等标准目录和文件。rootfs就 是各种不同的操作系统发行版，比如Ubuntu，Centos**等等。 平时我们安装的虚拟机的Centos都是好几个G ，为什么docker这里才要200m 对于一个精简的OS, rootfs可 以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel,自只需要提供rootfs就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别，因此不同的发行版可以公用bootfs。 分层的镜像 分层的镜像 为什么 Docker纪念馆想要采用这种分层结构最大的一个好处就是-共享资源比如:有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像,同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 特点Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称为容器层，容器层之下都叫镜像层 Docker镜像Commit操作docker commit 提交容器副本使之称为一个新的镜像 docker commit -m=”提交的描述信息” -a=”作者” 容器ID 要创建的目标镜像名:[标签名] 案例演示：1、从Hub上下载tomcat镜像到本地并成功运行 docker run -d -p 8080:8080 tomcat -p主机端口：docker容器端口 -P:随机分配端口 i:交互 t:终端 2、故意删除上一步镜像生产tomcat容器的文档 3、也即当前的tomcat运行实例是一个没有文档内容的容器，以他为模板commit一个没有doc的tomcat新镜像 atguigu/tomcat02 4、启动我们的新镜像并和原来的对比 ​ 启动atuigu/tomcat02 没有doc ​ 启动原来tomcat他有doc 第 五 章 Docker容器数据卷是什么先来看看Docker的理念:将运用与运行的环境打包形成容器运行，运行可以伴随着容器，但是我们对数据的要求希望是持久化的*容器之间希望有可能共享数据*Docker容器产生的数据，如果不通过docker** commit生成新的镜像，使得数据做为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了。为了能保存数据在docker中我们使用卷。| 一句话：有点类似我们Redis里面的rdb和aof文件 能干嘛卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union FileSystem提供一些用 于持续存储或共享数据的特性:卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不 会在容器删除时删除其挂载的数据卷 特点:1:数据卷可在容器之间共享或重用数据2:卷中的更改可以直接生效3:数据卷中的更改不会包含在镜像的更新中4:数据卷的生命周期一直持续到没有容器使用它为止 容器的持久化 容器间继承+共享数据 数据卷容器内添加直接命令添加 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 查看数据卷是否挂载成功 容器和宿主机之间数据共享 容器停止退出后，主机修改后的数据是否同步 命令(带权限) ​ docker run -it -v /宿主机绝对路径目录:/容器内目录**:ro** 镜像名 DockerFile添加根目录下新建mydocker文件夹并进入 可在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷 File构建 build后生成镜像 获得一个新镜像zzyy/centos run容器 通过上述步骤，容器内的卷目录地址已经知道，对应的主机目录在哪 主机对应默认地址 备注 Docker挂载主机目录Docker访问出现cannot open directory . Permission denied解决办法:在挂载目录后多加一个–privileged=true参数即可 数据卷容器是什么命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器. 总体介绍以上一步新建的zzyy/centos为模板并运行容器 doc1/doc2/doc3 他们已经具有容器卷 ​ /dataVolumeContainer1 ​ /dataVolumeContainer2 容器间传递共享(–volumes -from)先启动一个父容器doc1 启动后在 dataVolumeContainer1中新增内容 doc2/doc3 继承doc1​ –volumes -from doc2/doc3 分别在dataVolumeContainer2各自新增内容 回到doc1可以看到02/03各自添加的都能共享了 删除doc1 doc2修改后doc3是否可以访问 删除doc02后doc3是否访问 在进一步 新建doc04继承doc03 然后删除doc03 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 第 六 章 DockerFile解析是什么Dockerfile是用来构建Docker镜像的构建文件，由一系列命令和参数构成的脚本构建三步骤​ 编写Dockerfile文件 ​ docker build ​ docker run 文件什么样？？？​ 熟悉的Centos为例 http://hub.docker.com/_/centos DockerFile构建过程解析Dockerfile内容基础知识 1、每条保留字指令都必须为大写字母且后面要跟随至少一个参数2、 指令按照从.上到下，顺序执行3、#表示注释4、每条指令都会创建一个新的镜像层，并对镜像进行提交 Docker执行Dockerfile的大致流程1、 docker 从基础镜像运行一个容器2、执行一-条指令并对容器作出修改3、执行类似docker commit的操作提交- -个新的镜像层4、docker再基 于刚提交的镜像运行一一个新容器5、执行dockerfile中的 下一条指令直到所有指令都执行完成 小总结从应用软件的角度来看，Dockerfile、 Docker镜像与Docker容器分别代表软件的三个不同阶段，Dockerfile是软件的原材料Docker镜像是软件的交付品Docker容器则可以认为是软件的运行态。Dockerfile面向开发，Docker镜 像成为交付标准，Docker容 器则涉及部署与运维，三者缺- -不可，合力充当Docker体系的基石。 1、Dockerfile，需要定义一个Dockerfile，Dockerfile定 义了进程需要的一切东西。Dockerfile涉 及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等;2、Docker镜像，在用Dockerfile定义一文件之后，docker build时会产生- -个Docker镜像，当运行Docker镜像时，会真正开始提供服务;3、Docker容器，容器是直接提供服务的。 DockerFile体系结构(保留字指令) 小总结 案例Base 镜像(scratch)Docker Hub中 99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的 自定义镜像mycentos1、编写​ Hub默认Centos镜像是什么情况 准备Dockerfile文件 myCentOS内容Dockerfile FROM centos MAINTAINER ZZYY&lt;zzyy167@126.com&gt; ENV MYPATH /usr/local WORKDIR $MYPATH RUN yum -y install vim RUN yum -y install net-tools EXPOSE 80 CMD echo $MYPATH CMD echo \"success--------------ok\" CMD /bin/bash 2、构建docker build -t 新镜像名字:TAG . 3、运行docker run -it 新镜像名字:TAG 4、列出镜像的变更历史docker history 镜像名 CMD/ENTRYPOINT 镜像案例 都是指定一个容器启动时要运行的命令 CMD​ Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被dockerrun之后的参数替换 ​ Case ​ tomcat的讲解演示 docker run -it -p 8080:8080 tomcat ls -l ENTRYPOINT​ docker run 之后的参数会被当做参数传递给 ENTRYPOINT 之后形成新的命令组合 ​ Case 制作CMD版可以查询IP信息的容器 curl的命令解释curl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。 如果系统没有curl可以使用yum install curl安装，也可以下载安装。curl是将下载文件输出到stdout使用命令: curl http://www .baidu.com执行后，www.baidu.com的html就会显示在屏幕上了 这是最简单的使用方法。用这个命令获得了htp://curl.haxx.se指向的页面，同样，如果这里的URL指向的是–个文件或者一幅图都可以直接下载到本地。如果下载的是HTML文档，那么缺省的将只显示文件头部，即HTML文档的header。要全部显示，请加参数-i WHY 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是command,运行时会替换CMD的默认值。因此这里的-i替换了原来的CMD，而不是添加在原来的curl -s htp://ip.cn后面。而-i 根本不是命令，所以自然找不到。那么如果我们希望加入-i这参数，我们就必须重新完整的输入这个命令:$ docker run myip curl -s http://ip.cn -i 自定义镜像Tomcat1、mkdir -p /zzyy/mydockerfile/tomcat92、在上述目录下 touch c.txt3、将jdk和tomcat安装的压缩包拷贝进上一步目录4、在zzyyuse/mydockerfile/tomcat9目录下新建Dockerfile文件FROM centos MAINTAINER zzyy&lt;zzyybs@ 126.com&gt; #把宿主机当前上下文的c .txt拷贝到容器/usr/local/路径下 COPY c.txt /usr/local/cincontainer.txt #把java与tomcat添加到容器中 ADD jdk-8u171-linux x64.tar .gz /usr/local/ ADD apache-tomcat-9.0.8.tar.gz /usr/ocal/ #安装vim编辑器 RUN yum -y install vim #设置工 作访问时候的WORKDIR路径， 登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配:置java与tomcat环境变量 ENV JAVA_ HOME /usr/localjdk1 .8.0_ 171 ENV CLASSPATH $JAVA_ HOME/lib/dt.jar:$JAVA_ HOME/lib/tools.jar ENV CATALINA_ HOME /usr/local/apache-tomcat-9.0.8 ENV CATALINA_ BASE /usr/ocal/apache-tomcat-9.0.8 ENV PATH $PATH:$JAVA_ HOME/bin:$CATALINA_ HOME/ib:$CATALINA_ HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [\"/usrl/local/apache-tomcat-9.0.8/bin/startup.sh\" ] # CMD [\"/usr/local/apache-tomcat-9.0.8/bin/catalina.sh\",\"run\"] CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/in/logs/catalina.out 目录内容 5、构建 构建完成 6、rundocker run -d -p 9080:8080 -name myt9 -v /zyuse/mydockerfiletomcat9/test:/usrlocal/apache-tomcat9.0.8/webapps/test -v /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usrlocal/apache-tomcat-9.0.8/logs -privileged=true zzyytomcat9 备注 Docker挂载主机目录Docker访问出现cannot open directory : Permission denied解决办法:在挂载目录后多加一个–privileged=true参数即可 7、验证 8、综合前 述容器卷测试的web服务test发布 web.xml &lt;?xml version=\"1 .0\" encoding=\"UTF-8\"?&gt; &lt;web-app xmIns:xsi=\"http://www.w3.org/2001/XML Schema-instance\" xmIns=\"http://java sun.com/xm/ns/javaee\" xsi:schemaL ocation=\"http://java. sun.com/xml/ns/javaee htp:/:/java. sun.com/xml/ns/javaee/web-app_ 2_ _5.xsd\" id=\"WebApp_ ID\" version=\"2.5\"&gt; &lt;display-name&gt;test&lt;/display-name&gt; &lt;/web-app&gt; a.jsp &lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt; &lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN\" http://www.w3.org/TR/html4/loose.dtd\"&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;title&gt;Insert title here &lt;/title&gt; &lt;/head&gt; &lt;body&gt; welcome- &lt;%=\"i am in docker tomcat self \"%&gt; &lt;br&gt; &lt;br&gt; &lt;% System.out,.printIn(\"==========docker tomcat self\");%&gt; &lt;/body&gt; &lt;/htmI&gt; 测试 &lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt; &lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN\" http://www.w3.org/TR/html4/loose.dtd\"&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;title&gt;Insert title here &lt;/title&gt; &lt;/head&gt; &lt;body&gt; welcome- &lt;%=\"i am in docker tomcat self \"%&gt; &lt;br&gt; &lt;br&gt; &lt;% System.out,.printIn(\"==========docker tomcat self\");%&gt; &lt;/body&gt; &lt;/htmI&gt; 小总结 第 七 章 Docker常用安装总体步骤 搜索镜像 拉取镜像 查看镜像 启动镜像 停止容器 移除容器 安装Mysqldocker hub 上查找mysql镜像 从 docker hub(阿里云加速器)拉取mysql镜像到本地标签为5.6 使用mysql5.6镜像创建容器(也叫运行镜像)使用mysql镜像 docker run -p 12345:3306 --name mysql -v /ggcc/mysql/conf:/etc/mysql/conf.d -v /ggcc/mysql/logs:/logs -v /ggcc/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 ---------------------------------------------- 命令说明: -p 12345:3306:将主机的12345端口映射到docker容器的3306端口。 -name mysq:运行服务名字 -V /ggcc/mysql/conf:/etc/mysql/conf.d :将主机/zzyyuse/mysq|录下的conf/my.cnf挂载到容器的/etc/mysql/conf.d -v /ggcc/mysqlogs/logs: 将主机/zzyyuse/mysq|目 录下的logs 目录挂载到容器的/logs。 -V /ggcc/mysqldata:/var/lib/mysql :将主机lzzyyuse/mysql目录下的data目录挂载到容器的/var/lib/mysql . -e MYSQL_ ROOT_ PASSWORD=123456: 初始化root用户的密码。. -d mysql:5.6:后台程序运行mysql5.6 ---------------------------------------------- docker exec -it Mysql运行成功后的容器ID /bin/bash ---------------------------------------------- 数据备份小测试 docker exec mysql服务容器ID sh -c 'exec mysqldump --all-databases -uroot -p\"123456\"' &gt;/ggcc/all-database.sql 数据备份测试 安装Redis从docker hu上(阿里云加速器)拉取redis镜像到本地标签为：3.2 使用redis3.2镜像创建容器(也叫运行镜像)​ 使用镜像 docker run -p 6379:6379 -v /ggcc/myredis/data:/data -v /ggcc/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes 在主机/ggcc/myredis/conf/redis.conf目录上新建redis.conf文件vim /ggcc/myredis/conf/redis.conf/redis.conf # Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k =&gt; 1000 bytes # 1kb =&gt; 1024 bytes # 1m =&gt; 1000000 bytes # 1mb =&gt; 1024*1024 bytes # 1g =&gt; 1000000000 bytes # 1gb =&gt; 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\" # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so ################################## NETWORK ##################################### # By default, if no \"bind\" configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \"bind\" configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 loopback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \"bind\" directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \"bind\" directive. protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 ################################# TLS/SSL ##################################### # By default, TLS/SSL is disabled. To enable it, the \"tls-port\" configuration # directive can be used to define TLS-listening ports. To enable TLS on the # default port, use: # # port 0 # tls-port 6379 # Configure a X.509 certificate and private key to use for authenticating the # server to connected clients, masters or cluster peers. These files should be # PEM formatted. # # tls-cert-file redis.crt # tls-key-file redis.key # Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange: # # tls-dh-params-file redis.dh # Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL # clients and peers. Redis requires an explicit configuration of at least one # of these, and will not implicitly use the system wide configuration. # # tls-ca-cert-file ca.crt # tls-ca-cert-dir /etc/ssl/certs # By default, clients (including replica servers) on a TLS port are required # to authenticate using valid client side certificates. # # If \"no\" is specified, client certificates are not required and not accepted. # If \"optional\" is specified, client certificates are accepted and must be # valid if provided, but are not required. # # tls-auth-clients no # tls-auth-clients optional # By default, a Redis replica does not attempt to establish a TLS connection # with its master. # # Use the following directive to enable TLS on replication links. # # tls-replication yes # By default, the Redis Cluster bus uses a plain TCP connection. To enable # TLS for the bus protocol, use the following directive: # # tls-cluster yes # Explicitly specify TLS versions to support. Allowed values are case insensitive # and include \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\", \"TLSv1.3\" (OpenSSL &gt;= 1.1.1) or # any combination. To enable only TLSv1.2 and TLSv1.3, use: # # tls-protocols \"TLSv1.2 TLSv1.3\" # Configure allowed ciphers. See the ciphers(1ssl) manpage for more information # about the syntax of this string. # # Note: this configuration applies only to &lt;= TLSv1.2. # # tls-ciphers DEFAULT:!MEDIUM # Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more # information about the syntax of this string, and specifically for TLSv1.3 # ciphersuites. # # tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256 # When choosing a cipher, use the server's preference instead of the client # preference. By default, the server follows the client's preference. # # tls-prefer-server-ciphers yes # By default, TLS session caching is enabled to allow faster and less expensive # reconnections by clients that support it. Use the following directive to disable # caching. # # tls-session-caching no # Change the default number of TLS sessions cached. A zero value sets the cache # to unlimited size. The default size is 20480. # # tls-session-cache-size 5000 # Change the default timeout of cached TLS sessions. The default timeout is 300 # seconds. # # tls-session-cache-timeout 60 ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize no # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \"process is ready.\" # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \"/var/run/redis.pid\". # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \"\" # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT &lt;dbid&gt; where # dbid is a number between 0 and 'databases'-1 databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save &lt;seconds&gt; &lt;changes&gt; # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \"save\" lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \"\" save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that's set to 'yes' as it's almost always a win. # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # Remove RDB files used by replication in instances without persistence # enabled. By default this option is disabled, however there are environments # where for regulations or other security concerns, RDB files persisted on # disk by masters in order to feed replicas, or stored on disk by replicas # in order to load them for the initial synchronization, should be deleted # ASAP. Note that this option ONLY WORKS in instances that have both AOF # and RDB persistence disabled, otherwise is completely ignored. # # An alternative (and sometimes better) way to obtain the same effect is # to use diskless replication on both master and replicas instances. However # in the case of replicas, diskless is not always an option. rdb-del-sync-files no # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./ ################################# REPLICATION ################################# # Master-Replica replication. Use replicaof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # +------------------+ +---------------+ # | Master | ---&gt; | Replica | # | (receive writes) | | (exact copy) | # +------------------+ +---------------+ # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of replicas. # 2) Redis replicas are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition replicas automatically try to reconnect to masters # and resynchronize with them. # # replicaof &lt;masterip&gt; &lt;masterport&gt; # If the master is password protected (using the \"requirepass\" configuration # directive below) it is possible to tell the replica to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the replica request. # # masterauth &lt;master-password&gt; # # However this is not enough if you are using Redis ACLs (for Redis version # 6 or greater), and the default user is not capable of running the PSYNC # command and/or other commands needed for replication. In this case it's # better to configure a special user to use with replication, and specify the # masteruser configuration as such: # # masteruser &lt;username&gt; # # When masteruser is specified, the replica will authenticate against its # master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;. # When a replica loses its connection with the master, or when the replication # is still in progress, the replica can act in two different ways: # # 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if replica-serve-stale-data is set to 'no' the replica will reply with # an error \"SYNC with master in progress\" to all the kind of commands # but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, # SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, # COMMAND, POST, HOST: and LATENCY. # replica-serve-stale-data yes # You can configure a replica instance to accept writes or not. Writing against # a replica instance may be useful to store some ephemeral data (because data # written on a replica will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default replicas are read-only. # # Note: read only replicas are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only replica exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only replicas using 'rename-command' to shadow all the # administrative / dangerous commands. replica-read-only yes # Replication SYNC strategy: disk or socket. # # New replicas and reconnecting replicas that are not able to continue the # replication process just receiving differences, need to do what is called a # \"full synchronization\". An RDB file is transmitted from the master to the # replicas. # # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the replicas incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to replica sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more replicas # can be queued and served with the RDB file as soon as the current child # producing the RDB file finishes its work. With diskless replication instead # once the transfer starts, new replicas arriving will be queued and a new # transfer will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple # replicas will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the replicas. # # This is important since once the transfer starts, it is not possible to serve # new replicas arriving, that will be queued for the next RDB transfer, so the # server waits a delay in order to let more replicas arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # ----------------------------------------------------------------------------- # WARNING: RDB diskless load is experimental. Since in this setup the replica # does not immediately store an RDB on disk, it may cause data loss during # failovers. RDB diskless load + Redis modules not handling I/O reads may also # cause Redis to abort in case of I/O errors during the initial synchronization # stage with the master. Use only if your do what you are doing. # ----------------------------------------------------------------------------- # # Replica can load the RDB it reads from the replication link directly from the # socket, or store the RDB to a file and read that file after it was completely # recived from the master. # # In many cases the disk is slower than the network, and storing and loading # the RDB file may increase replication time (and even increase the master's # Copy on Write memory and salve buffers). # However, parsing the RDB file directly from the socket may mean that we have # to flush the contents of the current database before the full rdb was # received. For this reason we have the following options: # # \"disabled\" - Don't use diskless load (store the rdb file to the disk first) # \"on-empty-db\" - Use diskless load only when it is completely safe. # \"swapdb\" - Keep a copy of the current db contents in RAM while parsing # the data directly from the socket. note that this requires # sufficient memory, if you don't have it, you risk an OOM kill. repl-diskless-load disabled # Replicas send PINGs to server in a predefined interval. It's possible to # change this interval with the repl_ping_replica_period option. The default # value is 10 seconds. # # repl-ping-replica-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of replica. # 2) Master timeout from the point of view of replicas (data, pings). # 3) Replica timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-replica-period otherwise a timeout will be detected # every time there is low traffic between the master and the replica. # # repl-timeout 60 # Disable TCP_NODELAY on the replica socket after SYNC? # # If you select \"yes\" Redis will use a smaller number of TCP packets and # less bandwidth to send data to replicas. But this can add a delay for # the data to appear on the replica side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \"no\" the delay for data to appear on the replica side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and replicas are many hops away, turning this to \"yes\" may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # replica data when replicas are disconnected for some time, so that when a # replica wants to reconnect again, often a full resync is not needed, but a # partial resync is enough, just passing the portion of data the replica # missed while disconnected. # # The bigger the replication backlog, the longer the time the replica can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a replica connected. # # repl-backlog-size 1mb # After a master has no longer connected replicas for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last replica disconnected, for # the backlog buffer to be freed. # # Note that replicas never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \"partially # resynchronize\" with the replicas: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The replica priority is an integer number published by Redis in the INFO # output. It is used by Redis Sentinel in order to select a replica to promote # into a master if the master is no longer working correctly. # # A replica with a low priority number is considered better for promotion, so # for instance if there are three replicas with priority 10, 100, 25 Sentinel # will pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the replica as not able to perform the # role of master, so a replica with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. replica-priority 100 # It is possible for a master to stop accepting writes if there are less than # N replicas connected, having a lag less or equal than M seconds. # # The N replicas need to be in \"online\" state. # # The lag in seconds, that must be &lt;= the specified value, is calculated from # the last ping received from the replica, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough replicas # are available, to the specified number of seconds. # # For example to require at least 3 replicas with a lag &lt;= 10 seconds use: # # min-replicas-to-write 3 # min-replicas-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-replicas-to-write is set to 0 (feature disabled) and # min-replicas-max-lag is set to 10. # A Redis master is able to list the address and port of the attached # replicas in different ways. For example the \"INFO replication\" section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover replica instances. # Another place where this info is available is in the output of the # \"ROLE\" command of a master. # # The listed IP and address normally reported by a replica is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the replica to connect with the master. # # Port: The port is communicated by the replica during the replication # handshake, and is normally the port that the replica is using to # listen for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the replica may be actually reachable via different IP and port # pairs. The following two options can be used by a replica in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # replica-announce-ip 5.5.5.5 # replica-announce-port 1234 ############################### KEYS TRACKING ################################# # Redis implements server assisted support for client side caching of values. # This is implemented using an invalidation table that remembers, using # 16 millions of slots, what clients may have certain subsets of keys. In turn # this is used in order to send invalidation messages to clients. Please # to understand more about the feature check this page: # # https://redis.io/topics/client-side-caching # # When tracking is enabled for a client, all the read only queries are assumed # to be cached: this will force Redis to store information in the invalidation # table. When keys are modified, such information is flushed away, and # invalidation messages are sent to the clients. However if the workload is # heavily dominated by reads, Redis could use more and more memory in order # to track the keys fetched by many clients. # # For this reason it is possible to configure a maximum fill value for the # invalidation table. By default it is set to 1M of keys, and once this limit # is reached, Redis will start to evict keys in the invalidation table # even if they were not modified, just to reclaim memory: this will in turn # force the clients to invalidate the cached values. Basically the table # maximum size is a trade off between the memory you want to spend server # side to track information about who cached what, and the ability of clients # to retain cached objects in memory. # # If you set the value to 0, it means there are no limits, and Redis will # retain as many keys as needed in the invalidation table. # In the \"stats\" INFO section, you can find information about the number of # keys in the invalidation table at every given moment. # # Note: when key tracking is used in broadcasting mode, no memory is used # in the server side so this setting is useless. # # tracking-table-max-keys 1000000 ################################## SECURITY ################################### # Warning: since Redis is pretty fast an outside user can try up to # 1 million passwords per second against a modern box. This means that you # should use very strong passwords, otherwise they will be very easy to break. # Note that because the password is really a shared secret between the client # and the server, and should not be memorized by any human, the password # can be easily a long string from /dev/urandom or whatever, so by using a # long and unguessable password no brute force attack will be possible. # Redis ACL users are defined in the following format: # # user &lt;username&gt; ... acl rules ... # # For example: # # user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99 # # The special username \"default\" is used for new connections. If this user # has the \"nopass\" rule, then new connections will be immediately authenticated # as the \"default\" user without the need of any password provided via the # AUTH command. Otherwise if the \"default\" user is not flagged with \"nopass\" # the connections will start in not authenticated state, and will require # AUTH (or the HELLO command AUTH option) in order to be authenticated and # start to work. # # The ACL rules that describe what an user can do are the following: # # on Enable the user: it is possible to authenticate as this user. # off Disable the user: it's no longer possible to authenticate # with this user, however the already authenticated connections # will still work. # +&lt;command&gt; Allow the execution of that command # -&lt;command&gt; Disallow the execution of that command # +@&lt;category&gt; Allow the execution of all the commands in such category # with valid categories are like @admin, @set, @sortedset, ... # and so forth, see the full list in the server.c file where # the Redis command table is described and defined. # The special category @all means all the commands, but currently # present in the server, and that will be loaded in the future # via modules. # +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise # disabled command. Note that this form is not # allowed as negative like -DEBUG|SEGFAULT, but # only additive starting with \"+\". # allcommands Alias for +@all. Note that it implies the ability to execute # all the future commands loaded via the modules system. # nocommands Alias for -@all. # ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of # commands. For instance ~* allows all the keys. The pattern # is a glob-style pattern like the one of KEYS. # It is possible to specify multiple patterns. # allkeys Alias for ~* # resetkeys Flush the list of allowed keys patterns. # &gt;&lt;password&gt; Add this passowrd to the list of valid password for the user. # For example &gt;mypass will add \"mypass\" to the list. # This directive clears the \"nopass\" flag (see later). # &lt;&lt;password&gt; Remove this password from the list of valid passwords. # nopass All the set passwords of the user are removed, and the user # is flagged as requiring no password: it means that every # password will work against this user. If this directive is # used for the default user, every new connection will be # immediately authenticated with the default user without # any explicit AUTH command required. Note that the \"resetpass\" # directive will clear this condition. # resetpass Flush the list of allowed passwords. Moreover removes the # \"nopass\" status. After \"resetpass\" the user has no associated # passwords and there is no way to authenticate without adding # some password (or setting it as \"nopass\" later). # reset Performs the following actions: resetpass, resetkeys, off, # -@all. The user returns to the same state it has immediately # after its creation. # # ACL rules can be specified in any order: for instance you can start with # passwords, then flags, or key patterns. However note that the additive # and subtractive rules will CHANGE MEANING depending on the ordering. # For instance see the following example: # # user alice on +@all -DEBUG ~* &gt;somepassword # # This will allow \"alice\" to use all the commands with the exception of the # DEBUG command, since +@all added all the commands to the set of the commands # alice can use, and later DEBUG was removed. However if we invert the order # of two ACL rules the result will be different: # # user alice on -DEBUG +@all ~* &gt;somepassword # # Now DEBUG was removed when alice had yet no commands in the set of allowed # commands, later all the commands are added, so the user will be able to # execute everything. # # Basically ACL rules are processed left-to-right. # # For more information about ACL configuration please refer to # the Redis web site at https://redis.io/topics/acl # ACL LOG # # The ACL Log tracks failed commands and authentication events associated # with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below. acllog-max-len 128 # Using an external ACL file # # Instead of configuring users here in this file, it is possible to use # a stand-alone file just listing users. The two methods cannot be mixed: # if you configure users here and at the same time you activate the exteranl # ACL file, the server will refuse to start. # # The format of the external ACL user file is exactly the same as the # format that is used inside redis.conf to describe users. # # aclfile /etc/redis/users.acl # IMPORTANT NOTE: starting with Redis 6 \"requirepass\" is just a compatiblity # layer on top of the new ACL system. The option effect will be just setting # the password for the default user. Clients will still authenticate using # AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt; # if they follow the new protocol: both will work. # # requirepass foobared # Command renaming (DEPRECATED). # # ------------------------------------------------------------------------ # WARNING: avoid using this option if possible. Instead use ACLs to remove # commands from the default user, and put them only in some admin user you # create for administrative purposes. # ------------------------------------------------------------------------ # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \"\" # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to replicas may cause problems. ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # # IMPORTANT: When Redis Cluster is used, the max number of connections is also # shared with the cluster bus: every node in the cluster will use two # connections, one incoming and another outgoing. It is important to size the # limit accordingly in case of very large clusters. # # maxclients 10000 ############################## MEMORY MANAGEMENT ################################ # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have replicas attached to an instance with maxmemory on, # the size of the output buffers needed to feed the replicas are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of replicas is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have replicas attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for replica # output buffers (but this is not needed if the policy is 'noeviction'). # # maxmemory &lt;bytes&gt; # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select one from the following behaviors: # # volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set. # allkeys-lru -&gt; Evict any key using approximated LRU. # volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set. # allkeys-lfu -&gt; Evict any key using approximated LFU. # volatile-random -&gt; Remove a random key having an expire set. # allkeys-random -&gt; Remove a random key, any key. # volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL) # noeviction -&gt; Don't evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5 # Starting from Redis 5, by default a replica will ignore its maxmemory setting # (unless it is promoted to master after a failover or manually). It means # that the eviction of keys will be just handled by the master, sending the # DEL commands to the replica as keys evict in the master side. # # This behavior ensures that masters and replicas stay consistent, and is usually # what you want, however if your replica is writable, or you want the replica # to have a different memory setting, and you are sure all the writes performed # to the replica are idempotent, then you may change this default (but be sure # to understand what you are doing). # # Note that since the replica by default does not evict, it may end using more # memory than the one set via maxmemory (there are certain buffers that may # be larger on the replica, or data structures may sometimes take more memory # and so forth). So make sure you monitor your replicas and make sure they # have enough memory to never hit a real out-of-memory condition before the # master hits the configured maxmemory setting. # # replica-ignore-maxmemory yes # Redis reclaims expired keys in two ways: upon access when those keys are # found to be expired, and also in background, in what is called the # \"active expire key\". The key space is slowly and interactively scanned # looking for expired keys to reclaim, so that it is possible to free memory # of keys that are expired and will never be accessed again in a short time. # # The default effort of the expire cycle will try to avoid having more than # ten percent of expired keys still in memory, and will try to avoid consuming # more than 25% of total memory and to add latency to the system. However # it is possible to increase the expire \"effort\" that is normally set to # \"1\", to a greater value, up to the value \"10\". At its maximum value the # system will use more CPU, longer cycles (and technically may introduce # more latency), and will tollerate less already expired keys still present # in the system. It's a tradeoff betweeen memory, CPU and latecy. # # active-expire-effort 1 ############################# LAZY FREEING #################################### # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 4) During replication, when a replica performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transferred. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives. lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no # It is also possible, for the case when to replace the user code DEL calls # with UNLINK calls is not easy, to modify the default behavior of the DEL # command to act exactly like UNLINK, using the following configuration # directive: lazyfree-lazy-user-del no ################################ THREADED I/O ################################# # Redis is mostly single threaded, however there are certain threaded # operations such as UNLINK, slow I/O accesses and other things that are # performed on side threads. # # Now it is also possible to handle Redis clients socket reads and writes # in different I/O threads. Since especially writing is so slow, normally # Redis users use pipelining in order to speedup the Redis performances per # core, and spawn multiple instances in order to scale more. Using I/O # threads it is possible to easily speedup two times Redis without resorting # to pipelining nor sharding of the instance. # # By default threading is disabled, we suggest enabling it only in machines # that have at least 4 or more cores, leaving at least one spare core. # Using more than 8 threads is unlikely to help much. We also recommend using # threaded I/O only if you actually have performance problems, with Redis # instances being able to use a quite big percentage of CPU time, otherwise # there is no point in using this feature. # # So for instance if you have a four cores boxes, try to use 2 or 3 I/O # threads, if you have a 8 cores, try to use 6 threads. In order to # enable I/O threads use the following configuration directive: # # io-threads 4 # # Setting io-threads to 1 will just use the main thread as usually. # When I/O threads are enabled, we only use threads for writes, that is # to thread the write(2) syscall and transfer the client buffers to the # socket. However it is also possible to enable threading of reads and # protocol parsing using the following configuration directive, by setting # it to yes: # # io-threads-do-reads no # # Usually threading reads doesn't help much. # # NOTE 1: This configuration directive cannot be changed at runtime via # CONFIG SET. Aso this feature currently does not work when SSL is # enabled. # # NOTE 2: If you want to test the Redis speedup using redis-benchmark, make # sure you also run the benchmark itself in threaded mode, using the # --threads option to match the number of Redis theads, otherwise you'll not # be able to notice the improvements. ############################ KERNEL OOM CONTROL ############################## # On Linux, it is possible to hint the kernel OOM killer on what processes # should be killed first when out of memory. # # Enabling this feature makes Redis actively control the oom_score_adj value # for all its processes, depending on their role. The default scores will # attempt to have background child processes killed before all others, and # replicas killed before masters. oom-score-adj no # When oom-score-adj is used, this directive controls the specific values used # for master, replica and background child processes. Values range -1000 to # 1000 (higher means more likely to be killed). # # Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities) # can freely increase their value, but not decrease it below its initial # settings. # # Values are used relative to the initial value of oom_score_adj when the server # starts. Because typically the initial value is 0, they will often match the # absolute values. oom-score-adj-values 0 200 800 ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \"appendonly.aof\") appendfilename \"appendonly.aof\" # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don't fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \"everysec\", as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # \"no\" that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use \"always\" that's very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \"everysec\". # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \"appendfsync none\". In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \"yes\". Otherwise leave it as # \"no\" that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \"redis-check-aof\" utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \"REDIS\" # string and loads the prefixed RDB file, and continues loading the AOF # tail. aof-use-rdb-preamble yes ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 ################################ REDIS CLUSTER ############################### # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A replica of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a replica to actually have an exact measure of # its \"data age\", so the following two checks are performed: # # 1) If there are multiple replicas able to failover, they exchange messages # in order to try to give an advantage to the replica with the best # replication offset (more data from the master processed). # Replicas will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single replica computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \"connected\" state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the replica will not try to failover # at all. # # The point \"2\" can be tuned by user. Specifically a replica will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * replica-validity-factor) + repl-ping-replica-period # # So for example if node-timeout is 30 seconds, and the replica-validity-factor # is 10, and assuming a default repl-ping-replica-period of 10 seconds, the # replica will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large replica-validity-factor may allow replicas with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a replica at all. # # For maximum availability, it is possible to set the replica-validity-factor # to a value of 0, which means, that replicas will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-replica-validity-factor 10 # Cluster replicas are able to migrate to orphaned masters, that are masters # that are left without working replicas. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working replicas. # # Replicas migrate to orphaned masters only if there are still at least a # given number of other working replicas for their old master. This number # is the \"migration barrier\". A migration barrier of 1 means that a replica # will migrate only if there is at least 1 other working replica for its master # and so forth. It usually reflects the number of replicas you want for every # master in your cluster. # # Default is 1 (replicas migrate only if their masters remain with at least # one replica). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # This option, when set to yes, prevents replicas from trying to failover its # master during master failures. However the master can still perform a # manual failover, if forced to do so. # # This is useful in different scenarios, especially in the case of multiple # data center operations, where we want one side to never be promoted if not # in the case of a total DC failure. # # cluster-replica-no-failover no # This option, when set to yes, allows nodes to serve read traffic while the # the cluster is in a down state, as long as it believes it owns the slots. # # This is useful for two cases. The first case is for when an application # doesn't require consistency of data during node failures or network partitions. # One example of this is a cache, where as long as the node has the data it # should be able to serve it. # # The second use case is for configurations that don't meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the # entire cluster without this option set, with it set there is only a write outage. # Without a quorum of masters, slot ownership will not change automatically. # # cluster-allow-reads-when-down no # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \"CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;\" if needed. latency-monitor-threshold 0 ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \"foo\" stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix. # E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # t Stream commands # m Key-miss events (Note: It is not included in the 'A' class) # A Alias for g$lshzxet, so that the \"AKE\" string means all the events # (Except key-miss events which are excluded from 'A' due to their # unique nature). # # The \"notify-keyspace-events\" takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don't need # this feature and the feature has some overhead. Note that if you don't # specify at least one of K or E, no events will be delivered. notify-keyspace-events \"\" ############################### GOPHER SERVER ################################# # Redis contains an implementation of the Gopher protocol, as specified in # the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt). # # The Gopher protocol was very popular in the late '90s. It is an alternative # to the web, and the implementation both server and client side is so simple # that the Redis server has just 100 lines of code in order to implement this # support. # # What do you do with Gopher nowadays? Well Gopher never *really* died, and # lately there is a movement in order for the Gopher more hierarchical content # composed of just plain text documents to be resurrected. Some want a simpler # internet, others believe that the mainstream internet became too much # controlled, and it's cool to create an alternative space for people that # want a bit of fresh air. # # Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol # as a gift. # # --- HOW IT WORKS? --- # # The Redis Gopher support uses the inline protocol of Redis, and specifically # two kind of inline requests that were anyway illegal: an empty request # or any request that starts with \"/\" (there are no Redis commands starting # with such a slash). Normal RESP2/RESP3 requests are completely out of the # path of the Gopher protocol implementation and are served as usually as well. # # If you open a connection to Redis when Gopher is enabled and send it # a string like \"/foo\", if there is a key named \"/foo\" it is served via the # Gopher protocol. # # In order to create a real Gopher \"hole\" (the name of a Gopher site in Gopher # talking), you likely need a script like the following: # # https://github.com/antirez/gopher2redis # # --- SECURITY WARNING --- # # If you plan to put Redis on the internet in a publicly accessible address # to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance. # Once a password is set: # # 1. The Gopher server (when enabled, not by default) will still serve # content via Gopher. # 2. However other commands cannot be called before the client will # authenticate. # # So use the 'requirepass' option to protect your instance. # # To enable Gopher support uncomment the following line and set # the option from no (the default) to yes. # # gopher-enabled no ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb &lt;-- not recommended for normal workloads # -4: max size: 32 Kb &lt;-- not recommended # -3: max size: 16 Kb &lt;-- probably not recommended # -2: max size: 8 Kb &lt;-- good # -1: max size: 4 Kb &lt;-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \"don't start compressing until after 1 node into the list, # going from either the head or tail\" # So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail] # 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail, # but compress all nodes between them. # 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Streams macro node max size / items. The stream data structure is a radix # tree of big nodes that encode multiple items inside. Using this configuration # it is possible to configure how big a single node can be in bytes, and the # maximum number of items it may contain before switching to a new node when # appending new stream entries. If any of the following settings are set to # zero, the limit is ignored, so for instance it is possible to set just a # max entires limit by setting max-bytes to 0 and max-entries to the desired # value. stream-node-max-bytes 4096 stream-node-max-entries 100 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \"steps\" are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \"activerehashing no\" if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \"activerehashing yes\" if you don't have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -&gt; normal clients including MONITOR clients # replica -&gt; replica clients # pubsub -&gt; clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt; # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and replica clients, since # subscribers and replicas receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here, but must be 1mb or greater # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \"hz\" value. # # By default \"hz\" is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # Normally it is useful to have an HZ value which is proportional to the # number of clients connected. This is useful in order, for instance, to # avoid too many clients are processed for each background task invocation # in order to avoid latency spikes. # # Since the default HZ value by default is conservatively set to 10, Redis # offers, and enables by default, the ability to use an adaptive HZ value # which will temporary raise when there are many connected clients. # # When dynamic HZ is enabled, the actual configured HZ will be used # as a baseline, but multiples of the configured HZ value will be actually # used as needed once more clients are connected. In this way an idle # instance will use very little CPU time while a busy instance will be # more responsive. dynamic-hz yes # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # When redis saves RDB file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. rdb-save-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R &lt; P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits | # +--------+------------+------------+------------+------------+------------+ # | 0 | 104 | 255 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 1 | 18 | 49 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 10 | 10 | 18 | 142 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 100 | 8 | 11 | 49 | 143 | 255 | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # # redis-benchmark -n 1000000 incr foo # redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less &lt;= 10). # # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # # lfu-log-factor 10 # lfu-decay-time 1 ########################### ACTIVE DEFRAGMENTATION ####################### # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \"hot\" way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \"CONFIG SET activedefrag yes\". # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag no # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage, to be used when the lower # threshold is reached # active-defrag-cycle-min 1 # Maximal effort for defrag in CPU percentage, to be used when the upper # threshold is reached # active-defrag-cycle-max 25 # Maximum number of set/hash/zset/list fields that will be processed from # the main dictionary scan # active-defrag-max-scan-fields 1000 # Jemalloc background thread for purging will be enabled by default jemalloc-bg-thread yes # It is possible to pin different threads and processes of Redis to specific # CPUs in your system, in order to maximize the performances of the server. # This is useful both in order to pin different Redis threads in different # CPUs, but also in order to make sure that multiple Redis instances running # in the same host will be pinned to different CPUs. # # Normally you can do this using the \"taskset\" command, however it is also # possible to this via Redis configuration directly, both in Linux and FreeBSD. # # You can pin the server/IO threads, bio threads, aof rewrite child process, and # the bgsave child process. The syntax to specify the cpu list is the same as # the taskset command: # # Set redis server/io threads to cpu affinity 0,2,4,6: # server_cpulist 0-7:2 # # Set bio threads to cpu affinity 1,3: # bio_cpulist 1,3 # # Set aof rewrite child process to cpu affinity 8,9,10,11: # aof_rewrite_cpulist 8-11 # # Set bgsave child process to cpu affinity 1,10,11 # bgsave_cpulist 1,10-11 测试 redis-cli连接上来docker exec -it 运行着redis服务容器的ID redis-cli 测试持久化文件生成 第 8 章 将镜像推送到阿里云本地镜像发布到阿里云流程 镜像生成方法1、前面的Dockerfile 2、从容器中创建一个新的镜像 docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] 将本地镜像推送到阿里云1、本地镜像素材原型 2、阿里云开发者平台 https://promotion.aliyun.com/ntms/act/kubernetes.html 3、创建镜像仓库 命名空间 仓库名称 4、将镜像推送到registry$ sudo docker login --username=white3e registry.cn-shenzhen.aliyuncs.com $ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号] $ sudo docker push registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号] 其中[ImageId][镜像版本]自己填写 5、公有云可以查询得到 6、查看详情 将阿里云上的镜像下载到本地 至此Docker基础篇完结 ​ 2020-10-4","categories":[{"name":"docker","slug":"docker","permalink":"https://13592491893.github.io/categories/docker/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://13592491893.github.io/tags/linux/"},{"name":"docker","slug":"docker","permalink":"https://13592491893.github.io/tags/docker/"},{"name":"容器","slug":"容器","permalink":"https://13592491893.github.io/tags/%E5%AE%B9%E5%99%A8/"}]},{"title":"ObjectTools","slug":"ObjectTools","date":"2021-12-15T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/6d9732ac.html","link":"","permalink":"https://13592491893.github.io/posts/6d9732ac.html","excerpt":"","text":"ObjectTools package com.shys.industrialinternet.common.utils; import com.esotericsoftware.reflectasm.FieldAccess; import com.esotericsoftware.reflectasm.MethodAccess; import org.apache.commons.lang3.ArrayUtils; import org.apache.commons.lang3.StringUtils; import java.lang.reflect.Field; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.ConcurrentMap; public class ObjectTools { private static final ConcurrentMap&lt;Class, MethodAccess&gt; methodLocalCache = new ConcurrentHashMap(); private static final ConcurrentMap&lt;Class, Field[]&gt; fieldLocalCache = new ConcurrentHashMap(); private static MethodAccess getMethods(Class clazz) { if (methodLocalCache.containsKey(clazz)) { return methodLocalCache.get(clazz); } MethodAccess methodAccess = MethodAccess.get(clazz); methodLocalCache.putIfAbsent(clazz, methodAccess); return methodAccess; } private static Field[] getFields(Class clazz) { if (fieldLocalCache.containsKey(clazz)) { return fieldLocalCache.get(clazz); } Field[] result = new Field[0]; for (Class nextClass = clazz; nextClass != Object.class; nextClass = nextClass.getSuperclass()) { Field[] fields = nextClass.getDeclaredFields(); if (fields.length != 0) { result = ArrayUtils.addAll(result, fields); } } fieldLocalCache.putIfAbsent(clazz, result); return result; } public static &lt;F, T&gt; T copyProperties(F from, Class&lt;T&gt; to) { T result = null; try { result = to.newInstance(); } catch (Exception e) { e.printStackTrace(); } MethodAccess fromMethodAccess = getMethods(from.getClass()); MethodAccess toMethodAccess = getMethods(to); Field[] fromDeclaredFields = getFields(from.getClass()); for (Field field : fromDeclaredFields) { String name = field.getName(); try { Object value = fromMethodAccess.invoke(from, \"get\" + StringUtils.capitalize(name), null); toMethodAccess.invoke(result, \"set\" + StringUtils.capitalize(name), value); } catch (Exception e) { // 设置异常，可能会没有对应字段，忽略 } } return result; } }","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"utils","slug":"utils","permalink":"https://13592491893.github.io/tags/utils/"},{"name":"object","slug":"object","permalink":"https://13592491893.github.io/tags/object/"},{"name":"tool","slug":"tool","permalink":"https://13592491893.github.io/tags/tool/"}]},{"title":"Linux学习","slug":"Linux学习","date":"2021-12-06T17:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/deb5175a.html","link":"","permalink":"https://13592491893.github.io/posts/deb5175a.html","excerpt":"","text":"Linux学习1.虚拟机 虚拟机的网络连接三种形式说明 桥连接：Linux可以和其他的系统通信。但是可能造成IP冲突。 NAT：网络地址转换方式：Linux可以访问外网，不会造成IP冲突。 主机模式：你的Linux是一个独立的主机，不能访问外网。 vmtools: 共享文件夹 共享剪贴板 2.Linux目录结构Linux世界里，一切皆文件。 /bin：是Binary的缩写，这个目录存放着最经常使用的命令。 /sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。 /home：存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。 /root：该目录为系统管理员，也称作超级权限者的用户主目录。 /lib：系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found：这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /etc：所有的系统管理所需要的配置文件和子目录my.conf。 /usr：这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录。 /boot：存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。 /proc：这个目录是一个虚拟的目录，它是系统内存的映射，访问这个目录来获取系统信息。 /srv：service的缩写，该目录存放一些服务启动之后需要提供的数据。 /sys：这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统sysfs。 /tmp：这个目录是用来存放一些临时文件的。 /dev：类似windows的设备管理器，把所有的硬件用文件的形式存储。 /media：linux系统会自动识别一些设备，例如U盘光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。 /mnt：系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将外部的存储挂载在/mnt/上，然后进入该目录就可以查看里面的内容了。 /opt：这是给主机额外安装软件所摆放的目录，如安装ORACLE数据库就可放到该目录下。默认为空。 /usr/local：这是另一个给主机额外安装软件所安装的目录，一般是通过编译源码的方式安装的程序。 /var：这个目录中存放着在不断扩充着的东西，习惯将经常被修改的目录放在这个目录下，包括各种日志文件。 /selinux：SELinux是一种安全子系统，它能控制程序只能访问特定文件。 总结： Linux的目录中有且只有一个根目录。 Linux的各个目录存放的内容是规划好，不用乱放文件。 Linux是以文件的形式管理我们的设备，因此linux系统，一切皆为文件。 Linux的各个文件目录下存放什么内容，必须有一个认识。 3.远程登录Linux系统 远程登录：XShell5 远程上传下载文件：Xftp5 4.vi和vim编辑器 三种常见模式： 正常模式 在正常模式下，我们可以使用快捷键。 插入模式/编辑模式 在这个模式下，程序猿可以输入内容。 命令行模式 在这个模式中，可以提供相关指令。 快捷键使用练习： yy：拷贝当前行 5yy：拷贝当前5行 dd：删除当前行 5dd：删除当前行向下的5行 在文件中查找某个单词：命令行输入 /（查找内容），按n查找下一个 设置文件行号：set nu，取消文件行号：set nonu 编辑文件，正常模式下使用快捷键到达文档最末行：G，最首行：gg 撤销输入：在正常模式下输入u 编辑文件，光标移动到某行：shift+g 显示行号：set nu 输入行号这个数 输入shift+g 5.关机、重启和用户登录注销 shutdown -h now：表示立即关机 shutdown -h 1：表示1分钟后关机 shutdown -r now：立即重启 halt：直接使用，关机 reboot：重启 sync：把内存的数据同步到磁盘上，当我们关机或者重启时，都应该先执行一下sync，防止数据丢失。 logout：注销用户，在图形运行级别无效，在运行级别3有效。 6.用户管理用户，组，家目录。 Linux系统是一个多用户多任务的操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 Linux的用户需要至少要属于一个组。 添加用户：useradd [选项] 用户名。 cd：表示change directory，切换目录。 当创建用户成功后，会自动的创建和用户同名的家目录。 也可以通过useradd -d 指定目录 新的用户名。 指定/修改密码：passwd 用户名 删除用户，保留家目录：userdel 用户名，一般保留家目录，因为干过的活要留着。 删除用户以及家目录：userdel -r 用户名 查询用户信息：id 用户名 切换用户：su - 切换用户名，从权限高切换到权限低的用户不需要输密码。 返回切换前的用户：exit 查看当前用户/登录用户：who am i 用户组：类似于角色，系统可以对有共性的多个用户进行统一的管理。 增加组：groupadd 组名 删除组：groupdel 组名 增加用户时直接加上组：useradd -g 用户组 用户名 修改用户组：usermod -g 用户组 用户名 /etc/passwd 文件 用户（user）的配置文件，记录用户的各种信息。 每行的含义：用户名：口令：用户标识号：注释性描述：主目录：登录shell /etc/shadow 文件 口令配置文件 每行的含义：登录名：加密口令：最后一次修改时间：最小时间间隔：最大时间间隔：警告时间：不活动时间：失效时间：标志 /etc/group 文件 组（group）的配置文件，记录Linux包含的组的信息。 每行含义：组名：口令：组标识号：组内用户列表 7.实用指令 指定运行级别（7个级别） 关机 单用户【找回丢失密码】 多用户状态没有网络服务 多用户状态有网络服务 系统未使用保留给用户 图形界面 系统重启 系统的运行级别配置文件：/etc/inittab 切换到指定运行级别的指令：init [012356] 面试题：如何找回丢失的root密码？：进入到单用户模式，然后修改root密码。因为进入单用户模式，root不需要密码就可以登录。【开机-&gt;在引导时输入 回车键-&gt;看到一个界面输入 e-&gt;看到一个新的界面，选中第二行（编辑内核），再输入 e-&gt;在这行最后输入 1，再输入 回车键-&gt;再输入b，这时就会进入到单用户模式，使用passed来修改root密码。】 帮助指令： man [命令或配置文件] help 文件目录类 pwd：Print Working Directory，显示当前工作目录的绝对路径。 ls：-a：显示当前目录所有的文件和目录，包括隐藏的；-l：以列表的方式显示信息。 cd：cd ~：回到自己的家目录；cd ..：回到当前目录的上一级目录。 mkdir：创建目录；-p：创建多级目录。 rmdir：删除空目录。rmdir不能删除非空的目录。如果需要删除非空的目录，需要使用rm -rf。 touch：创建空文件。可以一次性创建多个文件 cp：拷贝文件到指定目录；-r：递归复制整个文件夹。强制覆盖不提示的方法：cp命令改为\\cp rm：移除文件或目录；-r：递归删除整个文件夹；-f：强制删除不提示。 mv：移动文件与目录或重命名，两种功能！ cat：查看文件内容。只能浏览文件，而不能修改文件。-n：显示行号。结尾加上 | more：分页显示，不会全部一下显示完。 more：是一个基于VI编辑器的文本过滤器，它以全屏幕的方式按页显示文本文件的内容。more还内置了很多快捷键： 操作 功能说明 空白键（Space） 向下翻一页 Enter 向下翻一行 q 立刻离开more，不再显示该文件内容 Ctrl + F 向下滚动一屏 Ctrl + B 返回上一屏 = 输出当前行的行号 ：f 输出文件名和当前行的行号 less：用来分屏查看文件内容，与more相似，但是更强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容。对于显示大型文件具有较高的效率。 &gt;指令：输出重定向。如果不存在会创建文件，否则会将原来的文件内容覆盖。 &gt;&gt;指令：追加。如果不存在会创建文件，否则不会覆盖原来的文件内容，而是追加到文件的尾部。 cat是查看，echo是写入，echo （内容） &gt;&gt; 文件 cal：显示当前月日历。 echo：输出内容到控制台。 head：显示文件的开头部分。-n 5：看前面5行内容。 tail：输出文件中尾部的内容。-n 5：看后面5行内容。-f：时事追踪该文档的所有更新 时间日期类 date：显示当前日期和时间 date “+%Y”：显示当前年份 date “+%d”：显示当前月份 date “+%Y-%m-%d %H:%M:%S”：显示年-月-日 时：分：秒 设置日期：date -s 字符串时间 cal：查看日历指令；cal 年份：显示某一年一整年的日历 搜索查找类 find：从指定目录向下递归的遍历其各个子目录，将满足条件的文件或者目录显示在终端。 find (搜索范围) -name (文件名)：按照指定的文件名查找模式查找文件。 find (搜索范围) -user (用户名)：按照指定的用户名查找模式查找文件。 find (搜索范围) -size (+多少/-多少/多少)：按照指定的文件大小查找模式查找文件（大于多少/小于多少/等于多少） 查询 /目录下所有.txt的文件：find / -name *.txt locate：locate (搜索文件) 可以快速定位文件路径。locate指令利用事先建立的系统中所有文件名称及路径的locate数据库实现快速定位给定的文件。locate指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate时刻。 在第一次运行之前，必须使用updatedb指令创建locate数据库。 grep：过滤查找，表示将前一个命令的处理结果输出传递给后面的命令处理。经常跟管道一起使用。 grep [选项] 查找内容 源文件 -n：显示匹配行及行号。 -i：忽略大小写字母。 cat hello.txt | grep yes 压缩和解压类 gzip/gunzip：压缩文件/解压 gzip (文件)：压缩为.gz文件，原来文件不保留。 gunzip (文件)：解压缩，同样也不保留源文件。 zip/unzip：压缩文件/解压 zip [选项] (压缩后文件xxx.zip) (将要压缩的文件) unzip [选项] (要解压的文件xxx.zip) zip -r：递归压缩，即压缩目录 unzip -d (目录)：指定解压后的文件的存放目录 tar：打包指令，最后打包后的文件是.tar.gz的文件 tar [选项] xxx.tar.gz (打包的内容) -c：产生.tar打包文件 -v：显示详细信息 -f：指定压缩后的文件名 -z：打包同时压缩 -x：解压.tar文件 压缩：tar -zcvf (压缩后文件名) (要压缩的文件) 解压：tar -zxvf (要解压的文件) 解压到指定目录：tar -zxvf (要解压的文件) -C (指定目录)，指定解压到的目录要存在。 8.组管理和权限管理 文件： 所有者 所在组 其他组 改变用户所在组 文件/目录所有者： 一般为文件的创建者，谁创建了该文件，就自然的称为该文件的所有者。 查看文件所有者：ls -ahl 修改文件所有者：chown (用户名) (文件名) 文件所在组不一定是文件所有者。 组的创建 groupadd (组名) 文件/目录所在组 修改文件所在组：chgrp (组名) (文件名) 其他组 除文件的所有者和所在组的用户外，系统的其他用户都是文件的其他组 改变用户所在组 在添加用户时，可以指定将该用户添加到哪个组中，同样的用root的管理权限可以改变某个用户所在的组 改变用户所在组：usermod -g 组名 用户名 改变用户登录的初始目录：usermod -d 目录名 用户名 权限的基本介绍 文件类型： -：普通类型 d：目录 l：软连接 c：字符设备【键盘、鼠标等】 b：块文件【硬盘】 ls -l 显示内容说明： rw-：表示文件所有者权限（rw，读写） r–：表示文件所在组的用户的权限（r，只有读的权限） r–：表示文件其他组的用户的权限（r，只有读的权限） 1：如果是文件，表示硬连接的数；如果是目录则表示该目录的子目录个数 tom：文件所有者 bandit：文件所在组 0：文件的大小，0个字节；如果是目录，则统一为4096 July 1 13：40：文件最后的修改时间 ok.txt：文件名 rwx权限详解 rwx作用到文件： r：read，可读。读取查看。 w：write，可以修改。但不代表可以删除该文件。删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件。 x：execute，可执行。可以被执行。 rwx作用到目录： r：可以读取，ls查看目录内容。 w：可以修改，目录内创建+删除+重命名目录。 x：可执行，可以进入该目录。 修改权限 chmod 修改文件或者目录的权限 u：所有者；g：所在组；o：其他人；a：所有人（u、g、o的总和） chmod u=rwx，g=rx，o=x 文件目录名：分别权限 chmod o+w 文件目录名：给其他人都增加写的权限 chmod a-x 文件目录名：给所有的用户都减掉执行权限 通过数字变更权限 规则：r=4 w=2 x=1 rwx=4+2+1=7 chmod u=rwx，g=rx，o=x 文件目录名 等价于 chmod 751 文件目录名 修改文件所有者 chown chown newowner file：改变文件的所有者 chown newowner：newgroup file：改变用户的所有者和所在组 -R：如果是目录，则使其下所有子文件或目录递归生效 修改文件所在组 chgrp chgrp newgroup file：改变文件的所有组 -R：如果是目录，则使其下所有子文件或目录递归生效 9.定时任务调度 crond任务调度：crontab进行定时任务调度 crontab [选项] -e：编辑crontab定时任务 -i：查询crontab任务 -r：删除当前用户所有的crontab任务 -l：列出当前有哪些任务调度 service crond restart：重启任务调度 当保存退出后就生效了 参数细节说明 项目 含义 范围 第一个“*” 一小时当中的第几分钟 0-59 第二个“*” 一天当中的第几小时 0-23 第三个“*” 一个月当中的第几天 1-31 第四个“*” 一年当中的第几月 1-12 第五个“*” 一周当中的星期几 0-7（0和7都代表星期日） 特殊符号说明 *：代表任何时间。比如第一个*就代表一小时中每分钟都执行一次的意思。 ,：代表不连续的时间。比如“0 8,12,16 * * *命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令。 -：代表连续的时间范围。比如“0 5 * * 1-6命令”，代表在周一到周六的凌晨5点0分执行命令。 /n：代表每隔多久执行一次。比如“*/10 * * * * 命令”，代表每隔10分钟就执行一遍命令。 10.Linux磁盘分区、挂载 分区的方式 mbr分区 最多支持四个主分区 系统只能安装在主分区 扩展分区要占一个主分区 MBR最大只支持2TB，但拥有最好的兼容性 gpt分区 支持无限多个主分区（但操作系统可能限制，比如windows下最多128个分区） 最大支持18EB的大容量（1EB=1024PB，PB=1024TB） windows7 64位以后支持gpt Linux分区 Linux来说无论有几个分区，分给哪一个目录使用，它归根结底就只有一个根目录，一个独立且唯一的文件结构，Linux中每个分区都是用来组成整个文件系统的一部分。 Linux采用了一种叫做“载入”的处理方法，它的整个文件系统中包含了一整套的文件和目录，且将一个分区和一个目录联系起来。这时要载入的一个分区将使它的存储空间在一个目录下获得。 硬盘说明 Linux硬盘分IDE硬盘和SCSI硬盘，目前基本上是SCSI硬盘 lsblk -f：查看当前系统的分区和挂载情况。（list block） 挂载的经典案例 需求是给我们的Linux系统增加一个新的硬盘，并且挂载到/home/newdisk 虚拟机添加硬盘 分区：fdsk /dev/sdb 格式化：mkfs -t ext4 /dev/sdb1 挂载：新建目录：mkdir /home/newdisk；挂载：mount /dev/sdb1 /home/newdisk 设置可以自动挂载（永久挂载）：重启系统后，仍然可以挂载。vim etc/fstab 增加挂载信息。mount -a：生效 取消挂载：unmount /dev/sdb1 磁盘情况查询：df -h / df -l 查询指定目录的磁盘占用情况：du -h /目录，默认为当前目录 -s：指定目录占用大小汇总 -h：带计量单位 -a：含文件 –max-depth=1：子目录深度 -c：列出明细的同时，增加汇总值 磁盘情况-工作实用指令 统计/home文件夹下文件的个数：ls -l /home | grep \"^-\" | wc -l 统计/home文件夹下目录的个数：ls -l /home | grep \"^d\" | wc -l 统计/home文件夹下文件的个数，包括子文件夹里的：ls -lR /home | grep \"^-\" | wc -l 统计文件夹下目录的个数，包括子文件夹里的：ls -lR /home | grep \"^d\" | wc -l 以树状显示目录结构：首先安装tree指令：yum install tree，tree 11.网络配置 指定固定IP：直接修改配置文件来指定IP，并可以连接到外网，编辑：vim /etc/sysconfig/network-scripts/ifcfg-eth0 重启网络服务：service network restart 12.进程管理 在Linux中，每个执行的程序（代码）都称为一个进程。每个进程都分配一个ID号 每一个进程，都会对应一个父进程，而这个父进程可以复制多个子进程。例如www服务器。 每个进程都可能以两种方式存在。 前台和后台 。 前台进程：用户目前的屏幕上可以进行操作的。 后台进程：实际在操作，但由于屏幕上无法看到的进程，通常使用后台方式执行。 一般系统的服务都是以后台进程的方式存在，而且都会常驻在系统中，直到关机才结束。 显示系统执行的进程 ps：查看目前系统中，有哪些正在执行，以及它们执行的状况。可以不加任何参数。PID：进程识别号；TTY：终端机号；TIME：此进程所消耗的CPU时间；CMD：正在执行的命令或进程名 ps -a：显示当前终端的所有进程信息。 ps -u：以用户的格式显示进程信息。 ps -x：显示后台进程运行的参数。 ps -axu | grep xxx：过滤得到xxx的信息。 ps -ef：以全格式显示当前所有的进程，查看进程的父进程。 -e：显示所有进程。 -f：全格式。 终止进程 kill [选项] 进程号：通过进程号杀死进程 killall 进程名称：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用 -9：表示强迫进程立刻停止 案例1：踢掉非法用户：kill 进程号 案例2：终止远程登录服务sshd，在适当时候再次重启sshd服务 案例3：终止多个gedit编辑器：killall 进程名称 案例4：强制杀掉一个终端：kill -9 进程号 查看进程树：pstree [选项] -p：显示进程的PID -u：显示进程的所属用户 服务（service）管理 service管理指令：service 服务名 [start | stop | restart | reload | status] 在CentOS7.0之后，不再使用service，而是systemctl 查看防火墙情况： service iptables status systemctl status firewalld（7.0之后的版本） 测试某个端口是否在监听：telnet 查看服务名： 方式1：使用setup-&gt;系统服务就可以看到 方式2：/etc/init.d/服务名称 服务的运行级别（runlevel）： 查看或修改默认级别：vim /etc/inittab 每个服务对应的每个运行级别都可以设置 如果不小心将默认的运行级别设置成0或者6，怎么处理？ 进入单用户模式，修改成正常的即可。 chkconfig：可以给每个服务的各个运行级别设置自启动/关闭 查看xxx服务：chkconfig –list | grep xxx 查看服务的状态：chkconfig 服务名 –list 给服务的运行级别设置自启动：chkconfig –level 5 服务名 on/off 要所有运行级别关闭或开启：chkconfig 服务名 on/off 动态监控进程 top [选项] top和ps命令很相似。它们都用来显示正在执行的进程。top和ps最大的不同之处在于top在执行一段时间可以更新正在运行的进程。 -d 秒数：指定top命令每隔几秒更新。默认是3秒。 -i：使top不显示任何闲置或者僵死进程。 -p：通过指定监控进程ID来仅仅监控某个进程的状态。 案例1：监控特定用户：top查看进程；u输入用户名。 案例2：终止指定的进程：top查看进程；k输入要结束的进程。 案例3：指定系统状态更新的时间（每隔10秒自动更新，默认是3秒）：top -d 10 交互操作说明： P：以CPU使用率排序，默认就是此项 M：以内存的使用率排序 N：以PID排序 q：退出top 监控网络状态 netstat [选项] -an：按一定顺序排列输出 -p：显示哪个进程在调用 13.RPM RPM：RedHat Package Manager，红帽软件包管理工具。 RPM查询已安装的rpm列表：rpm -qa | grep xx rpm包的其它查询指令： rpm -qa：查询所安装的所有rpm软件包 rpm -qa | more rpm -qa | grep xx rpm -q xx：查询xx软件包是否安装 rpm -qi xx：查询软件包信息 rpm -ql xx：查询软件包中的文件 rpm -qf 文件全路径名：查询文件所属的软件包 卸载rpm包：rpm -e 软件包名称 删除时可能会发生依赖错误，忽视依赖强制删除的方法：rpm -e –nodeps 软件包名称 安装rpm包：rpm -ivh 软件包全路径名称 i=install：安装 v=verbose：提示 h=hash：进度条 14.YUM YUM：是一个shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包。使用yum的前提是联网。 yum list | grep xx：查询yum服务器是否有需要安装的软件 yum install xx：安装指定的yum包 yum -y remove xx：卸载指定的yum包 15.搭建JAVAEE环境 将软件上传到/opt下 解压缩 配置环境变量的配置文件vim /etc/profile JAVA_HOME=/opt/jdk1.7.0_79 PATH=/opt/jdk1.7.0_79/bin:$PATH export JAVA_HOME PATH 保存然后source /etc/profile生效 16.安装Tomcat 解压缩到/opt：tar -zxvf apache-tomcat-7.0.70.tar.gz 进入tomcat的bin目录，启动tomcat ./startup.sh：./startup.sh 开放端口 vim /etc/sysconfig/iptables firewall-cmd –zone=public –add-port=8080/tcp –permanent（Centos7） systemctl restart firewalld.service firewall-cmd –reload 重启防火墙生效 测试是否安装成功：在windows和Linux下访问http://linuxip:8080 17.安装Eclipse 解压缩到/opt：tar -zxvf eclipse-jee-mars-2-linux-gtk-x86_64.tar.gz 启动eclipse，配置jre和server：./eclipse 编写Hello world程序并测试成功 编写jsp页面，并测试成功 18.安装mysql 查看是否有mysql：rpm -qa | grep mysql 删除旧mysql：rpm -e –nopdeps mysql（强制删除） 安装环境：yum -y install make gcc-c++ cmake bison-devel ncurses-devel 解压mysql：tar -zxvf mysql-5.6.14.tar.gz 进入mysql目录 编译安装： cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql-DMYSQL_DATADIR=/usr/local/mysql/data -DSYSCONFDIR=/etc-DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1-DWITH_MEMORY_STORAGE_ENGINE=1 -DWITH_READLINE=1-DMYSQL_UNIX_ADDR=/var/lib/mysql/mysql.sock -DMYSQL_TCP_PORT=3306-DENABLED_LOCAL_INFILE=1 -DWITH_PARTITION_STORAGE_ENHINE=1-DEXTRA_CHARSETS=all -DDEFAULT_CHARSET=utf8-DDEFAULT_COLLATION=utf8_general_ci 编译并安装：make &amp;&amp; make install 配置mysql，设置权限 查看是否有mysql用户和组：cat /etc/passwd，cat /etc/group 添加mysql组：groupadd mysql 添加mysql用户并放在mysql组中：useradd -g mysql mysql 修改/usr/local/mysql权限：chown -R mysql:mysql /usr/local/mysql/ 初始化mysql： scripts/mysql_install_db –basedir=/usr/local/mysql –datadir=/usr/local/mysql/data–user=mysql 如果报错：Can't locate Data/Dumper.pm，则运行：yum install 'perl(Data::Dumper)'，参考链接：https://www.cnblogs.com/yanghongfei/p/7118072.html 删除之前mysql的配置文件：mv /etc/my.cnf /etc/my.cnf.bak 启动MySQL 添加服务，拷贝服务脚本到init.d目录，并设置开机启动 [注意在 /usr/local/mysql 下执行] cp support-files/mysql.server /etc/init.d/mysql chkconfig mysql on service mysql start 执行下面的命令修改root密码 cd /usr/local/mysql/bin ./mysql -u root -p set password = password(‘root’);（quit退出mysql） 19.Shell编程 Shell是一个命令行解释器，它为用户提供了一个向Linux内核发送请求以便裕兴程序的界面系统级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序。 Shell脚本的执行方式： 脚本格式要求： 脚本以#!/bin/bash 开头 脚本需要有可执行权限 脚本的常用执行方式： 方式1（输入脚本的绝对路径或相对路径） 首先要赋予xx.sh脚本的+x权限：chmod 744 myShell.sh 执行脚本：./myShell.sh 方式2（sh+脚本）： 说明：不用赋予+x权限，直接执行即可 sh ./myShell.sh shell的变量 shell变量的介绍 Linux Shell的变量分为，系统变量和用户自定义变量 系统变量：$HOME、$PWD、$SHELL、$USER等等 显示当前shell中所有变量：set shell变量的定义 基本语法 定义变量：变量=值，**=两边不能有空格** 撤销变量：unset 变量 声明静态变量：readonly 变量，注意：不能unset 定义变量的规则 变量名称可以由字母、数字和下划线组成，但是不能以数字开头 等号两侧不能有空格 变量名称一般习惯为大写 将命令的返回值赋给变量 A=ls -la这里有反引号（ESC下面），运行里面的命令，并把结果返回给变量A A=$(ls -la)等价于上面 设置环境变量 基本语法 export 变量名=变量值：将shell变量输出为环境变量 source 配置文件：让修改后的配置文件信息立即生效 echo $变量名：查询环境变量的值 多行注释： :&lt;&lt;! 需要注释的内容 ! 位置参数变量 当我们执行一个shell脚本时，如果希望获取到命令行的参数信息就可以使用到位置参数变量。比如： ./myshell.sh 100 200，这个就是一个执行shell的命令行，可以在myshell脚本中传参100，200。 基本语法： $n：n为数字，$0代表命令本身，$1-$9代表第一到第九个参数，10以上的参数需要用大括号包含，如${10} $*：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体 $@：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待 $#：这个变量代表命令行中所有参数的个数 预定义变量 shell设计者事先已经定义好的变量，可以直接在shell脚本中使用 基本语法： $$：当前进程的进程号（PID） $!：后台运行的最后一个进程的进程号（PID） $?：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确。 后台运行：./myShell.sh &amp; 运算符 在Shell中进行各种运算操作 “$((运算式))”或“$[运算时]” expr m + n，注意expr运算符间要有空格 expr m - n expr \\* / %，乘，除，取余 条件判断 基本语法：[ condition ]，注意condition前后有空格！ 非空返回true，可使用$?验证（0为true，&gt;1为false） 两个整数比较 =：字符串比较 -lt：小于 -le：小于等于 -eq：等于 -gt：大于 -ge：大于等于 -ne：不等于 按照文件权限进行判断 -r：有读的权限 -w：有写的权限 -x：有执行的权限 按照文件类型进行判断 -f：文件存在并且是一个常规的文件 -e：文件存在 -d：文件存在并且是一个目录 流程控制if语句 if判断基本语法： if [ 条件判断式 ];then 程序 fi 或者： if [ 条件判断式 ] then 程序 elif [ 条件判断式 ] then 程序 fi 流程控制case语句 case语句基本语法： case $变量名 in “值1”) 如果变量的值等于值1，则执行程序1 ;; “值2”) 如果变量的值等于值2，则执行程序2 ;; …省略其他分支… *) 如果变量的值都不是以上的值，则执行此程序 ;; esac 流程控制for循环 for循环基本语法1： for 变量 in 值1 值2 值3… do 程序 done for循环基本语法2 for ((初始值;循环控制条件;变量变化)) do 程序 done 流程控制while循环 while循环基本语法1： while [ 条件判断式 ] do 程序 done read读取控制台的输入 read [选项] (参数) -p：指定读取值时的提示符 -t：指定读取值时等待的时间（秒），如果没有在指定的时间内输入，就不再等待了。 参数：变量：指定读取值的变量名 函数 系统函数 basename：返回完整路径最后/的部分，常用于获取文件名 basename [pathname] [suffix] basename [string] [suffix] basename命令会删掉所有的前缀包括最后一个/ 选项：suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉 dirname：返回完整路径最后/的前面的部分，常用于返回路径部分 dirname 文件绝对路径：从给定的包含绝对路径的文件名中出去文件名（非目录部分），然后返回剩下的路径（目录部分） 反正两个系统函数都不要最后一个/ 自定义函数 基本语法： function funname() { Action; [return int;] } 调用直接写函数名：funname（不用写括号），然后在后面写参数 20.Shell编程综合案例 需求分析 每天凌晨2：10备份数据库atguiguDB到/data/backup/db 备份开始和备份结束能够给出相应的提示信息 备份后的文件要求以备份时间为文件名，并打包成.tar.gz的形式，比如：2018-03-12_230201.tar.gz 在备份的同时，检查是否有10天前备份的数据库文件，如果有就将其删除。 如果报错：mysqldump: command not found 解决方案： 先找到mysqldump的位置：find / -name mysqldump -print 然后建立一个链接：ln -fs /usr/local/mysql/bin/mysql /usr/bin crontab -e 10 2 * * * /usr/sbin/mysql_backup_db.sh 21.Python定制篇 开发平台Ubuntu 设置Ubuntu支持中文 su root显示认证失败：是因为我们还没有对root用户设置密码 给root用户设密码：sudo passwd 如果ubuntu没有vim：apt install vim apt软件管理和远程登录 apt：Advanced Packaging Tool，是一款安装包管理工具。在Ubuntu下，我们可以使用apt命令进行软件包的安装、删除、清理等。 常用命令： sudo apt-get update更新源sudo apt-get install package 安装包sudo apt-get remove package 删除包sudo apt-cache search package 搜索软件包sudo apt-cache show package获取包的相关信息,如说明、大小、版本等sudo apt-get install package –reinstall重新安装包 sudo apt-get -f install修复安装sudo apt-get remove package –purge 删除包,包括配置文件等sudo apt-get build-dep package 安装相关的编译环境 sudo apt-get upgrade 更新已安装的包sudo apt-get dist-upgrade 升级系统sudo apt-cache depends package 了解使用该包依赖那些包sudo apt-cache rdepends package 查看该包被哪些包依赖sudo apt-get source package下载该包的源代码 更新Ubuntu软件下载地址 查看Ubuntu版本：cat /proc/version 需要修改的文件位置：/etc/apt/source.list Windows使用SSH远程登录Ubuntu 安装SSH：sudo apt-get install openssh-server 启用SSH：service sshd start Linux使用SSH远程登录Ubuntu 同上 基本语法：ssh 用户名@IP 例如：ssh atguigu@192.168.188.130 使用shh访问，如访问出现错误。可查看是否有该文件 ~/.ssh/known_ssh，尝试删除该文件解决。 登出：exit或者logout","categories":[{"name":"linux","slug":"linux","permalink":"https://13592491893.github.io/categories/linux/"}],"tags":[{"name":"命令","slug":"命令","permalink":"https://13592491893.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"尚硅谷","slug":"尚硅谷","permalink":"https://13592491893.github.io/tags/%E5%B0%9A%E7%A1%85%E8%B0%B7/"},{"name":"linux","slug":"linux","permalink":"https://13592491893.github.io/tags/linux/"}]},{"title":"线程面试题","slug":"线程面试题","date":"2021-11-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/e7c06c22.html","link":"","permalink":"https://13592491893.github.io/posts/e7c06c22.html","excerpt":"","text":"线程1) 什么是线程？线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对 运算密集型任务提速。比如，如果一个线程完成一个任务要100毫秒，那么用十个线程完成改任务只需10毫秒。Java在语言层面对多线程提供了卓越的支 持，它也是一个很好的卖点。 2) 线程和进程有什么区别？线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。 3) 如何在Java中实现线程？在语言层面有三种方式。java.lang.Thread 类的实例就是一个线程但是它需要调用java.lang.Runnable接口来执行，由于线程类本身就是调用的Runnable接口所以你可以继承 java.lang.Thread 类或者直接调用Runnable接口来重写run()方法实现线程。第三种 实现Callable&lt;&gt;接口并重写call方法。 4) 用Runnable还是Thread？这个问题是上题的后续，大家都知道我们可以通过继承Thread类或者调用Runnable接口来实现线程，问题是，那个方法更好呢？什么情况下使 用它？这个问题很容易回答，如果你知道Java不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是调用Runnable接口好 了。 6) Thread 类中的start() 和 run() 方法有什么区别？这个问题经常被问到，但还是能从此区分出面试者对Java线程模型的理解程度。start()方法被用来启动新创建的线程，而且start()内部 调用了run()方法，这和直接调用run()方法的效果不一样。当你调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启 动，start()方法才会启动新线程。 7) Java中Runnable和Callable有什么不同？Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在 JDK1.5增加的。它们的主要区别是Callable的 call() 方法可以返回值和抛出异常，而Runnable的run()方法没有这些功能。Callable可以返回装载有计算结果的Future对象。 8) Java中的volatile 变量是什么？volatile是一个特殊的修饰符，只有成员变量才能使用它。保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 11) 什么是线程安全？Vector是一个线程安全类吗？如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量 的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分 成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。 12) Java中notify 和 notifyAll有什么区别？这又是一个刁钻的问题，因为多线程可以等待单监控锁，Java API 的设计人员提供了一些方法当等待条件改变的时候通知它们，但是这些方法没有完全实现。notify()方法不能唤醒某个具体的线程，所以只有一个线程在等 待的时候它才有用武之地。而notifyAll()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 17) 为什么wait, notify 和 notifyAll这些方法不在thread类里面？这是个设计相关的问题，它考察的是面试者对现有系统和一些普遍存在但看起来不合理的事物的看法。回答这些问题的时候，你要说明为什么把这些方法放在 Object类里是有意义的，还有不把它放在Thread类里的原因。一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通 过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁 就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。 18) 什么是ThreadLocal变量？ThreadLocal使用场合主要解决多线程中数据数据因并发产生不一致问题。ThreadLocal为每个线程的中并发访问的数据提供一个副本，通过访问副本来运行业务，这样的结果是耗费了内存，单大大减少了线程同步所带来性能消耗，也减少了线程并发控制的复杂度。 ThreadLocal不能使用原子类型，只能使用Object类型。ThreadLocal的使用比synchronized要简单得多。 ThreadLocal和Synchonized都用于解决多线程并发访问。但是ThreadLocal与synchronized有本质的区别。synchronized是利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。而ThreadLocal为每一个线程都提供了变量的副本，使得每个线程在某一时间访问到的并不是同一个对象，这样就隔离了多个线程对数据的数据共享。而Synchronized却正好相反，它用于在多个线程间通信时能够获得数据共享。 19) 什么是FutureTask？在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完 成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包 装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。 20) Java中interrupted 和 isInterruptedd方法的区别？interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted()来 检查中断状态时，中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛 出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 21) 为什么wait和notify方法要在同步块中调用？主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。还有一个原因是为了避免wait和notify之间产生竞态条件。 22) 为什么你应该在循环中检查等待条件?处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来 时，不能认为它原来的等待状态仍然是有效的，在notify()方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用wait()方 法效果更好的原因，你可以在Eclipse中创建模板调用wait和notify试一试。如果你想了解更多关于这个问题的内容，我推荐你阅读《Effective Java》这本书中的线程和同步章节。 23) Java中的同步集合与并发集合有什么区别？同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在Java1.5之前程序员们只有同步集合来用且在 多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分 区等现代技术提高了可扩展性。 24） Java中堆和栈有什么不同？为什么把这个问题归类在多线程和并发面试题里？因为栈是一块和线程紧密相关的内存区域。每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈 调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己 的栈，如果多个线程使用该变量就可能引发问题，这时volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 25） 什么是线程池？ 为什么要使用它？创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时 候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从JDK1.5开始，Java API提供了Executor框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短 的任务的程序的可扩展线程池）。 26） 如何写代码来解决生产者消费者问题？在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比 较低级的办法是用wait和notify来解决这个问题，比较赞的办法是用Semaphore 或者 BlockingQueue来实现生产者消费者模型，这篇教程有实现它。 27） 如何避免死锁？Java多线程中的死锁死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。 28) Java中活锁和死锁有什么区别？这是上题的扩展，活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个 人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者 进程的状态可以改变但是却不能继续执行。 29） 怎么检测一个线程是否拥有锁？我一直不知道我们竟然可以检测一个线程是否拥有锁，直到我参加了一次电话面试。在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。 30) 你如何在Java中获取线程堆栈？对于不同的操作系统，有多种方法来获得Java进程的线程堆栈。当你获取线程堆栈时，JVM会把所有线程的状态存到日志文件或者输出到控制台。在 Windows你可以使用Ctrl + Break组合键来获取线程堆栈，Linux下用kill -3命令。你也可以用jstack这个工具来获取，它对线程id进行操作，你可以用jps这个工具找到id。 31) JVM中哪个参数是用来控制线程的栈堆栈小的这个问题很简单， -Xss参数用来控制线程的堆栈大小。 32） Java中synchronized 和 ReentrantLock 有什么不同？Java在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁 时不能中途取消等。Java 5 通过Lock接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。 33） 有三个线程T1，T2，T3，怎么确保它们按顺序执行？在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。 34) Thread类中的yield方法有什么作用？Yield方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，执行yield()的线程有可能在进入到暂停状态后马上又被执行。 35） Java中ConcurrentHashMap的并发度是什么？ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap类构造函数的一个可选参数，默认值为16，这样在多线程情况下就能避免争用。 36） Java中Semaphore是什么？Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前 会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采 取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 37）如果你提交任务时，线程池队列已满。会时发会生什么？这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常。 38) Java线程池中submit() 和 execute()方法有什么区别？两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中, 而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线 程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。 39) 什么是阻塞式方法？阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept()方法就是一直等待客户端连接。这里的阻塞是 指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 40) Swing是线程安全的吗？ 为什么？你可以很肯定的给出回答，Swing不是线程安全的，但是你应该解释这么回答的原因即便面试官没有问你为什么。当我们说swing不是线程安全的常 常提到它的组件，这些组件不能在多线程中进行修改，所有对GUI组件的更新都要在AWT线程中完成，而Swing提供了同步和异步两种回调方法来进行更 新。 41） Java中invokeAndWait 和 invokeLater有什么区别？这两个方法是Swing API 提供给Java开发者用来从当前线程而不是事件派发线程更新GUI组件用的。InvokeAndWait()同步更新GUI组件，比如一个进度条，一旦进 度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用invokeAndWait()方法请求事件派发线程对组件进行相应更新。而 invokeLater()方法是异步调用更新组件的。 42) Swing API中那些方法是线程安全的？这个问题又提到了swing和线程安全，虽然组件不是线程安全的但是有一些方法是可以被多线程安全调用的，比如repaint(), revalidate()。 JTextComponent的setText()方法和JTextArea的insert() 和 append() 方法也是线程安全的。 43) 如何在Java中创建Immutable对象？这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java 没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员 声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。我的文章how to make an object Immutable in Java有详细的教程，看完你可以充满自信。 44） Java中的ReadWriteLock是什么？一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java中的ReadWriteLock是Java 5 中新增的一个接口，一个ReadWriteLock维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程 持有。写锁是独占的，你可以使用JDK中的ReentrantReadWriteLock来实现这个规则，它最多支持65535个写锁和65535个读 锁。 45) 多线程中的忙循环是什么?忙循环就是程序员用循环让一个线程等待，不像传统方法wait(), sleep() 或 yield() 它们都放弃了CPU控制，而忙循环不会放弃CPU，它就是在运行一个空循环。这么做的目的是为了保留CPU缓存，在多核系统中，一个等待线程醒来的时候可 能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。 46）volatile 变量和 atomic 变量有什么不同？这是个有趣的问题。首先，volatile 变量和 atomic 变量看起来很像，但功能却不一样。Volatile变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用volatile修饰count变量那么 count++ 操作就不是原子性的。而AtomicInteger类提供的atomic方法可以让这种操作具有原子性如getAndIncrement()方法会原子性 的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 47) 如果同步块内的线程抛出异常会发生什么？这个问题坑了很多Java程序员，若你能想到锁是否释放这条线索来回答还有点希望答对。无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 48） 单例模式的双检锁是什么？这个问题在Java面试中经常被问到，但是面试官对回答此问题的满意度仅为50%。一半的人写不出双检锁还有一半的人说不出它的隐患和 Java1.5是如何对它修正的。它其实是一个用来创建线程安全的单例的老方法，当单例实例第一次被创建时它试图用单个锁进行性能优化，但是由于太过于复 杂在JDK1.4中它是失败的，我个人也不喜欢它。无论如何，即便你也不喜欢它但是还是要了解一下，因为它经常被问到。 49） 如何在Java中创建线程安全的Singleton？这是上面那个问题的后续，如果你不喜欢双检锁而面试官问了创建Singleton类的替代方法，你可以利用JVM的类加载和静态变量初始化特征来创建Singleton实例，或者是利用枚举类型来创建Singleton，我很喜欢用这种方法。 50) 写出3条你遵循的多线程最佳实践这种问题我最喜欢了，我相信你在写并发代码来提升性能的时候也会遵循某些最佳实践。以下三条最佳实践我觉得大多数Java程序员都应该遵循： 给你的线程起个有意义的名字。这样可以方便找bug或追踪。OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。 避免锁定和缩小同步的范围锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。 多用同步类少用wait 和 notify首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断 优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。 多用并发集合少用同步集合这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到map，你应该首先想到用ConcurrentHashMap。 51) 如何强制启动一个线程？这个问题就像是如何强制进行Java垃圾回收，目前还没有觉得方法，虽然你可以使用System.gc()来进行垃圾回收，但是不保证能成功。在Java里面没有办法强制启动一个线程，它是被线程调度器控制着且Java没有公布相关的API。 52) Java中的fork join框架是什么？fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块 设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。 53） Java多线程中调用wait() 和 sleep()方法有什么不同？Java程序中wait 和 sleep都会造成某种形式的暂停，它们可以满足不同的需要。wait()方法用于线程间通信，如果等待条件为真且其它线程被唤醒时它会释放锁，而 sleep()方法仅仅释放CPU资源或者让当前线程停止执行一段时间，但不会释放锁。","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"},{"name":"thread","slug":"java/thread","permalink":"https://13592491893.github.io/categories/java/thread/"}],"tags":[{"name":"命令","slug":"命令","permalink":"https://13592491893.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"thread","slug":"thread","permalink":"https://13592491893.github.io/tags/thread/"}]},{"title":"git学习","slug":"git学习","date":"2021-11-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/e28ceec1.html","link":"","permalink":"https://13592491893.github.io/posts/e28ceec1.html","excerpt":"","text":"git学习1.git init初始化本地库 2.git status查看本地库状态 3.git add ttt.txt把本地文件新增到暂存区 4.https://learngitbranching.js.org/?locale=zh_CN","categories":[{"name":"git","slug":"git","permalink":"https://13592491893.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://13592491893.github.io/tags/git/"},{"name":"命令","slug":"命令","permalink":"https://13592491893.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"尚硅谷","slug":"尚硅谷","permalink":"https://13592491893.github.io/tags/%E5%B0%9A%E7%A1%85%E8%B0%B7/"}]},{"title":"Markdown快速入门小技巧(hexo博客文章--格式用法)","slug":"03-Markdown快速入门小技巧","date":"2021-11-22T16:00:00.000Z","updated":"2021-12-29T03:19:25.623Z","comments":true,"path":"posts/15546.html","link":"","permalink":"https://13592491893.github.io/posts/15546.html","excerpt":"","text":"相遇皆是缘分 Markdown 的快速入门(后缀是 .md)快捷键ctrl+shift+1 大纲显示 ctrl+/ 源代码显示 代码块：​```java(html等等) 会自动提示 标题#标题1 （大） ##标题2 ###标题3 ####标题4 （小） 以此类推 最高标题6 加粗//加粗 **加粗** //代码高亮显示 ==高亮== //删除线 ~~删除线~~ //斜体 *斜体内容* 引用：//引用语法 &gt;作者：泽 &gt;&gt;作者：泽 &gt;&gt;&gt;作者：泽 作者：泽 作者：泽 作者：泽 分割线//分割线 --- *** 图片插入//在线图片与本地图片 ![照片名子]（/image/me.png） 超链接//超链接语法 [超链接名字]（https://gihub.com/yerenping） 我的天空 列表//无需列表 - 目录1 -后加空格 - 目录2 - 目录3 //有序列表 1+. +名称 表格右键》插入》表格 用代码过于复杂不推荐使用 姓名 数字 语文 小王 85 21","categories":[{"name":"hexo博客","slug":"hexo博客","permalink":"https://13592491893.github.io/categories/hexo%E5%8D%9A%E5%AE%A2/"},{"name":"Markdown","slug":"hexo博客/Markdown","permalink":"https://13592491893.github.io/categories/hexo%E5%8D%9A%E5%AE%A2/Markdown/"}],"tags":[{"name":"hexo博客","slug":"hexo博客","permalink":"https://13592491893.github.io/tags/hexo%E5%8D%9A%E5%AE%A2/"},{"name":"Markdown","slug":"Markdown","permalink":"https://13592491893.github.io/tags/Markdown/"}]},{"title":"session学习","slug":"session学习","date":"2021-10-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/39104.html","link":"","permalink":"https://13592491893.github.io/posts/39104.html","excerpt":"","text":"session学习package com.bjsxt.servlet; import java.io.IOException; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import javax.servlet.http.HttpSession; /** * session技术学习: * 问题： * 一个用户的不同请求处理的数据共享怎么办？ * 解决： * 使用session技术 * 原理： * 用户第一次访问服务器，服务器会创建一个session对象给此用户，并将 * 该session对象的JSESSIONID使用Cookie技术存储到浏览器中，保证 * 用户的其他请求能够获取到同一个session对象，也保证了不同请求能够获取到 * 共享的数据。 * 特点： * 存储在服务器端 * 服务器进行创建 * 依赖Cookie技术 * 一次会话 * 默认存储时间是30分钟 * 作用： * 解决了一个用户不同请求处理的数据共享问题 * 使用： * 创建session对象/获取session对象 HttpSession hs=req.getSession(); 如果请求中拥有session的标识符也就是JSESSIONID，则返回其对应的session队形 如果请求中没有session的标识符也就是JSESSIONID，则创建新的session对象，并将其JSESSIONID作为从cookie数据存储到浏览器内存中 * 如果session对象是失效了，也会重新创建一个session对象，并将其JSESSIONID存储在浏览器内存中。 * 设置session存储时间 * hs.setMaxInactiveInterval(int seconds); * 注意： * 在指定的时间内session对象没有被使用则销毁，如果使用了则重新计时。 * 设置session强制失效 * hs.invalidate(); * 存储和获取数据 * 存储：hs.setAttribute(String name,Object value); * 获取：hs.getAttribute(String name) 返回的数据类型为Object * 注意： * 存储的动作和取出的动作发生在不同的请求中，但是存储要先于取出执行。 * 使用时机: * 一般用户在登陆web项目时会将用户的个人信息存储到Sesion中，供该用户的其他请求使用。 * 总结： * session解决了一个用户的不同请求的数据共享问题，只要在JSESSIONID不失效和session对象不失效的情况下。 * 用户的任意请求在处理时都能获取到同一个session对象。 * 作用域： * 一次会话 * 在JSESSIONID和SESSION对象不失效的情况下为整个项目内。 * session失效处理： * 将用户请求中的JSESSIONID和后台获取到的SESSION对象的JSESSIONID进行比对，如果一致 * 则session没有失效，如果不一致则证明session失效了。重定向到登录页面，让用户重新登录。 * 注意： * JSESSIONID存储在了Cookie的临时存储空间中，浏览器关闭即失效。 * * @author MyPC * */ public class SessionServlet extends HttpServlet { @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { //设置请求编码格式 req.setCharacterEncoding(\"utf-8\"); //设置响应编码格式 resp.setContentType(\"text/html;charset=utf-8\"); //获取请求信息 String name=\"张三\"; //处理请求信息 //创建session对象 HttpSession hs=req.getSession(); //设置session的存储时间 //hs.setMaxInactiveInterval(5); System.out.println(hs.getId()); //设置session强制失效 //hs.invalidate(); //存储数据 hs.setAttribute(\"name\",name); //响应处理结果 //直接响应 resp.getWriter().write(\"session学习\"); //请求转发 //重定向 } }","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"session","slug":"session","permalink":"https://13592491893.github.io/tags/session/"},{"name":"面试","slug":"面试","permalink":"https://13592491893.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"RPC与Http的区别","slug":"RPC与Http的区别","date":"2021-09-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/33119.html","link":"","permalink":"https://13592491893.github.io/posts/33119.html","excerpt":"","text":"RPC与Http的区别一.远程调用方式无论是微服务还是分布式服务（都是SOA，都是面向服务编程），都面临着服务间的远程调用。那么服务间的远程调用方式有哪些呢？ 常见的远程调用方式有以下几种： RPC：Remote Produce Call远程过程调用，类似的还有RMI（Remote Methods Invoke 远程方法调用，是JAVA中的概念，是JAVA十三大技术之一）。自定义数据格式，基于原生TCP通信，速度快，效率高。早期的webservice，现在热门的dubbo，都是RPC的典型 RPC的框架：webservie(cxf)、dubbo RMI的框架：hessian Http：http其实是一种网络传输协议，基于TCP，规定了数据传输的格式。现在客户端浏览器与服务端通信基本都是采用Http协议。也可以用来进行远程服务调用。缺点是消息封装臃肿。 现在热门的Rest风格，就可以通过http协议来实现。 http的实现技术：HttpClient 相同点：底层通讯都是基于socket，都可以实现远程调用，都可以实现服务调用服务 不同点：RPC：框架有：dubbo、cxf、（RMI远程方法调用）Hessian当使用RPC框架实现服务间调用的时候，要求服务提供方和服务消费方 都必须使用统一的RPC框架，要么都dubbo，要么都cxf 跨操作系统在同一编程语言内使用优势：调用快、处理快 http：框架有：httpClient当使用http进行服务间调用的时候，无需关注服务提供方使用的编程语言，也无需关注服务消费方使用的编程语言，服务提供方只需要提供restful风格的接口，服务消费方，按照restful的原则，请求服务，即可 跨系统跨编程语言的远程调用框架优势：通用性强 总结：对比RPC和http的区别1 RPC要求服务提供方和服务调用方都需要使用相同的技术，要么都hessian，要么都dubbo而http无需关注语言的实现，只需要遵循rest规范2 RPC的开发要求较多，像Hessian框架还需要服务器提供完整的接口代码(包名.类名.方法名必须完全一致)，否则客户端无法运行3 Hessian只支持POST请求4 Hessian只支持JAVA语言 1.1.认识RPC RPC，即 Remote Procedure Call（远程过程调用），是一个计算机通信协议。 该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。说得通俗一点就是：A计算机提供一个服务，B计算机可以像调用本地服务那样调用A计算机的服务。 通过上面的概念，我们可以知道，实现RPC主要是做到两点： 实现远程调用其他计算机的服务 要实现远程调用，肯定是通过网络传输数据。A程序提供服务，B程序通过网络将请求参数传递给A，A本地执行后得到结果，再将结果返回给B程序。这里需要关注的有两点： 1）采用何种网络通讯协议？ 现在比较流行的RPC框架，都会采用TCP作为底层传输协议 2）数据传输的格式怎样？ 两个程序进行通讯，必须约定好数据传输格式。就好比两个人聊天，要用同一种语言，否则无法沟通。所以，我们必须定义好请求和响应的格式。另外，数据在网路中传输需要进行序列化，所以还需要约定统一的序列化的方式。 像调用本地服务一样调用远程服务 如果仅仅是远程调用，还不算是RPC，因为RPC强调的是过程调用，调用的过程对用户而言是应该是透明的，用户不应该关心调用的细节，可以像调用本地服务一样调用远程服务。所以RPC一定要对调用的过程进行封装 RPC调用流程图： 1.2.认识Http Http协议：超文本传输协议，是一种应用层协议。规定了网络传输的请求格式、响应格式、资源定位和操作的方式等。但是底层采用什么网络传输协议，并没有规定，不过现在都是采用TCP协议作为底层传输协议。说到这里，大家可能觉得，Http与RPC的远程调用非常像，都是按照某种规定好的数据格式进行网络通信，有请求，有响应。没错，在这点来看，两者非常相似，但是还是有一些细微差别。 RPC并没有规定数据传输格式，这个格式可以任意指定，不同的RPC协议，数据格式不一定相同。 Http中还定义了资源定位的路径，RPC中并不需要 最重要的一点：RPC需要满足像调用本地服务一样调用远程服务，也就是对调用过程在API层面进行封装。Http协议没有这样的要求，因此请求、响应等细节需要我们自己去实现。 优点：RPC方式更加透明，对用户更方便。Http方式更灵活，没有规定API和语言，跨语言、跨平台 缺点：RPC方式需要在API层面进行封装，限制了开发的语言环境。 例如我们通过浏览器访问网站，就是通过Http协议。只不过浏览器把请求封装，发起请求以及接收响应，解析响应的事情都帮我们做了。如果是不通过浏览器，那么这些事情都需要自己去完成。 1.3.如何选择？ 既然两种方式都可以实现远程调用，我们该如何选择呢？ 速度来看，RPC要比http更快，虽然底层都是TCP，但是http协议的信息往往比较臃肿 难度来看，RPC实现较为复杂，http相对比较简单 灵活性来看，http更胜一筹，因为它不关心实现细节，跨平台、跨语言。 因此，两者都有不同的使用场景： 如果对效率要求更高，并且开发过程使用统一的技术栈，那么用RPC还是不错的。 如果需要更加灵活，跨语言、跨平台，显然http更合适 那么我们该怎么选择呢？ 微服务，更加强调的是独立、自治、灵活。而RPC方式的限制较多，因此微服务框架中，一般都会采用基于Http的Rest风格服务。 1.4.Http客户端工具 既然微服务选择了Http，那么我们就需要考虑自己来实现对请求和响应的处理。不过开源世界已经有很多的http客户端工具，能够帮助我们做这些事情，例如： HttpClient OKHttp URLConnection 接下来，我们就一起了解一款比较流行的客户端工具：HttpClient public void testGet() throws IOException { HttpGet request = new HttpGet(\"http://www.baidu.com\"); String response = this.httpClient.execute(request, new BasicResponseHandler()); System.out.println(response); } public void testPost() throws IOException { HttpPost request = new HttpPost(\"http://www.oschina.net/\"); request.setHeader(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"); String response = this.httpClient.execute(request, new BasicResponseHandler()); System.out.println(response); } HttpClient请求数据后是json字符串，需要我们自己把Json字符串反序列化为对象，我们会使用JacksonJson工具来实现。 JacksonJson是SpringMVC内置的json处理工具，其中有一个ObjectMapper类，可以方便的实现对json的处理： #### 对象转json // json处理工具 private ObjectMapper mapper = new ObjectMapper(); @Test public void testJson() throws JsonProcessingException { User user = new User(); user.setId(8L); user.setAge(21); user.setName(\"柳岩\"); user.setUserName(\"liuyan\"); // 序列化 String json = mapper.writeValueAsString(user); System.out.println(\"json = \" + json); } #### json转对象 ​```java // json处理工具 private ObjectMapper mapper = new ObjectMapper(); @Test public void testJson() throws IOException { User user = new User(); user.setId(8L); user.setAge(21); user.setName(\"柳岩\"); user.setUserName(\"liuyan\"); // 序列化 String json = mapper.writeValueAsString(user); // 反序列化，接收两个参数：json数据，反序列化的目标类字节码 User result = mapper.readValue(json, User.class); System.out.println(\"result = \" + result); } #### json转集合 json转集合比较麻烦，因为你无法同时把集合的class和元素的class同时传递到一个参数。 因此Jackson做了一个类型工厂，用来解决这个问题： ​```java // json处理工具 private ObjectMapper mapper = new ObjectMapper(); @Test public void testJson() throws IOException { User user = new User(); user.setId(8L); user.setAge(21); user.setName(\"柳岩\"); user.setUserName(\"liuyan\"); // 序列化,得到对象集合的json字符串 String json = mapper.writeValueAsString(Arrays.asList(user, user)); // 反序列化，接收两个参数：json数据，反序列化的目标类字节码 List&lt;User&gt; users = mapper.readValue(json, mapper.getTypeFactory().constructCollectionType(List.class, User.class)); for (User u : users) { System.out.println(\"u = \" + u); } } #### json转任意复杂类型 当对象泛型关系复杂时，类型工厂也不好使了。这个时候Jackson提供了TypeReference来接收类型泛型，然后底层通过反射来获取泛型上的具体类型。实现数据转换。 ​```java // json处理工具 private ObjectMapper mapper = new ObjectMapper(); @Test public void testJson() throws IOException { User user = new User(); user.setId(8L); user.setAge(21); user.setName(\"柳岩\"); user.setUserName(\"liuyan\"); // 序列化,得到对象集合的json字符串 String json = mapper.writeValueAsString(Arrays.asList(user, user)); // 反序列化，接收两个参数：json数据，反序列化的目标类字节码 List&lt;User&gt; users = mapper.readValue(json, new TypeReference&lt;List&lt;User&gt;&gt;(){}); for (User u : users) { System.out.println(\"u = \" + u); } } Spring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： - HttpClient - OkHttp - JDK原生的URLConnection（默认的） 首先在项目中注册一个`RestTemplate`对象，可以在启动类位置注册： ​```java @SpringBootApplication public class HttpDemoApplication { public static void main(String[] args) { SpringApplication.run(HttpDemoApplication.class, args); } @Bean public RestTemplate restTemplate() { // 默认的RestTemplate，底层是走JDK的URLConnection方式。 return new RestTemplate(); } } ​``` 在测试类中直接`@Autowired`注入： ​```java @RunWith(SpringRunner.class) @SpringBootTest(classes = HttpDemoApplication.class) public class HttpDemoApplicationTests { @Autowired private RestTemplate restTemplate; @Test public void httpGet() { User user = this.restTemplate.getForObject(\"http://localhost/hello\", User.class); System.out.println(user); } } ​``` - 通过RestTemplate的getForObject()方法，传递url地址及实体类的字节码，RestTemplate会自动发起请求，接收响应，并且帮我们对响应结果进行反序列化。","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"rpc","slug":"rpc","permalink":"https://13592491893.github.io/tags/rpc/"},{"name":"http","slug":"http","permalink":"https://13592491893.github.io/tags/http/"},{"name":"远程调用","slug":"远程调用","permalink":"https://13592491893.github.io/tags/%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"}]},{"title":"redis","slug":"Redis主从复制原理总结","date":"2021-08-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/64786.html","link":"","permalink":"https://13592491893.github.io/posts/64786.html","excerpt":"","text":"Redis主从复制原理总结和Mysql主从复制的原因一样，Redis虽然读取写入的速度都特别快，但是也会产生读压力特别大的情况。为了分担读压力，Redis支持主从复制，Redis的主从结构可以采用一主多从或者级联结构，Redis主从复制可以根据是否是全量分为全量同步和增量同步。下图为级联结构。 1.全量同步Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。具体步骤如下： 从服务器连接主服务器，发送SYNC命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令； 完成上面几个步骤后就完成了从服务器数据初始化的所有操作，从服务器此时可以接收来自用户的读请求。 2.增量同步Redis增量复制是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令。 3.Redis主从同步策略主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。 4.注意点如果多个Slave断线了，需要重启的时候，因为只要Slave启动，就会发送sync请求和主机全量同步，当多个同时出现的时候，可能会导致Master IO剧增宕机。 Redis主从复制的配置十分简单，它可以使从服务器是主服务器的完全拷贝。需要清除Redis主从复制的几点重要内容：Redis使用异步复制。但从Redis 2.8开始，从服务器会周期性的应答从复制流中处理的数据量。 一个主服务器可以有多个从服务器。 从服务器也可以接受其他从服务器的连接。除了多个从服务器连接到一个主服务器之外，多个从服务器也可以连接到一个从服务器上，形成一个图状结构。 Redis主从复制不阻塞主服务器端。也就是说当若干个从服务器在进行初始同步时，主服务器仍然可以处理请求。 主从复制也不阻塞从服务器端。当从服务器进行初始同步时，它使用旧版本的数据来应对查询请求，假设你在redis.conf配置文件是这么配置的。否则的话，你可以配置当复制流关闭时让从服务器给客户端返回一个错误。但是，当初始同步完成后，需要删除旧的数据集和加载新的数据集，在这个短暂的时间内，从服务器会阻塞连接进来的请求。 主从复制可以用来增强扩展性，使用多个从服务器来处理只读的请求（比如，繁重的排序操作可以放到从服务器去做），也可以简单的用来做数据冗余。 使用主从复制可以为主服务器免除把数据写入磁盘的消耗：在主服务器的redis.conf文件中配置“避免保存”（注释掉所有“保存“命令），然后连接一个配置为“进行保存”的从服务器即可。但是这个配置要确保主服务器不会自动重启（要获得更多信息请阅读下一段） 5.主从复制的一些特点： 采用异步复制； 一个主redis可以含有多个从redis； 每个从redis可以接收来自其他从redis服务器的连接； 主从复制对于主redis服务器来说是非阻塞的，这意味着当从服务器在进行主从复制同步过程中，主redis仍然可以处理外界的访问请求； 主从复制对于从redis服务器来说也是非阻塞的，这意味着，即使从redis在进行主从复制过程中也可以接受外界的查询请求，只不过这时候从redis返回的是以前老的数据，如果你不想这样，那么在启动redis时，可以在配置文件中进行设置，那么从redis在复制同步过程中来自外界的查询请求都会返回错误给客户端；（虽然说主从复制过程中对于从redis是非阻塞的，但是当从redis从主redis同步过来最新的数据后还需要将新数据加载到内存中，在加载到内存的过程中是阻塞的，在这段时间内的请求将会被阻，但是即使对于大数据集，加载到内存的时间也是比较多的）； 主从复制提高了redis服务的扩展性，避免单个redis服务器的读写访问压力过大的问题，同时也可以给为数据备份及冗余提供一种解决方案； 为了编码主redis服务器写磁盘压力带来的开销，可以配置让主redis不再将数据持久化到磁盘，而是通过连接让一个配置的从redis服务器及时的将相关数据持久化到磁盘，不过这样会存在一个问题，就是主redis服务器一旦重启，因为主redis服务器数据为空，这时候通过主从同步可能导致从redis服务器上的数据也被清空； 6.Redis大概主从同步是怎么实现的？全量同步：master服务器会开启一个后台进程用于将redis中的数据生成一个rdb文件，与此同时，服务器会缓存所有接收到的来自客户端的写命令（包含增、删、改），当后台保存进程处理完毕后，会将该rdb文件传递给slave服务器，而slave服务器会将rdb文件保存在磁盘并通过读取该文件将数据加载到内存，在此之后master服务器会将在此期间缓存的命令通过redis传输协议发送给slave服务器，然后slave服务器将这些命令依次作用于自己本地的数据集上最终达到数据的一致性。 部分同步：从redis 2.8版本以前，并不支持部分同步，当主从服务器之间的连接断掉之后，master服务器和slave服务器之间都是进行全量数据同步，但是从redis 2.8开始，即使主从连接中途断掉，也不需要进行全量同步，因为从这个版本开始融入了部分同步的概念。部分同步的实现依赖于在master服务器内存中给每个slave服务器维护了一份同步日志和同步标识，每个slave服务器在跟master服务器进行同步时都会携带自己的同步标识和上次同步的最后位置。当主从连接断掉之后，slave服务器隔断时间（默认1s）主动尝试和master服务器进行连接，如果从服务器携带的偏移量标识还在master服务器上的同步备份日志中，那么就从slave发送的偏移量开始继续上次的同步操作，如果slave发送的偏移量已经不再master的同步备份日志中（可能由于主从之间断掉的时间比较长或者在断掉的短暂时间内master服务器接收到大量的写操作），则必须进行一次全量更新。在部分同步过程中，master会将本地记录的同步备份日志中记录的指令依次发送给slave服务器从而达到数据一致。 7.主从同步中需要注意几个问题 在上面的全量同步过程中，master会将数据保存在rdb文件中然后发送给slave服务器，但是如果master上的磁盘空间有效怎么办呢？那么此时全部同步对于master来说将是一份十分有压力的操作了。此时可以通过无盘复制来达到目的，由master直接开启一个socket将rdb文件发送给slave服务器。（无盘复制一般应用在磁盘空间有限但是网络状态良好的情况下） 主从复制结构，一般slave服务器不能进行写操作，但是这不是死的，之所以这样是为了更容易的保证主和各个从之间数据的一致性，如果slave服务器上数据进行了修改，那么要保证所有主从服务器都能一致，可能在结构上和处理逻辑上更为复杂。不过你也可以通过配置文件让从服务器支持写操作。（不过所带来的影响还得自己承担哦。。。） 主从服务器之间会定期进行通话，但是如果master上设置了密码，那么如果不给slave设置密码就会导致slave不能跟master进行任何操作，所以如果你的master服务器上有密码，那么也给slave相应的设置一下密码吧（通过设置配置文件中的masterauth） 关于slave服务器上过期键的处理，由master服务器负责键的过期删除处理，然后将相关删除命令已数据同步的方式同步给slave服务器，slave服务器根据删除命令删除本地的key。 8.当主服务器不进行持久化时复制的安全性在进行主从复制设置时，强烈建议在主服务器上开启持久化，当不能这么做时，比如考虑到延迟的问题，应该将实例配置为避免自动重启。为什么不持久化的主服务器自动重启非常危险呢？为了更好的理解这个问题，看下面这个失败的例子，其中主服务器和从服务器中数据库都被删除了。设置节点A为主服务器，关闭持久化，节点B和C从节点A复制数据。这时出现了一个崩溃，但Redis具有自动重启系统，重启了进程，因为关闭了持久化，节点重启后只有一个空的数据集。节点B和C从节点A进行复制，现在节点A是空的，所以节点B和C上的复制数据也会被删除。当在高可用系统中使用Redis Sentinel，关闭了主服务器的持久化，并且允许自动重启，这种情况是很危险的。比如主服务器可能在很短的时间就完成了重启，以至于Sentinel都无法检测到这次失败，那么上面说的这种失败的情况就发生了。如果数据比较重要，并且在使用主从复制时关闭了主服务器持久化功能的场景中，都应该禁止实例自动重启。 9.Redis主从复制是如何工作的如果设置了一个从服务器，在连接时它发送了一个SYNC命令，不管它是第一次连接还是再次连接都没有关系。 然后主服务器开始后台存储，并且开始缓存新连接进来的修改数据的命令。当后台存储完成后，主服务器把数据文件发送到从服务器，从服务器将其保存在磁盘上，然后加载到内存中。然后主服务器把刚才缓存的命令发送到从服务器。这是作为命令流来完成的，并且和Redis协议本身格式相同。你可以通过telnet自己尝试一下。在Redis服务器工作时连接到Redis端口，发送SYNC命令，会看到一个批量的传输，并且主服务器接收的每一个命令都会通过telnet会话重新发送一遍。当主从服务器之间的连接由于某些原因断开时，从服务器可以自动进行重连接。当有多个从服务器同时请求同步时，主服务器只进行一个后台存储。当连接断开又重新连上之后，一般都会进行一个完整的重新同步，但是从Redis2.8开始，只重新同步一部分也可以。 10.部分重新同步从Redis 2.8开始，如果遭遇连接断开，重新连接之后可以从中断处继续进行复制，而不必重新同步。 它的工作原理是这样：主服务器端为复制流维护一个内存缓冲区（in-memory backlog）。主从服务器都维护一个复制偏移量（replication offset）和master runid，当连接断开时，从服务器会重新连接上主服务器，然后请求继续复制，假如主从服务器的两个master run id相同，并且指定的偏移量在内存缓冲区中还有效，复制就会从上次中断的点开始继续。如果其中一个条件不满足，就会进行完全重新同步（在2.8版本之前就是直接进行完全重新同步）。因为主运行id不保存在磁盘中，如果从服务器重启了的话就只能进行完全同步了。部分重新同步这个新特性内部使用PSYNC命令，旧的实现中使用SYNC命令。Redis2.8版本可以检测出它所连接的服务器是否支持PSYNC命令，不支持的话使用SYNC命令。 11.无磁盘复制通常来讲，一个完全重新同步需要在磁盘上创建一个RDB文件，然后加载这个文件以便为从服务器发送数据。如果使用比较低速的磁盘，这种操作会给主服务器带来较大的压力。Redis从2.8.18版本开始尝试支持无磁盘的复制。使用这种设置时，子进程直接将RDB通过网络发送给从服务器，不使用磁盘作为中间存储。 12.配置主从复制的配置十分简单：把下面这行加入到从服务器的配置文件中即可。slaveof 192.168.1.1 6379 当然你需要把其中的192.168.1.1 6379替换为你自己的主服务器IP（或者主机名hostname）和端口。另外你可以调用SLAVEOF命令，主服务器就会开始与从服务器同步。关于部分重新同步，还有一些针对复制内存缓冲区的优化参数。查看Redis介质中的Redis.conf示例获得更多信息。使用repl-diskless-sync配置参数来启动无磁盘复制。使用repl-diskless-sync-delay 参数来配置传输开始的延迟时间，以便等待``更多的从服务器连接上来。查看Redis介质中的Redis.conf示例获得更多信息。 13.只读从服务器从Redis 2.6开始，从服务器支持只读模式，并且是默认模式。 这个行为是由Redis.conf文件中的slave-read-only 参数控制的，可以在运行中通过CONFIG SET来启用或者禁用。只读的从服务器会拒绝所有写命令，所以对从服务器不会有误写操作。但这不表示可以把从服务器实例暴露在危险的网络环境下，因为像DEBUG或者CONFIG这样的管理命令还是可以运行的。不过你可以通过使用rename-command命令来为这些命令改名来增加安全性。你可能想知道为什么只读限制还可以被还原，使得从服务器还可以进行写操作。虽然当主从服务器进行重新同步或者从服务器重启后，这些写操作都会失效，还是有一些使用场景会想从服务器中写入临时数据的，但将来这个特性可能会被去掉。 14.限制有N个以上从服务器才允许写入从Redis 2.8版本开始，可以配置主服务器连接N个以上从服务器才允许对主服务器进行写操作。但是，因为Redis使用的是异步主从复制，没办法确保从服务器确实收到了要写入的数据，所以还是有一定的数据丢失的可能性。这一特性的工作原理如下： 从服务器每秒钟ping一次主服务器，确认处理的复制流数量。 主服务器记住每个从服务器最近一次ping的时间。 用户可以配置最少要有N个服务器有小于M秒的确认延迟。 如果有N个以上从服务器，并且确认延迟小于M秒，主服务器接受写操作。还可以把这看做是CAP原则（一致性，可用性，分区容错性）不严格的一致性实现，虽然不能百分百确保一致性，但至少保证了丢失的数据不会超过M秒内的数据量。如果条件不满足，主服务器会拒绝写操作并返回一个错误。 min-slaves-to-write（最小从服务器数） min-slaves-max-lag（从服务器最大确认延迟） 15.通过redis实现服务器崩溃等数据恢复由于redis存储在内存中且提供一般编程语言常用的数据结构存储类型，所以经常被用于做服务器崩溃宕机的数据恢复处理。服务器可以在某些指定过程中将需要保存的数据以json对象等方式存储到redis中，也就是我们常说的快照，当服务器运行时读取redis来判断是否有待需要恢复数据继续处理的业务。当一次业务处理结束后再删除redis的数据即可。redis提供两种将内存数据导出到硬盘实现数据备份的方法： RDB方式(默认)RDB方式的持久化是通过快照（snapshotting）完成的，当符合一定条件时Redis会自动将内存中的所有数据进行快照并存储在硬盘上。进行快照的条件可以由用户在配置文件中自定义，由两个参数构成：时间和改动的键的个数。当在指定的时间内被更改的键的个数大于指定的数值时就会进行快照。RDB是redis默认采用的持久化方式，在配置文件中已经预置了3个条件：save 900 1 #900秒内有至少1个键被更改则进行快照save 300 10 #300秒内有至少10个键被更改则进行快照save 60 10000 #60秒内有至少10000个键被更改则进行快照 可以存在多个条件，条件之间是”或”的关系，只要满足其中一个条件，就会进行快照。 如果想要禁用自动快照，只需要将所有的save参数删除即可。Redis默认会将快照文件存储在当前目录(可CONFIG GET dir来查看)的dump.rdb文件中，可以通过配置dir和dbfilename两个参数分别指定快照文件的存储路径和文件名。 Redis实现快照的过程- Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）；- 父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；- 当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此一次快照操作完成。- 在执行fork的时候操作系统（类Unix操作系统）会使用写时复制（copy-on-write）策略，即fork函数发生的一刻父子进程共享同一内存数据，当父进程要更改其中某片数据时（如执行一个写命令 ），操作系统会将该片数据复制一份以保证子进程的数据不受影响，所以新的RDB文件存储的是执行fork一刻的内存数据。 Redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。这使得我们可以通过定时备份RDB文件来实 现Redis数据库备份。RDB文件是经过压缩（可以配置rdbcompression参数以禁用压缩节省CPU占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。 除了自动快照，还可以手动发送SAVE或BGSAVE命令让Redis执行快照，两个命令的区别在于，前者是由主进程进行快照操作，会阻塞住其他请求，后者会通过fork子进程进行快照操作。 Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。根据数据量大小与结构和服务器性能不同，这个时间也不同。通常将一个记录一千万个字符串类型键、大小为1GB的快照文件载入到内 存中需要花费20～30秒钟。 通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。这就需要开发者根据具体的应用场合，通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。 AOF方式默认情况下Redis没有开启AOF(append only file)方式的持久化，可以在redis.conf中通过appendonly参数开启： appendonly yes在启动时Redis会逐个执行AOF文件中的命令来将硬盘中的数据载入到内存中，载入的速度相较RDB会慢一些 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof，可以通过appendfilename参数修改： appendfilename appendonly.aof配置redis自动重写AOF文件的条件 auto-aof-rewrite-percentage 100 # 当目前的AOF文件大小超过上一次重写时的AOF文件大小的百分之多少时会再次进行重写，如果之前没有重写过，则以启动时的AOF文件大小为依据 auto-aof-rewrite-min-size 64mb # 允许重写的最小AOF文件大小配置写入AOF文件后，要求系统刷新硬盘缓存的机制 # appendfsync always # 每次执行写入都会执行同步，最安全也最慢appendfsync everysec # 每秒执行一次同步操作 # appendfsync no # 不主动进行同步操作，而是完全交由操作系统来做（即每30秒一次），最快也最不安全 Redis允许同时开启AOF和RDB，既保证了数据安全又使得进行备份等操作十分容易。此时重新启动Redis后Redis会使用AOF文件来恢复数据，因为AOF方式的持久化可能丢失的数据更少","categories":[{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/categories/sql/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://13592491893.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"redis","slug":"redis","permalink":"https://13592491893.github.io/tags/redis/"},{"name":"nosql","slug":"nosql","permalink":"https://13592491893.github.io/tags/nosql/"}]},{"title":"Quartz","slug":"Quartz","date":"2021-07-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/20121.html","link":"","permalink":"https://13592491893.github.io/posts/20121.html","excerpt":"","text":"Quartz搭配gulimall-shy项目中renren-fast模块理解 一、精进 Quartz—Quartz大致介绍:https://blog.csdn.net/u010648555/article/details/54863144 二、精进Quartz—Quartz简单入门Demohttps://blog.csdn.net/u010648555/article/details/54863394 三…","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"Quartz","slug":"Quartz","permalink":"https://13592491893.github.io/tags/Quartz/"},{"name":"定时任务","slug":"定时任务","permalink":"https://13592491893.github.io/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"job","slug":"job","permalink":"https://13592491893.github.io/tags/job/"}]},{"title":"注册中心zookeeper和eureka中的CP和AP","slug":"注册中心zookeeper和eureka中的CP和AP","date":"2021-07-12T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/40450.html","link":"","permalink":"https://13592491893.github.io/posts/40450.html","excerpt":"","text":"注册中心zookeeper和eureka中的CP和AP前言在分布式架构中往往伴随CAP的理论。因为分布式的架构，不再使用传统的单机架构，多机为了提供可靠服务所以需要冗余数据因而会存在分区容忍性Ｐ。 冗余数据的同时会在复制数据的同时伴随着可用性A 和强一致性C的问题。是选择停止可用性达到强一致性还是保留可用性选择最终一致性。通常选择后者。 其中 zookeeper 和 eureka分别是注册中心CP AP 的两种的实践。他们都提供服务注册中心的功能。建议使用AP。不强求数据的强一致性，达成数据的最终一致性。 服务注册中心的数据也就是返回的可用服务节点(ip+端口号) 服务A开了0-9十个服务节点，服务B需要调用服务A，两次查询返回0-8，1-9 不一致的数据。产生的影响就是0 和9 节点的负载不均衡 只要注册中心在 SLA 承诺的时间内（例如 1s 内）将数据收敛到一致状态（即满足最终一致），流量将很快趋于统计学意义上的一致，所以注册中心以最终一致的模型设计在生产实践中完全可以接受。 1 eureka APeureka 保证了可用性，实现最终一致性。 Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)，其中说明了，eureka是不满足强一致性，但还是会保证最终一致性 2 zookeeper CPzookeeper在选举leader时，会停止服务，直到选举成功之后才会再次对外提供服务，这个时候就说明了服务不可用，但是在选举成功之后，因为一主多从的结构，zookeeper在这时还是一个高可用注册中心，只是在优先保证一致性的前提下，zookeeper才会顾及到可用性 2.1 zookeeper 应用场景 感知消息队列异步操作后的结果 分布式锁 元数据 或者配置中心 如 dubbo 和Kafka 都需要zookeeper dubbo 也可以不使用zookeeper 采用直连提供的方式，但限制了分布式的拓展性。 HA高可用 主备切换 (两个服务分别为主备，备用平时不提供服务，当主的挂掉后，备用顶上作为新主。当原来的主恢复后作为新备) 选型依据： 在粗粒度分布式锁，分布式选主，主备高可用切换等不需要高 TPS 支持的场景下有不可替代的作用，而这些需求往往多集中在大数据、离线任务等相关的业务领域，因为大数据领域，讲究分割数据集，并且大部分时间分任务多进程 / 线程并行处理这些数据集，但是总是有一些点上需要将这些任务和进程统一协调，这时候就是 ZooKeeper 发挥巨大作用的用武之地。 但是在交易场景交易链路上，在主业务数据存取，大规模服务发现、大规模健康监测等方面有天然的短板，应该竭力避免在这些场景下引入 ZooKeeper，在阿里巴巴的生产实践中，应用对 ZooKeeper 申请使用的时候要进行严格的场景、容量、SLA 需求的评估。 所以可以使用 ZooKeeper，但是大数据请向左，而交易则向右，分布式协调向左，服务发现向右。 2.2 不建议使用zookeeper 的场景和原因不建议使用zookeeper 的原因是当它没满足A带来的影响。 当机房 3 出现网络分区 (Network Partitioned) 的时候，即机房 3 在网络上成了孤岛，我们知道虽然整体 ZooKeeper 服务是可用的，但是节点 ZK5 是不可写的，因为联系不上 Leader。 也就是说，这时候机房 3 的应用服务 svcB 是不可以新部署，重新启动，扩容或者缩容的，但是站在网络和服务调用的角度看，机房 3 的 svcA 虽然无法调用机房 1 和机房 2 的 svcB, 但是与机房 3 的 svcB 之间的网络明明是 OK 的啊，为什么不让我调用本机房的服务？ 现在因为注册中心自身为了保脑裂 (P) 下的数据一致性（C）而放弃了可用性，导致了同机房的服务之间出现了无法调用，这是绝对不允许的！可以说在实践中，注册中心不能因为自身的任何原因破坏服务之间本身的可连通性，这是注册中心设计应该遵循的铁律 2.3 zookeeper 的拓展ZooKeeper 的写并不是可扩展的，不可以通过加节点解决水平扩展性问题。 要想在 ZooKeeper 基础上硬着头皮解决服务规模的增长问题，一个实践中可以考虑的方法是想办法梳理业务，垂直划分业务域，将其划分到多个 ZooKeeper 注册中心，但是作为提供通用服务的平台机构组，因自己提供的服务能力不足要业务按照技术的指挥棒配合划分治理业务，真的可行么？ 而且这又违反了因为注册中心自身的原因（能力不足）破坏了服务的可连通性，举个简单的例子，1 个搜索业务，1 个地图业务，1 个大文娱业务，1 个游戏业务，他们之间的服务就应该老死不相往来么？也许今天是肯定的，那么明天呢，1 年后呢，10 年后呢？谁知道未来会要打通几个业务域去做什么奇葩的业务创新？注册中心作为基础服务，无法预料未来的时候当然不能妨碍业务服务对未来固有联通性的需求。 2.4 zookeeper 的持久化存储ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，以及宕机之后的数据可恢复，这是非常好的特性，但是我们要问，在服务发现场景中，其最核心的数据 - 实时的健康的服务的地址列表是不需要数据持久化的 需要持久化存储的地方在于一个完整的生产可用的注册中心，除了服务的实时地址列表以及实时的健康状态之外，还会存储一些服务的元数据信息，例如服务的版本，分组，所在的数据中心，权重，鉴权策略信息，service label 等元信息，这些数据需要持久化存储，并且注册中心应该提供对这些元信息的检索的能力。 2.5 容灾能力如果注册中心（Registry）本身完全宕机了，服务A 调用 服务B 链路应该受到影响么？ 是的，不应该受到影响。 服务调用（请求响应流）链路应该是弱依赖注册中心，必须仅在服务发布，机器上下线，服务扩缩容等必要时才依赖注册中心。 这需要注册中心仔细的设计自己提供的客户端，客户端中应该有针对注册中心服务完全不可用时做容灾的手段，例如设计客户端缓存数据机制（我们称之为 client snapshot）就是行之有效的手段。另外，注册中心的 health check 机制也要仔细设计以便在这种情况不会出现诸如推空等情况的出现。 ZooKeeper 的原生客户端并没有这种能力，所以利用 ZooKeeper 实现注册中心的时候我们一定要问自己，如果把 ZooKeeper 所有节点全干掉，你生产上的所有服务调用链路能不受任何影响么？而且应该定期就这一点做故障演练。 zookeeper 的健康检查使用 ZooKeeper 作为服务注册中心时，服务的健康检测常利用 ZooKeeper 的 Session 活性 Track 机制 以及结合 Ephemeral ZNode 的机制，简单而言，就是将服务的健康监测绑定在了 ZooKeeper 对于 Session 的健康监测上，或者说绑定在 TCP 长链接活性探测上了。 这在很多时候也会造成致命的问题，ZK 与服务提供者机器之间的 TCP 长链接活性探测正常的时候，该服务就是健康的么？答案当然是否定的！注册中心应该提供更丰富的健康监测方案，服务的健康与否的逻辑应该开放给服务提供方自己定义，而不是一刀切搞成了 TCP 活性检测！ 健康检测的一大基本设计原则就是尽可能真实的反馈服务本身的真实健康状态，否则一个不敢被服务调用者相信的健康状态判定结果还不如没有健康检测。","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://13592491893.github.io/tags/zookeeper/"},{"name":"eureka","slug":"eureka","permalink":"https://13592491893.github.io/tags/eureka/"},{"name":"cap","slug":"cap","permalink":"https://13592491893.github.io/tags/cap/"}]},{"title":"nginx","slug":"nginx","date":"2021-06-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/58122.html","link":"","permalink":"https://13592491893.github.io/posts/58122.html","excerpt":"","text":"nginx1.常用命令启动：start nginx 关闭：nginx -s stop 重启：nginx -s reload 关闭所有nginx进程： windows：taskkill /f /t /im nginx.exe linux: killall -9 nginx 或者 ps -A|grep nginx ​","categories":[{"name":"nginx","slug":"nginx","permalink":"https://13592491893.github.io/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://13592491893.github.io/tags/nginx/"},{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"指令","slug":"指令","permalink":"https://13592491893.github.io/tags/%E6%8C%87%E4%BB%A4/"}]},{"title":"mysql面试题","slug":"Mysql面试题","date":"2021-06-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/62688.html","link":"","permalink":"https://13592491893.github.io/posts/62688.html","excerpt":"","text":"mysql面试题作者：阿亮链接：https://zhuanlan.zhihu.com/p/116866170来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 一、数据库字段设计1、为什么要一定要设置主键?其实这个不是一定的，有些场景下，小系统或者没什么用的表，不设置主键也没关系，mysql最好是用自增主键，主要是以下两个原因：果定义了主键，那么InnoDB会选择主键作为聚集索引、如果没有显式定义主键，则innodb 会选择第一个不包含有NULL值的唯一索引作为主键索引、如果也没有这样的唯一索引，则innodb 会选择内置6字节长的ROWID作为隐含的聚集索引。所以，反正都要生成一个主键，那你还不如自己指定一个主键，提高查询效率！ 2、主键是用自增还是UUID?最好是用自增主键，主要是以下两个原因： 1. 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。 2. 如果使用非自增主键（如uuid），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到索引页的随机某个位置，此时MySQL为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成索引碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 不过，也不是所有的场景下都得使用自增主键，可能场景下，主键必须自己生成，不在乎那些性能的开销。那也没有问题。 3、自增主机用完了怎么办?在mysql中，Int整型的范围（-2147483648~2147483648），约20亿！因此不用考虑自增ID达到最大值这个问题。而且数据达到千万级的时候就应该考虑分库分表了。 4、主键为什么不推荐有业务含义?最好是主键是无意义的自增ID，然后另外创建一个业务主键ID， 因为任何有业务含义的列都有改变的可能性,主键一旦带上了业务含义，那么主键就有可能发生变更。主键一旦发生变更，该数据在磁盘上的存储位置就会发生变更，有可能会引发页分裂，产生空间碎片。 还有就是，带有业务含义的主键，不一定是顺序自增的。那么就会导致数据的插入顺序，并不能保证后面插入数据的主键一定比前面的数据大。如果出现了，后面插入数据的主键比前面的小，就有可能引发页分裂，产生空间碎片。 5、货币字段用什么类型?货币字段一般都用 Decimal类型，float和double是以二进制存储的，数据大的时候，可能存在误差。 以下是FLOAT和DOUBLE的区别： 浮点数以8位精度存储在FLOAT中，并且有四个字节。 浮点数存储在DOUBLE中，精度为18位，有八个字节。 6、时间字段用什么类型?这个看具体情况和实际场景，timestamp ，datatime ，bigint 都行！ timestamp，该类型是四个字节的整数，它能表示的时间范围为1970-01-01 08:00:01到2038-01-19 11:14:07。2038年以后的时间，是无法用timestamp类型存储的。但是它有一个优势，timestamp类型是带有时区信息的。一旦你系统中的时区发生改变，例如你修改了时区，该字段的值会自动变更。这个特性用来做一些国际化大项目，跨时区的应用时，特别注意！ datetime，占用8个字节，它存储的时间范围为1000-01-01 00:00:00 ~ 9999-12-31 23:59:59。显然，存储时间范围更大。但是它坑的地方在于，它存储的是时间绝对值，不带有时区信息。如果你改变数据库的时区，该项的值不会自己发生变更！ bigint，也是8个字节，自己维护一个时间戳，查询效率高，不过数据写入，显示都需要做转换。这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便，毕竟只是存放的数值。缺点也很明显，就是数据的可读性太差了，你无法直观的看到具体时间。 7、为什么不直接存储图片、音频、视频等大容量内容?我们在实际应用中，都是文件形式存储的。mysql中，只存文件的存放路径。虽然mysql中blob类型可以用来存放大容量文件，但是，我们在生产中，基本不用！ 主要有如下几个原因： 1. Mysql内存临时表不支持TEXT、BLOB这样的大数据类型，如果查询中包含这样的数据，查询效率会非常慢。 2. 数据库特别大，内存占用高，维护也比较麻烦。 3. binlog太大，如果是主从同步的架构，会导致主从同步效率问题！ 因此，不推荐使用blob等类型！ 8、表中有大字段X(例如：text类型)，且字段X不会经常更新，以读为主，那么是拆成子表好？还是放一起好？其实各有利弊，拆开带来的问题：连接消耗；不拆可能带来的问题：查询性能，所以要看你的实际情况，如果表数据量比较大，最好还是拆开为好。这样查询速度更快。 9、字段为什么要定义为NOT NULL?一般情况，都会设置一个默认值，不会出现字段里面有null，又有空的情况。主要有以下几个原因： \\1. 索引性能不好，Mysql难以优化引用可空列查询，它会使索引、索引统计和值更加复杂。可空列需要更多的存储空间，还需要mysql内部进行特殊处理。可空列被索引后，每条记录都需要一个额外的字节，还能导致MYisam 中固定大小的索引变成可变大小的索引。 \\2. 如果某列存在null的情况，可能导致count() 等函数执行不对的情况。 \\3. sql 语句写着也麻烦，既要判断是否为空，又要判断是否为null等。 二、数据库查询优化10、where执行顺序是怎样的？where 条件从左往右执行的，在数据量小的时候不用考虑，但数据量多的时候要考虑条件的先后顺序，此时应遵守一个原则：排除越多的条件放在第一个。 11、应该在这些列上创建索引：在经常需要搜索的列上，可以加快搜索的速度；在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构；在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度；在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的；在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间；在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 12、mysql联合索引联合索引是两个或更多个列上的索引。对于联合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部分，但只能是最左侧部分。例如索引是key index (a,b,c). 可以支持a 、 a,b 、 a,b,c 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效。利用索引中的附加列，您可以缩小搜索的范围，但使用一个具有两列的索引 不同于使用两个单独的索引。复合索引的结构与电话簿类似，人名由姓和名构成，电话簿首先按姓氏对进行排序，然后按名字对有相同姓氏的人进行排序。如果您知 道姓，电话簿将非常有用；如果您知道姓和名，电话簿则更为有用，但如果您只知道名不姓，电话簿将没有用处。 13、什么是最左前缀原则？最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以被用到。如下： select * from user where name=xx and city=xx ; ／／可以命中索引 select * from user where name=xx ; // 可以命中索引 select * from user where city=xx ; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的。 由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDER BY子句也遵循此规则。 14、怎么验证 MySQL 的索引是否满足需求？使用 explain 查看 SQL 是如何执行查询语句的，从而分析你的索引是否满足需求。 explain 语法：explain select * from table where type=1。 具体来说 MySQL 中的索引，不同的数据引擎实现有所不同，但目前主流的数据库引擎的索引都是 B+ 树实现的，B+ 树的搜索效率，可以到达二分法的性能，找到数据区域之后就找到了完整的数据结构了，所有索引的性能也是更好的。 15、问了下MySQL数据库cpu飙升到100%的话他怎么处理？\\1. 列出所有进程 show processlist 观察所有进程 多秒没有状态变化的(干掉) \\2. 查看慢查询，找出执行时间长的sql；explain分析sql是否走索引，sql优化； \\3. 检查其他子系统是否正常，是否缓存失效引起，需要查看buffer命中率； 4.开启慢查询日志，查看慢查询的 SQL。 16、mysql中表锁和行锁的区别在开发的时候，应该很少会注意到这些锁的问题，也很少会给程序加锁(除了库存这些对数量准确性要求极高的情况下)，即使我们不会这些锁知识，我们的程序在一般情况下还是可以跑得好好的。因为这些锁数据库隐式帮我们加了，只会在某些特定的场景下才需要手动加锁。 对于UPDATE、DELETE、INSERT语句，InnoDB会自动给涉及数据集加排他锁（X) MyISAM在执行查询语句SELECT前，会自动给涉及的所有表加读锁，在执行增、删、改操作前，会自动给涉及的表加写锁，这个过程并不需要用户干预 Mysql有很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁；这些锁统称为悲观锁(Pessimistic Lock) 行锁 特点：锁的粒度小，发生锁冲突的概率低、处理并发的能力强；开销大、加锁慢、会出现死锁不同的存储引擎支持的锁粒度是不一样的==：InnoDB行锁和表锁都支持、MyISAM只支持表锁！InnoDB只有通过索引条件检索数据才使用行级锁==，否则，InnoDB使用表锁也就是说，InnoDB的行锁是基于索引的！ InnoDB和MyISAM有两个本质的区别：InnoDB支持行锁、InnoDB支持事务 共享锁（S锁、读锁）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。即多个客户可以同时读取同一个资源，但不允许其他客户修改。 排他锁（X锁、写锁)：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的读锁和写锁。写锁是排他的，写锁会阻塞其他的写锁和读锁。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 意向锁也是数据库隐式帮我们做了，不需要程序员关心！ 加锁的方式：自动加锁。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁；对于普通SELECT语句，InnoDB不会加任何锁。 表锁 特点：开销小、加锁快、无死锁；锁粒度大，发生锁冲突的概率高，高并发下性能低 加锁的方式：自动加锁。查询操作（SELECT），会自动给涉及的所有表加读锁，更新操作（UPDATE、DELETE、INSERT），会自动给涉及的表加写锁。 如果某个进程想要获取读锁，同时另外一个进程想要获取写锁。在mysql中，写锁是优先于读锁的！ 写锁和读锁优先级的问题是可以通过参数调节的：max_write_lock_count和low-priority-updates 表锁下又分为两种模式： 表读锁（Table Read Lock）&amp;&amp; 表写锁（Table Write Lock） 从下图可以清晰看到，在表读锁和表写锁的环境下：读读不阻塞，读写阻塞，写写阻塞！ 读读不阻塞：当前用户在读数据，其他的用户也在读数据，不会加锁 读写阻塞：当前用户在读数据，其他的用户不能修改当前用户读的数据，会加锁！ 写写阻塞：当前用户在修改数据，其他的用户不能修改当前用户正在修改的数据，会加锁！ 从上面已经看到了：读锁和写锁是互斥的，读写操作是串行。 MVCC MVCC(Multi-Version ConcurrencyControl)多版本并发控制，可以简单地认为：MVCC就是行级锁的一个变种(升级版)。在表锁中我们读写是阻塞的，基于提升并发性能的考虑，MVCC一般读写是不阻塞的(很多情况下避免了加锁的操作)。 可以简单的理解为：对数据库的任何修改的提交都不会直接覆盖之前的数据，而是产生一个新的版本与老版本共存，使得读取时可以完全不加锁。 事务的隔离级别 事务的隔离级别就是通过锁的机制来实现，锁的应用最终导致不同事务的隔离级别，只不过隐藏了加锁细节，事务的隔离级别有4种： Read uncommitted：会出现脏读，不可重复读，幻读 Read committed：会出现不可重复读，幻读 Repeatable read：会出现幻读(Mysql默认的隔离级别，但是Repeatable read配合gap锁不会出现幻读！) Serializable：串行，避免以上的情况 悲观锁 我们使用悲观锁的话其实很简单(手动加行锁就行了)：select * from xxxx for update，在select 语句后边加了for update相当于加了排它锁(写锁)，加了写锁以后，其他事务就不能对它修改了！需要等待当前事务修改完之后才可以修改. 乐观锁 乐观锁不是数据库层面上的锁，需要用户手动去加的锁。一般我们在数据库表中添加一个版本字段version来实现，在更新User表的时，执行语句如下： update A set Name=lisi,version=version+1 where ID=#{id} and version=#{version}， 此时即可避免更新丢失。 举例：下单操作包括3步骤： 1.查询出商品信息 select (status,status,version) from t_goods where id=#{id} 2.根据商品信息生成订单 3.修改商品status为2 update t_goods set status=2,version=version+1 where id=#{id} and version=#{version}; 除了自己手动实现乐观锁之外，现在网上许多框架已经封装好了乐观锁的实现，如hibernate，需要时，可能自行搜索”hiberate 乐观锁”试试看。 间隙锁GAP 当我们用范围条件检索数据而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合范围条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在 的记录，叫做“间隙（GAP)”。InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。例子：假如emp表中只有101条记录，其empid的值分别是1,2,…,100,101 Select * from emp where empid &gt; 100 for update; 上面是一个范围查询，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁 InnoDB使用间隙锁的目的有2个： 为了防止幻读(上面也说了，Repeatable read隔离级别下再通过GAP锁即可避免了幻读) 满足恢复和复制的需要：MySQL的恢复机制要求在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读 死锁 并发的问题就少不了死锁，在MySQL中同样会存在死锁的问题 锁总结 表锁其实我们程序员是很少关心它的： 在MyISAM存储引擎中，当执行SQL语句的时候是自动加的。 在InnoDB存储引擎中，如果没有使用索引，表锁也是自动加的。 现在我们大多数使用MySQL都是使用InnoDB，InnoDB支持行锁： 共享锁–读锁–S锁 排它锁–写锁–X锁 在默认的情况下，select是不加任何行锁的~事务可以通过以下语句显示给记录集加共享锁或排他锁。 共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE 排他锁（X)：SELECT * FROM table_name WHERE … FOR UPDATE InnoDB基于行锁还实现了MVCC多版本并发控制，MVCC在隔离级别下的Read committed和Repeatable read下工作。MVCC实现了读写不阻塞 事务是数据库中的一个核心概念，指的是对数据库的一组操作作为一个整体，要么都执行要么都不执行。事务有四大特性：\\1. 原子性：每个事务都是一个整体，不可再拆分，事务中的sql语句要么都执行成功，要么都执行失败。\\2. 一致性：事务执行前后数据库的状态保持一致。比如不管如何转账，转账前后的总钱数是不变的。\\3. 隔离性：事务和事务之间不应该相互影响,保持隔离。\\4. 持久性：事务一旦提交对数据库的修改就是永久的，即使电脑发生故障也不会影响该修改，因为他的结果是记录在存储设备上的。事务中有一个重要的特性“事务的隔离性”指的是事务和事务之间不应该相互影响,保持隔离，然而在现实中多个事务可能会操作同一个数据，造成并发问题： \\1. 脏读：一个事务读取到了另一个事务尚未提交的数据。\\2. 不可重复读：事务一读取到了age的值20，事务二将该值修改成了28，事务一再次读取age的值28，事务一两次读取的age值不一致。\\3. 幻读：事务一读取到A表中有一条记录，事务二往A表中插入一条记录，事务一再次读取的时候记录变成了两条，就像发生幻觉一样。不可重复读和幻读很相似，可以从两个角度理解两者的差别：\\1. 不可重复读是另一个事务修改了数据，导致该事务多次读取出来的值不一样，而幻读是另一个事务插入或删除了记录，导致该事务多次读取出来的记录数不一样\\2. 不可重复读的解决只需要锁住会发生修改的记录就可以，幻读需要锁住更大的范围。正是因为有这些问题存在，数据库设置了隔离级别来处理：\\1. 读未提交（read uncommitted）: 事务中的修改，即使没有提交，其他事务也可以看得到在这种隔离级别下有可能发生脏读，不可重复读和幻读。一家酒店对外预定房间，现在还剩四间房，一个顾客到小王这里来预定四间房，小王查询系统发现还剩四间就将这四间房预定出去，该事务还没提交的时候另一个顾客到小李这里来预定房间，小李查询系统发现没房了，就拒绝了这个订单，此时小王的电脑发生故障，事务回滚，订单失效，这就是脏读造成的影响。\\2. 读已提交（read committed）: 事务中的修改只有提交以后才能被其它事务看到。在这种隔离级别下有可能发生不可重复读和幻读。还是定房间的例子，一个顾客到小王这里来预定四间房，小王将这四间房预定了出去，该事务还没提交的时候另一个顾客到小李这里来预定房间，小李查询系统发现还有四间房，刚想预定的时候小王的事务提交了，小李的系统立马呈现0间房。这就是不可重复读造成的影响。\\3. 可重复读 （repeatable read）：该级别保证了在事务中看到的每行的记录的结果是一致的，但是这种级别下有可能发生幻读。默认是可重复读公司规定如果销售额达不到就要扣工资，经理查询小王的销售业绩，发现还差几间房，经理喜上眉梢，把结果打印出来，结果打印出来的结果业绩正好合格，原来小王在这当口又卖了几张票正好填上了这个空缺。这就是幻读造成的影响。串行化（serializable）：该级别下所有的事务都是串行执行的，一个事务执行完了才能执行其它的事务，可以解决所有的并发问题，它是靠大量加锁实现的，所以效率很低下。只有在需要绝对保证数据一致性，并且并发量不大的情况下，可以考虑。 17、mysql主键索引和普通索引之间的区别是什么1）普通索引（INDEX）； 2）唯一索引（UNIQUE INDEX）； 3）全文索引（FULLTEXT）（全文索引是MyISAM的一个特殊索引类型，主要用于全文检索）； 4）主键索引； 5）组合索引（最左前缀）。 普通索引 普通索引是最基本的索引类型，而且它没有唯一性之类的限制。普通索引可以通过以下几种方式创建： 创建索引，例如 CREATEINDEX&lt;索引的名字&gt;ONtablename (列的列表); 修改表，例如 ALTERTABLEtablenameADDINDEX[索引的名字] (列的列表); 创建表的时候指定索引，例如 CREATETABLEtablename ( […],INDEX[索引的名字] (列的列表) ); 主键索引 主键是一种唯一性索引，但它必须指定为“PRIMARY KEY”。 主键一般在创建表的时候指定，例如 CREATETABLEtablename ( […],PRIMARYKEY(列的列表) ); 但是，我们也可以通过修改表的方式加入主键，例如“ALTER TABLE tablename ADD PRIMARY KEY (列的列表); ”。每个表只能有一个主键。 区别 普通索引是最基本的索引类型，没有任何限制，值可以为空，仅加速查询。普通索引是可以重复的，一个表中可以有多个普通索引。 主键索引是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值；索引列的所有值都只能出现一次，即必须唯一。简单来说：主键索引是加速查询 + 列值唯一（不可以有null）+ 表中只有一个。2.唯一索引和主键索引区别答：1）主键为一种约束，唯一索引为一种索引，本质上就不同；2）主键创建后一定包含唯一性索引，而唯一索引不一定就是主键；3）主键不允许空值，唯一索引可以为空；4）主键可以被其他表引用，而唯一索引不可以；5）主键只允许一个，唯一索引允许多个；6）主键和索引都是键，主键是逻辑键，索引为物理键，即主键不实际存在。 3.索引失效答：1）最佳左前缀原则（组合索引，不按索引定义时制定的顺序，最左优先）；2）like模糊查询时，以%开头，导致索引失效;3）使用“!=”和“&lt;&gt;”都会使索引失效，如果是主键或者索引列是整数，索引不会失效；4）遇到null值，索引失效；5）索引列上的显式或者隐式运算，导致索引失效；6）如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引（如select * from USER where name=123;7）用or连接导致索引失效（or条件有未建立索引的列导致索引失效）。 4.索引的坏处答：1）创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加；2）索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间。如果要建立聚簇索引，那么需要的空间就会更大；3）当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。 5.索引的方法答：主要有以下几种索引方法：B-Tree，Hash，R-Tree。1）B-Tree：B-Tree是最常见的索引类型，所有值（被索引的列）都是排过序的，每个叶节点到跟节点距离相等。所以B-Tree适合用来查找某一范围内的数据，而且可以直接支持数据排序（ORDER BY）B-Tree在MyISAM里的形式和Innodb稍有不同：MyISAM表数据文件和索引文件是分离的，索引文件仅保存数据记录的磁盘地址，InnoDB表数据文件本身就是主索引，叶节点data域保存了完整的数据记录。2）Hash索引：a.仅支持”=”,“IN”和”&lt;=&gt;”精确查询，不能使用范围查询：由于Hash索引比较的是进行Hash运算之后的Hash值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的Hash算法处理之后的Hash；b.不支持排序：由于Hash索引中存放的是经过Hash计算之后的Hash值，而且Hash值的大小关系并不一定和Hash运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；c.在任何时候都不能避免表扫描：由于Hash索引比较的是进行Hash运算之后的Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果；d.检索效率高，索引的检索可以一次定位，不像B-Tree索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以Hash索引的查询效率要远高于B-Tree索引；e.只有Memory引擎支持显式的Hash索引，但是它的Hash是nonunique的，冲突太多时也会影响查找性能。Memory引擎默认的索引类型即是Hash索引，虽然它也支持B-Tree索引。3）R-Tree索引：R-Tree在MySQL很少使用，仅支持geometry数据类型，支持该类型的存储引擎只有MyISAM、BDb、InnoDb、NDb、Archive几种。 18、SQL JOIN 中 on 与 where 的区别 left join : 左连接，返回左表中所有的记录以及右表中连接字段相等的记录。 right join : 右连接，返回右表中所有的记录以及左表中连接字段相等的记录。 inner join : 内连接，又叫等值连接，只返回两个表中连接字段相等的行。 full join : 外连接，返回两个表中的行：left join + right join。 cross join : 结果是笛卡尔积，就是第一个表的行数乘以第二个表的行数。 关键字 on数据库在通过连接两张或多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。在使用 left jion 时，on 和 where 条件的区别如下： 1、 on 条件是在生成临时表时使用的条件，它不管 on 中的条件是否为真，都会返回左边表中的记录。 2、where 条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有 left join 的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。 假设有两张表：表1：tab2idsize110220330表2：tab2sizename10AAA20BBB20CCC两条 SQL:select * form tab1 left join tab2 on (tab1.size = tab2.size) where tab2.name=’AAA’select * form tab1 left join tab2 on (tab1.size = tab2.size and tab2.name=’AAA’) 第一条SQL的过程：1、中间表on条件:tab1.size = tab2.sizetab1.idtab1.sizetab2.sizetab2.name11010AAA22020BBB22020CCC330(null)(null)2、再对中间表过滤where 条件：tab2.name=’AAA’tab1.idtab1.sizetab2.sizetab2.name11010AAA第二条SQL的过程：1、中间表on条件:tab1.size = tab2.size and tab2.name=’AAA’(条件不为真也会返回左表中的录) tab1.idtab1.sizetab2.sizetab2.name11010AAA220(null)(null)330(null)(null)其实以上结果的关键原因就是 left join、right join、full join 的特殊性，不管 on 上的条件是否为真都会返回 left 或 right 表中的记录，full 则具有 left 和 right 的特性的并集。 而 inner jion没这个特殊性，则条件放在 on 中和 where 中，返回的结果集是相同的。 1、一张表，里面有ID自增主键，当insert了17条记录之后，删除了第15,16,17条记录，再把Mysql重启，再insert一条记录，这条记录的ID是18还是15 ？ 如果表的类型是myisam，那么是18。 因为myisam表会把自增主键的最大ID记录到数据文件里，重启mysql自增主键的最大ID也不会丢失。 如果表的类型是innoDB，那么是15. innoDB表只是把自增主键的最大ID记录到内存中，所以重启数据库或者是对表进行OPTIMIZE操作，都会导致最大ID丢失 19、优化 MYSQL 数据库的方法1、对查询进行优化,应尽可能避免全表扫描 1）减少where 字段值null判断 2）应尽量避免在 where 子句中使用!=或&lt;&gt;操作符 3）应尽量避免在 where 子句中使用 or 来连接条件 4）in 和 not in 也要慎用 5）少使用模糊匹配 like 6）应尽量避免在 where 子句中对字段进行表达式操作 7）任何地方都不要使用*通配符去查询所有 3、不要在条件判断时进行 算数运算 4、很多时候用 exists 代替 in 是一个好的选择 (1) 选取最适用的字段属性，尽可能减少定义字段长度，尽量把字段设置 NOT NULL, 例如’省份，性别’, 最好设置为 ENUM 二、SQL语句中IN包含的值不应过多 三、SELECT语句务必指明字段名称 SELECT *增加很多不必要的消耗（cpu、io、内存、网络带宽）；增加了使用覆盖索引的可能性；当表结构发生改变时，前断也需要更新。所以要求直接在select后面接上字段名。 四、当只需要一条数据的时候，使用limit 1 这是为了使EXPLAIN中type列达到const类型 五、如果排序字段没有用到索引，就尽量少排序 六、如果限制条件中其他字段没有索引，尽量少用or 七、尽量用union all代替union 九、区分in和exists， not in和not exists 十、使用合理的分页方式以提高分页的效率 十一、分段查询 十二、避免在 where 子句中对字段进行 null 值判断 十三、不建议使用%前缀模糊查询 例如LIKE “%name”或者LIKE “%name%”，这种查询会导致索引失效而进行全表扫描。但是可以使用LIKE “name%”。 十四、避免在where子句中对字段进行表达式操作 十六、对于联合索引来说，要遵守最左前缀法则 十九、关于JOIN优化 20、适用MySQL 5.0以上版本:1.一个汉字占多少长度与编码有关: UTF-8:一个汉字=3个字节 GBK:一个汉字=2个字节 21、什么时候适合创建索引1、适合创建索引条件 1.、主键自动建立唯一索引 2、频繁作为查询条件的字段应该建立索引 3、查询中与其他表关联的字段，外键关系建立索引 4、单键/组合索引的选择问题，组合索引性价比更高 5、查询中排序的字段，排序字段若通过索引去访问将大大提高排序效率 6、查询中统计或者分组字段 2、不适合创建索引条件 1、表记录少的 2、经常增删改的表或者字段 3、where条件里用不到的字段不创建索引 4、过滤性不好的不适合建索引 9、在Mysql中ENUM的用法是什么？ ENUM是一个字符串对象，用于指定一组预定义的值，并可在创建表时使用。 Create table size(name ENUM(‘Smail,’Medium’,’Large’); 22、常见的sql语句1、表名order中有 1 2 3 4 1 去掉重复值 sql : select distinct from order 结果为company 1 2 3 4 2、asc 是升序 是从小到大 desc 是大到小 group 是分组 3、IFNULL() 函数用于判断第一个表达式是否为 NULL，如果为 NULL 则返回第二个参数的值，如果不为 NULL 则返回第一个参数的值。 6、查找学生 查询姓“赵”的用户 select * from table where name like ‘赵%’ 查询姓名中最后一个字段带赵字 select * from table where name like ‘%赵’ 查询姓名中带有赵的字段 select * from table where name like ‘%赵%’ 7、汇总分析 查询一个学生总分 select sum(*) from table where 课程号=’0002’ 查询选课程的学生人数 select count(distinct 学号) as 学生人数 from table 8、分组 查询各科成绩最高和最低的分 select 课程号 max（成绩）as 最高分，min（成绩）as 最低分 from table group by 课程号 查询每门课程被选修的学生数 select 课程号，count(学号) from score group by 课程号 查询男生 和女生人数 select 性别，count(*) from tabel group by 性别 9、分组结果的条件 查询平均成绩大于60分的学号和平均成绩 先说下Having是一个过滤声明，是在查询返回结果集以后对查询结果进行的过滤操作，在Having中可以使用聚合函数。 在查询过程中聚合语句(sum,min,max,avg,count)要比having子句优先执行。而where子句在查询过程中执行优先级高于聚合语句。 select 学号，avg(成绩)from group by 学号 having avg(成绩) &gt; 60 查询至少选修俩门课程的学生学号 select 学号,count(课程号)as 选修课程数目 from table group by 学号 having count(课程号)&gt;=2; 查询同名同性学生名单并统计人数 select 姓名，count(*)as 人数from table group by 姓名 having count(**)&gt;=2; 查询不及格的课程并按课程号从大到小排序 select 课程号 from table where 成绩&lt;60 order by 课程号 desc; 10、类似于成绩这一类型表类型为float 23、php读取文件内容的几种方法和函数？打开文件，然后读取。Fopen() fread() 打开读取一次完成 file_get_contents() 24、数据库开启慢查询show variables like ‘slow_query%’; show variables like ‘long_query_time’; 方法一：全局变量设置 可以在navicat设置方便 有得时候修改时间不变还是10那就采用第二种 将 slow_query_log 全局变量设置为“ON”状态 set global slow_query_log=’ON’; 设置慢查询日志存放的位置 set global slow_query_log_file=’/usr/local/mysql/data/www-slow.log’; 查询超过10秒就记录 set global long_query_time=10; 方法二：配置文件设置 修改配置文件my.cnf，在[mysqld]下的下方加入 [mysqld] slow_query_log = ON slow_query_log_file = /usr/local/mysql/data/www-slow.log long_query_time = 1 3.重启MySQL服务 service mysqld restart 4.查看设置后的参数 mysql&gt; show variables like 'slow_query%'; +---------------------+--------------------------------+ | Variable_name | Value | +---------------------+--------------------------------+ | slow_query_log | ON | | slow_query_log_file | /usr/local/mysql/data/slow.log | +---------------------+--------------------------------+ mysql&gt; show variables like 'long_query_time'; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | long_query_time | 1.000000 | +-----------------+----------+ 25、MYSQL的主从延迟怎么解决。实际上主从同步延迟根本没有什么一招制敌的办法，因为所有的 SQL 必须都要在从服务器里面执行一遍，但是主服务器如果不断的有更新操作源源不断的写入， 那么一旦有延迟产生，那么延迟加重的可能性就会越来越大。 当然我们可以做一些缓解的措施。 a）、最简单的减少 slave 同步延时的方案就是在架构上做优化，尽量让主库的 DDL 快速执行。还有就是主库是写，对数据安全性较高，比如 sync_binlog=1，innodb_flush_log_at_trx_commit = 1 之类的设置，而 slave 则不需要这么高的数据安全，完全可以将 sync_binlog 设置为 0 或者关闭 binlog，innodb_flushlog 也可以设置为 0 来提高 sql 的执行效率。另外就是使用比主库更好的硬件设备作为 slave。 b）、把一台从服务器当作备份使用， 而不提供查询， 这样他的负载就下来了， 执行 relay log 里面的 SQL 效率自然就高了。 c）、增加从服务器，这个目的还是分散读的压力， 从而降低服务器负载。 26、mysql中in 和exists 区别。mysql 中的 in 语句是把外表和内表作 hash 连接，而 exists 语句是对外表作 loop 循环，每次 loop 循环再对内表进行查询。一直大家都认为 exists 比 in 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 ㊤、如果查询的两个表大小相当，那么用 in 和 exists 差别不大。 ㊥、如果两个表中一个较小，一个是大表，则子查询表大的用 exists，子查询表小的用 in。 ㊦、not in 和 not exists 如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而 not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用 not exists 都比 not in 要快。 EXISTS 只返回 TRUE 或 FALSE，不会返回 UNKNOWN IN 当遇到包含NULL的情况，那么就会返回 UNKNOWN 27、mysql数据库监控指标之吞吐量如果你的数据库运行缓慢，或者出于某种原因无法响应查询，技术栈中每个依赖数据库的组件都会遭受性能问题。为了保证数据库的平稳运行，你可以监控下吞吐量这个指标。 吞吐量 在监控任何系统时，我们最关心的应该是确保系统能够高效地完成工作。数据库的工作是运行查询，因此首要任务是确保 MySQL 能够如期执行查询。 MySQL 有一个名为 Questions 的内部计数器（根据 MySQL 用语，这是一个服务器状态变量），客户端每发送一个查询语句，其值就会加一。 28、MySQL中通过EXPLAIN如何分析SQL的执行计划详解 简单说下有用的字段 table: sql所查询的表名 possible_keys: sql可能用到的索引 key: sql实际用到的索引,如果是Null,说明没有用到索引 ows: MySQL认为执行查询时必须检查的行数 下面分别对EXPLAIN命令结果的每一列进行说明： **.select_type:**表示SELECT的类型，常见的取值有： 类型说明 SIMPLE简单表，不使用表连接或子查询 PRIMARY主查询，即外层的查询 UNIONUNION中的第二个或者后面的查询语句 SUBQUERY子查询中的第一个 **.table:**输出结果集的表（表别名） **.type:**表示MySQL在表中找到所需行的方式，或者叫访问类型。常见访问类型如下，从上到下，性能由差到最好： ALL 全表扫描 index 索引全扫描 range 索引范围扫描 ref 非唯一索引扫描 eq_ref 唯一索引扫描 const,system 单表最多有一个匹配行 NULL 不用扫描表或索引 type结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题 一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题。 1、type=ALL，全表扫描，MySQL遍历全表来找到匹配行 一般是没有where条件或者where条件没有使用索引的查询语句 EXPLAIN SELECT * FROM customer WHERE active=0; 2、type=index，索引全扫描，MySQL遍历整个索引来查询匹配行，并不会扫描表 一般是查询的字段都有索引的查询语句 EXPLAIN SELECT store_id FROM customer; 3、type=range，索引范围扫描，常用于&lt;、&lt;=、&gt;、&gt;=、between等操作 EXPLAIN SELECT* FROM customer WHEREcustomer_id&gt;=10 ANDcustomer_id&lt;=20; 注意这种情况下比较的字段是需要加索引的，如果没有索引，则MySQL会进行全表扫描，如下面这种情况，create_date字段没有加索引： EXPLAIN SELECT * FROM customer WHERE create_date&gt;=’2006-02-13’ ; 4、type=ref，使用非唯一索引或唯一索引的前缀扫描，返回匹配某个单独值的记录行 store_id字段存在普通索引（非唯一索引） EXPLAIN SELECT* FROMcustomer WHEREstore_id=10; ref类型还经常会出现在join操作中： customer、payment表关联查询，关联字段customer.customer_id（主键），payment.customer_id（非唯一索引）。表关联查询时必定会有一张表进行全表扫描，此表一定是几张表中记录行数最少的表，然后再通过非唯一索引寻找其他关联表中的匹配行，以此达到表关联时扫描行数最少。 因为customer、payment两表中customer表的记录行数最少，所以customer表进行全表扫描，payment表通过非唯一索引寻找匹配行。 EXPLAIN SELECT * FROM customer customer INNER JOIN payment payment ON customer.customer_id = payment.customer_id; 6、type=const/system，单表中最多有一条匹配行，查询起来非常迅速，所以这个匹配行的其他列的值可以被优化器在当前查询中当作常量来处理 const/system出现在根据主键primary key或者 唯一索引 unique index 进行的查询 根据主键primary key进行的查询： EXPLAIN SELECT* FROMcustomer WHEREcustomer_id =10; 根据唯一索引unique index进行的查询： EXPLAIN SELECT * FROM customer WHERE email ='MARY.SMITH@sakilacustomer.org'; 7、type=NULL，MySQL不用访问表或者索引，直接就能够得到结果 .possible_keys: 表示查询可能使用的索引 .key: 实际使用的索引 .key_len: 使用索引字段的长度 .ref: 使用哪个列或常数与key一起从表中选择行。 .rows: 扫描行的数量 .filtered: 存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例(百分比) .Extra: 执行情况的说明和描述，包含不适合在其他列中显示但是对执行计划非常重要的额外信息 最主要的有一下三种： Using Index 表示索引覆盖，不会回表查询 Using Where 表示进行了回表查询 Using Index Condition 表示进行了ICP优化 Using Flesort 表示MySQL需额外排序操作, 不能通过索引顺序达到排序效果 什么是ICP？ MySQL5.6引入了Index Condition Pushdown（ICP）的特性，进一步优化了查询。Pushdown表示操作下放，某些情况下的条件过滤操作下放到存储引擎。 EXPLAIN SELECT* FROMrental WHERErental_date='2005-05-25'ANDcustomer_id&gt;=300 ANDcustomer_id&lt;=400; 在5.6版本之前： 优化器首先使用复合索引idx_rental_date过滤出符合条件rental_date='2005-05-25'的记录，然后根据复合索引idx_rental_date回表获取记录，最终根据条件customer_id&gt;=300 AND customer_id&lt;=400过滤出最后的查询结果（在服务层完成）。 在5.6版本之后： MySQL使用了ICP来进一步优化查询，在检索的时候，把条件customer_id&gt;=300 AND customer_id&lt;=400也推到存储引擎层完成过滤，这样能够降低不必要的IO访问。Extra为Using index condition就表示使用了ICP优化。 查看当前表中哪些字段创建索引了 show index from table; 29、Hash索引和B+树区别是什么？你在设计索引是怎么抉择的？ B+树可以进行范围查询，Hash索引不能。 B+树支持联合索引的最左侧原则，Hash索引不支持。 B+树支持order by排序，Hash索引不支持。 Hash索引在等值查询上比B+树效率更高。 B+树使用like 进行模糊查询的时候，like后面（比如%开头）的话可以起到优化的作用，Hash索引根本无法进行模糊查询。 1、查询Questions服务器状态变量值 SHOW GLOBAL STATUS LIKE “Questions”; 2、监控读指令的分解情况 SHOW GLOBAL STATUS LIKE “Com_select”; 3、监控写指令的分解情况 Writes = Com_insert + Com_update + Com_delete； show GLOBAL status like “com_insert”; SHOW GLOBAL STATUS LIKE “com_update”; SHOW GLOBAL STATUS LIKE “com_delete”; 30、MyISAM 和 InnoDB 的基本区别？索引结构如何实现？MyISAM类型不支持事务，表锁，易产生碎片，要经常优化，读写速度较快，而InnoDB类型支持事务，行锁，有崩溃恢复能力。读写速度比MyISAM慢，适合插入和更新操作比较多的应用，占空间大。 创建索引：alert table tablename add index (字段名) 31、sql语句应该考虑哪些安全性（1）防止sql注入，对特殊字符进行转义，过滤或者使用预编译sql语句绑定 （2）使用最小权限原则，特别是不要使用root账户，微不同的动作或者操作建立不同的账户 （3）当sql出错时，不要把数据库出错的信息暴露到客户端 32、mysql_fetch_array和mysql_fetch_object的区别是什么？以下是mysql_fetch_array和mysql_fetch_object的区别： mysql_fetch_array（） - 将结果行作为关联数组或来自数据库的常规数组返回。 mysql_fetch_object - 从数据库返回结果行作为对象。 33、mysql_fetch_row() 和mysql_fetch_array之间有什么区别?Mysql_fetch_row()是从结果集中取出一行作为枚举数组，mysql_fetch_array()是从结果集中取出一行作为索引数组或关联数组或两种方式都有。 实现中文字串截取无乱码的方法 Mb_substr(); 34、请写出数据类型(int char varchar datetime text)的意思；请问 varchar 和 char有什么区别？Int 整数 Datetime 日期时间型 Text 文本型 char(n) ：固定长度类型，比如订阅 char(10)，当你输入”abc”三个字符的时候，它们占的空间还是 10 个字节，其他 7 个是空字节。 chat 优点：效率高；缺点：占用空间；适用场景：存储密码的 md5 值，固定长度的，使用 char 非常合适。 varchar(n) ：可变长度，存储的值是每个值占用的字节再加上一个用来记录其长度的字节的长度。 所以，从空间上考虑 varcahr 比较合适；从效率上考虑 char 比较合适，二者使用需要权衡。","categories":[{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/categories/sql/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://13592491893.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/tags/sql/"},{"name":"mysql","slug":"mysql","permalink":"https://13592491893.github.io/tags/mysql/"}]},{"title":"JavaWeb——Servlet","slug":"JavaWeb-Servlet","date":"2021-05-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.623Z","comments":true,"path":"posts/49848.html","link":"","permalink":"https://13592491893.github.io/posts/49848.html","excerpt":"","text":"JavaWeb——Servlet版权声明：本文为CSDN博主「刘扬俊」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_19782019/article/details/80292110 ———————————————— 一、Tomcat工作机制动画演示(点击动图可全屏观看） 二、什么是ServletServlet（Server Applet），全称Java Servlet，未有中文译文。是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。狭义的Servlet是指Java语言实现的一个接口，广义的Servlet是指任何实现了这个Servlet接口的类，一般情况下，人们将Servlet理解为后者。Servlet运行于支持Java的应用服务器中。从实现上讲，Servlet可以响应任何类型的请求，但绝大多数情况下Servlet只用来扩展基于HTTP协议的Web服务器。 三、Servlet的工作模式 客户端发送请求至服务器 服务器启动并调用Servlet，Servlet根据客户端请求生成响应内容并将其传给服务器 服务器将响应返回客户端 四、Servlet API 概览Servlet API 包含以下4个Java包: javax.servlet 其中包含定义servlet和servlet容器之间契约的类和接口。 javax.servlet.http 其中包含定义HTTP Servlet 和Servlet容器之间的关系。 javax.servlet.annotation 其中包含标注servlet，Filter,Listener的标注。它还为被标注元件定义元数据。 javax.servlet.descriptor，其中包含提供程序化登录Web应用程序的配置信息的类型。 五、Servlet 的主要类型 六、Servlet 的使用方法Servlet技术的核心是Servlet，它是所有Servlet类必须直接或者间接实现的一个接口。在编写实现Servlet的Servlet类时，直接实现它。在扩展实现这个这个接口的类时，间接实现它。 Servlet中@WebServlet属性详解:(同理理解到Filter) 在Servlet中,设置了@WebServlet注解,当请求该Servlet时,服务器就会自动读取当中的信息,如果注解@WebServlet(“/category”),则表示该Servlet默认的请求路径为…/category,这里省略了urlPatterns属性名,完整的写法应该是:@WebServlet(urlPatterns = “/category”),如果在@WebServlet中需要设置多个属性,必须给属性值加上属性名称,中间用逗号隔开,否则会报错.若没有设置@WebServlet的name属性，默认值会是Servlet的类完整名称. 在servlet3.0以后，web.xml中对Servlet配置，同样可以在@WebServlet注解中配置.下面是@WebServlet的属性列表: servlet的url-pattern匹配规则： 首先需要明确几容易混淆的规则： servlet容器中的匹配规则既不是简单的通配，也不是正则表达式，而是特定的规则。所以不要用通配符或者正则表达式的匹配规则来看待servlet的url-pattern。 Servlet 2.5开始，一个servlet可以使用多个url-pattern规则，标签声明了与该servlet相应的匹配规则，每个标签代表1个匹配规则； 当servlet容器接收到浏览器发起的一个url请求后，容器会用url减去当前应用的上下文路径，以剩余的字符串作为servlet映射，假如url是http://localhost:8080/appDemo/index.html，其应用上下文是appDemo，容器会将http://localhost:8080/appDemo去掉，用剩下的/index.html部分拿来做servlet的映射匹配 url-pattern映射匹配过程是有优先顺序的 而且当有一个servlet匹配成功以后，就不会去理会剩下的servlet了。 需要注意的问题： 路径匹配和扩展名匹配无法同时设置：匹配方法只有三种，要么是路径匹配（以“/”字符开头，并以“/”结尾），要么是扩展名匹配（以“.”开头），要么是精确匹配，三种匹配方法不能进行组合，不要想当然使用通配符或正则规则。如&lt;url-pattern&gt;/user/*.action&lt;/url-pattern&gt;是非法的，另外注意：&lt;url-pattern&gt;/aa/*/bb&lt;/url-pattern&gt;是精确匹配，合法，这里的*不是通配的含义 “/*”和”/“含义并不相同“/”属于路径匹配，并且可以匹配所有request，由于路径匹配的优先级仅次于精确匹配，所以“/*”会覆盖所有的扩展名匹配，很多404错误均由此引起，所以这是一种特别恶劣的匹配模式，一般只用于filter的url-pattern“/”是servlet中特殊的匹配模式，切该模式有且仅有一个实例，优先级最低，不会覆盖其他任何url-pattern，只是会替换servlet容器的内建default servlet ，该模式同样会匹配所有request。配置“/”后，一种可能的现象是myServlet会拦截诸如http://localhost:8080/appDemo/user/addUser.action、http://localhost:8080/appDemo/user/updateUser的格式的请求，但是并不会拦截http://localhost:8080/appDemo/user/users.jsp、http://localhost:8080/appDemo/index.jsp，这是应为servlet容器有内置的“.jsp”匹配器，而扩展名匹配的优先级高于缺省匹配，所以才会有上述现象。 七、Servlet 的工作原理 Servlet接口定义了Servlet与servlet容器之间的契约。这个契约是：Servlet容器将Servlet类载入内存，并产生Servlet实例和调用它具体的方法。但是要注意的是，在一个应用程序中，每种Servlet类型只能有一个实例。用户请求致使Servlet容器调用Servlet的Service（）方法，并传入一个ServletRequest对象和一个ServletResponse对象。ServletRequest对象和ServletResponse对象都是由Servlet容器（例如TomCat）封装好的，并不需要程序员去实现，程序员可以直接使用这两个对象。ServletRequest中封装了当前的Http请求，因此，开发人员不必解析和操作原始的Http数据。ServletResponse表示当前用户的Http响应，程序员只需直接操作ServletResponse对象就能把响应轻松的发回给用户。对于每一个应用程序，Servlet容器还会创建一个ServletContext对象。这个对象中封装了上下文（应用程序）的环境详情。每个应用程序只有一个ServletContext。每个Servlet对象也都有一个封装Servlet配置的ServletConfig对象。 八、Servlet 接口中定义的方法让我们首先来看一看Servlet接口中定义了哪些方法吧。 public interface Servlet { void init(ServletConfig var1) throws ServletException; ServletConfig getServletConfig(); void service(ServletRequest var1, ServletResponse var2) throws ServletException, IOException; String getServletInfo(); void destroy(); } 九、Servlet 的生命周期其中，init( ),service( ),destroy( )是Servlet生命周期的方法。代表了Servlet从“出生”到“工作”再到“死亡 ”的过程。Servlet容器（例如TomCat）会根据下面的规则来调用这三个方法： 1.init( ),当Servlet第一次被请求时，Servlet容器就会开始调用这个方法来初始化一个Servlet对象出来，但是这个方法在后续请求中不会在被Servlet容器调用，就像人只能“出生”一次一样。我们可以利用init（ ）方法来执行相应的初始化工作。调用这个方法时，Servlet容器会传入一个ServletConfig对象进来从而对Servlet对象进行初始化。 2.service( )方法，每当请求Servlet时，Servlet容器就会调用这个方法。就像人一样，需要不停的接受老板的指令并且“工作”。第一次请求时，Servlet容器会先调用init( )方法初始化一个Servlet对象出来，然后会调用它的service( )方法进行工作，但在后续的请求中，Servlet容器只会调用service方法了。 3.destory(),当要销毁Servlet时，Servlet容器就会调用这个方法，就如人一样，到时期了就得死亡。在卸载应用程序或者关闭Servlet容器时，就会发生这种情况，一般在这个方法中会写一些清除代码。 首先，我们来编写一个简单的Servlet来验证一下它的生命周期： public class MyFirstServlrt implements Servlet { @Override public void init(ServletConfig servletConfig) throws ServletException { System.out.println(\"Servlet正在初始化\"); } @Override public ServletConfig getServletConfig() { return null; } @Override public void service(ServletRequest servletRequest, ServletResponse servletResponse) throws ServletException, IOException { //专门向客服端提供响应的方法 System.out.println(\"Servlet正在提供服务\"); } @Override public String getServletInfo() { return null; } @Override public void destroy() { System.out.println(\"Servlet正在销毁\"); } } 然后在xml中配置正确的映射关系，在浏览器中访问Servlet，第一次访问时，控制台输出了如下信息： 然后，我们在浏览器中刷新3遍： 控制台输出的信息变成了下面这样： 接下来，我们关闭Servlet容器： 控制台输出了Servlet的销毁信息，这就是一个Servlet的完整生命周期。 十、Servlet 的其它两个方法getServletInfo（ ），这个方法会返回Servlet的一段描述，可以返回一段字符串。 getServletConfig（ ），这个方法会返回由Servlet容器传给init（ ）方法的ServletConfig对象。 十一、ServletRequset接口Servlet容器对于接受到的每一个Http请求，都会创建一个ServletRequest对象(ServletRequset是一个接口，为什么可以创建一个ServletRequset对象?==&gt;&gt;HttpServletRequest接口的实现类是Web容器负责的，Tomcat服务器有自己的实现。但是程序要还是只需要面向HttpServletRequest接口调用方法即可，不需要关心具体的实现类)，并把这个对象传递给Servlet的Sevice( )方法。其中，ServletRequest对象内封装了关于这个请求的许多详细信息。 让我们来看一看ServletRequest接口的部分内容： public interface ServletRequest { int getContentLength();//返回请求主体的字节数 String getContentType();//返回主体的MIME类型 String getParameter(String var1);//返回请求参数的值 } getParameter获取参数并返回给页面： 其中，getParameter是在ServletRequest中最常用的方法，可用于获取查询字符串的值。 十二、ServletResponse接口 javax.servlet.ServletResponse接口表示一个Servlet响应，在调用Servlet的Service( )方法前，Servlet容器会先创建一个ServletResponse对象，并把它作为第二个参数传给Service( )方法。ServletResponse隐藏了向浏览器发送响应的复杂过程。 让我们也来看看ServletResponse内部定义了哪些方法： public interface ServletResponse { String getCharacterEncoding(); String getContentType(); ServletOutputStream getOutputStream() throws IOException; PrintWriter getWriter() throws IOException; void setCharacterEncoding(String var1); void setContentLength(int var1); void setContentType(String var1); void setBufferSize(int var1); int getBufferSize(); void flushBuffer() throws IOException; void resetBuffer(); boolean isCommitted(); void reset(); void setLocale(Locale var1); Locale getLocale(); } 其中的getWriter方法，它返回了一个可以向客户端发送文本的的Java.io.PrintWriter对象。默认情况下，PrintWriter对象使用ISO-8859-1编码（该编码在输入中文时会发生乱码）。 在向客户端发送响应时，大多数都是使用该对象向客户端发送HTML。 还有一个方法也可以用来向浏览器发送数据，它就是getOutputStream，从名字就可以看出这是一个二进制流对象，因此这个方法是用来发送二进制数据的。 在发送任何HTML之前，应该先调用setContentType（）方法，设置响应的内容类型，并将“text/html”作为一个参数传入，这是在告诉浏览器响应的内容类型为HTML，需要以HTML的方法解释响应内容而不是普通的文本，或者也可以加上“charset=UTF-8”改变响应的编码方式以防止发生中文乱码现象。 十三、ServletConfig接口当Servlet容器初始化Servlet时，Servlet容器会给Servlet的init( )方式传入一个ServletConfig对象。 其中几个方法如下： 十四、ServletContext对象ServletContext对象表示Servlet应用程序。每个Web应用程序都只有一个ServletContext对象。在将一个应用程序同时部署到多个容器的分布式环境中，每台Java虚拟机上的Web应用都会有一个ServletContext对象。 通过在ServletConfig中调用getServletContext方法，也可以获得ServletContext对象。 那么为什么要存在一个ServletContext对象呢？存在肯定是有它的道理，因为有了ServletContext对象，就可以共享从应用程序中的所有资料处访问到的信息，并且可以动态注册Web对象。前者将对象保存在ServletContext中的一个内部Map中。保存在ServletContext中的对象被称作属性。 ServletContext中的下列方法负责处理属性： Object getAttribute(String var1); Enumeration&lt;String&gt; getAttributeNames(); void setAttribute(String var1, Object var2); void removeAttribute(String var1); 热知识： 1.Session(做验证码时疑问)访问同一个应用程序，一个浏览器对应一个Session，利用此知识可以产生验证码图片和字符串，把字符串存到session里，图片返回前端，登录时传入验证码与session里的做对比 2.Request、Session、servletContext区别首先从作用范围来说 Request 保存的键值仅在下一个request对象中可以得到。 Session 它是一个会话范围，相当于一个局部变量，从Session第一次创建知道关闭，数据都一直 保存，每一个客户都有一个Session，所以它可以被客户一直访问，只要Session没有关闭和超时即浏览器关闭。 servletContext 它代表了servlet环境的上下文，相当于一个全局变量，即只要某个web应用在启动中，这个对象就一直都有效的存在，所以它的范围是最大的，存储的数据可以被所有用户使用，只要服务器不关闭，数据就会一直都存在。 它们的优缺点： request：好处：用完就仍，不会导致资源占用的无限增长。弊处：数据只能被下一个对象获取，所以在写程序时会因为无法共享数据导致每次要用都从数据库中取，多做操作，自然会对性能有一些影响。 session：好处：是一个局部变量，可以保存用户的信息并直接取出，不用每次都去数据库抓，少做操作，极大的方便了程序的编写。弊处：每个客户都有一个session，只能自己使用，不同session可能保存大量重复数据； 可能耗费大量服务器内存； 另外session构建在cookie和url重写的基础上，所以用session实现会话跟踪，会用掉一点点服务器带宽和客户端保持联络， 当然session越多，耗费的带宽越多，理论上也会对性能造成影响。 集群的session同步会是个问题。 servletContext： 好处：不用每次都去数据库抓，少做操作。 存储的数据所有客户都可以用。 可减少重复在内存中存储数据造成的开销。 package com.cdsxt.action; import java.io.IOException; import java.io.PrintWriter; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import javax.servlet.http.HttpSession; public class SessionServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { this.doPost(request, response); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { //创建 session的时机 //首先 服务器调用request.getSession(); //请求头里没有JsessionId 创建一个新的session 对象 //请求头里有JsessionId 按照这个JsessionId去找对应的session对象 //如果session对象没有失效 不创建新的session //如果session对象失效 创建新的session //什么叫一次会话 笼统回答 浏览器启动并且访问项目 到 关闭浏览器 .事实上，取决于JsessionID对应的session是否失效 HttpSession session=request.getSession(); session.setAttribute(\"name\", \"aaaaaaaa\"); session.setAttribute(\"value\", \"bbbbbbbbb\"); // session.setMaxInactiveInterval(10); //请求转发可以拿到 // request.getRequestDispatcher(\"sessionServlet2\").forward(request, response); //重定向也可以拿到 // response.sendRedirect(\"sessionServlet2\"); //request 同一次请求 //servletContext(application) 整个web的servlet都享用 从web启动 到web卸载 //session 同一次会话 } } 十五、GenericServlet抽象类前面我们编写Servlet一直是通过实现Servlet接口来编写的，但是，使用这种方法，则必须要实现Servlet接口中定义的所有的方法，即使有一些方法中没有任何东西也要去实现，并且还需要自己手动的维护ServletConfig这个对象的引用。因此，这样去实现Servlet是比较麻烦的。 void init(ServletConfig var1) throws ServletException; 幸好，GenericServlet抽象类的出现很好的解决了这个问题。本着尽可能使代码简洁的原则，GenericServlet实现了Servlet和ServletConfig接口，下面是GenericServlet抽象类的具体代码： public abstract class GenericServlet implements Servlet, ServletConfig, Serializable { private static final String LSTRING_FILE = \"javax.servlet.LocalStrings\"; private static ResourceBundle lStrings = ResourceBundle.getBundle(\"javax.servlet.LocalStrings\"); private transient ServletConfig config; public GenericServlet() { } public void destroy() { } public String getInitParameter(String name) { ServletConfig sc = this.getServletConfig(); if (sc == null) { throw new IllegalStateException(lStrings.getString(\"err.servlet_config_not_initialized\")); } else { return sc.getInitParameter(name); } } public Enumeration&lt;String&gt; getInitParameterNames() { ServletConfig sc = this.getServletConfig(); if (sc == null) { throw new IllegalStateException(lStrings.getString(\"err.servlet_config_not_initialized\")); } else { return sc.getInitParameterNames(); } } public ServletConfig getServletConfig() { return this.config; } public ServletContext getServletContext() { ServletConfig sc = this.getServletConfig(); if (sc == null) { throw new IllegalStateException(lStrings.getString(\"err.servlet_config_not_initialized\")); } else { return sc.getServletContext(); } } public String getServletInfo() { return \"\"; } public void init(ServletConfig config) throws ServletException { this.config = config; this.init(); } public void init() throws ServletException { } public void log(String msg) { this.getServletContext().log(this.getServletName() + \": \" + msg); } public void log(String message, Throwable t) { this.getServletContext().log(this.getServletName() + \": \" + message, t); } public abstract void service(ServletRequest var1, ServletResponse var2) throws ServletException, IOException; public String getServletName() { ServletConfig sc = this.getServletConfig(); if (sc == null) { throw new IllegalStateException(lStrings.getString(\"err.servlet_config_not_initialized\")); } else { return sc.getServletName(); } } } 其中，GenericServlet抽象类相比于直接实现Servlet接口，有以下几个好处： 1.为Servlet接口中的所有方法提供了默认的实现，则程序员需要什么就直接改什么，不再需要把所有的方法都自己实现了。 2.提供方法，包围ServletConfig对象中的方法。 3.将init( )方法中的ServletConfig参数赋给了一个内部的ServletConfig引用从而来保存ServletConfig对象，不需要程序员自己去维护ServletConfig了。 public void init(ServletConfig config) throws ServletException { this.config = config; this.init(); } 但是，我们发现在GenericServlet抽象类中还存在着另一个没有任何参数的Init()方法： public void init() throws ServletException { } 设计者的初衷到底是为了什么呢？在第一个带参数的init（）方法中就已经把ServletConfig对象传入并且通过引用保存好了，完成了Servlet的初始化过程，那么为什么后面还要加上一个不带任何参数的init（）方法呢？这不是多此一举吗？当然不是多此一举了，存在必然有存在它的道理。我们知道，抽象类是无法直接产生实例的，需要另一个类去继承这个抽象类，那么就会发生方法覆盖的问题，如果在类中覆盖了GenericServlet抽象类的init（）方法，那么程序员就必须手动的去维护ServletConfig对象了，还得调用super.init(servletConfig）方法去调用父类GenericServlet的初始化方法来保存ServletConfig对象，这样会给程序员带来很大的麻烦。GenericServlet提供的第二个不带参数的init( )方法，就是为了解决上述问题的。这个不带参数的init（）方法，是在ServletConfig对象被赋给ServletConfig引用后，由第一个带参数的init(ServletConfig servletconfig)方法调用的，那么这意味着，当程序员如果需要覆盖这个GenericServlet的初始化方法，则只需要覆盖那个不带参数的init( )方法就好了，此时，servletConfig对象仍然有GenericServlet保存着。说了这么多，通过扩展GenericServlet抽象类，就不需要覆盖没有计划改变的方法。因此，代码将会变得更加的简洁，程序员的工作也会减少很多。然而，虽然GenricServlet是对Servlet一个很好的加强，但是也不经常用，因为他不像HttpServlet那么高级。HttpServlet才是主角，在现实的应用程序中被广泛使用。那么我们接下来就看看传说中的HttpServlet到底厉害在哪里吧。 十六、javax.servlet.http包内容之所以所HttpServlet要比GenericServlet强大，其实也是有道理的。HttpServlet是由GenericServlet抽象类扩展而来的，HttpServlet抽象类的声明如下所示： public abstract class HttpServlet extends GenericServlet implements Serializable HttpServlet之所以运用广泛的另一个原因是现在大部分的应用程序都要与HTTP结合起来使用。这意味着我们可以利用HTTP的特性完成更多更强大的任务。Javax。servlet.http包是Servlet API中的第二个包，其中包含了用于编写Servlet应用程序的类和接口。Javax.servlet.http中的许多类型都覆盖了Javax.servlet中的类型。 十七、HttpServlet抽象类HttpServlet抽象类是继承于GenericServlet抽象类而来的。使用HttpServlet抽象类时，还需要借助分别代表Servlet请求和Servlet响应的HttpServletRequest和HttpServletResponse对象。 HttpServletRequest接口扩展于javax.servlet.ServletRequest接口，HttpServletResponse接口扩展于javax.servlet.servletResponse接口。 public interface HttpServletRequest extends ServletRequest public interface HttpServletResponse extends ServletResponse HttpServlet抽象类覆盖了GenericServlet抽象类中的Service( )方法，并且添加了一个自己独有的Service(HttpServletRequest request，HttpServletResponse方法。 让我们来具体的看一看HttpServlet抽象类是如何实现自己的service方法吧： 首先来看GenericServlet抽象类中是如何定义service方法的： public abstract void service(ServletRequest var1, ServletResponse var2) throws ServletException, IOException; 我们看到是一个抽象方法，也就是HttpServlet要自己去实现这个service方法，我们在看看HttpServlet是怎么覆盖这个service方法的： public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException { HttpServletRequest request; HttpServletResponse response; try { request = (HttpServletRequest)req; response = (HttpServletResponse)res; } catch (ClassCastException var6) { throw new ServletException(\"non-HTTP request or response\"); } this.service(request, response); } 我们发现，HttpServlet中的service方法把接收到的ServletRequsest类型的对象转换成了HttpServletRequest类型的对象，把ServletResponse类型的对象转换成了HttpServletResponse类型的对象。之所以能够这样强制的转换，是因为在调用Servlet的Service方法时，Servlet容器总会传入一个HttpServletRequest对象和HttpServletResponse对象，预备使用HTTP。因此，转换类型当然不会出错了。 转换之后，service方法把两个转换后的对象传入了另一个service方法，那么我们再来看看这个方法是如何实现的： protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { String method = req.getMethod(); long lastModified; if (method.equals(\"GET\")) { lastModified = this.getLastModified(req); if (lastModified == -1L) { this.doGet(req, resp); } else { long ifModifiedSince = req.getDateHeader(\"If-Modified-Since\"); if (ifModifiedSince &lt; lastModified) { this.maybeSetLastModified(resp, lastModified); this.doGet(req, resp); } else { resp.setStatus(304); } } } else if (method.equals(\"HEAD\")) { lastModified = this.getLastModified(req); this.maybeSetLastModified(resp, lastModified); this.doHead(req, resp); } else if (method.equals(\"POST\")) { this.doPost(req, resp); } else if (method.equals(\"PUT\")) { this.doPut(req, resp); } else if (method.equals(\"DELETE\")) { this.doDelete(req, resp); } else if (method.equals(\"OPTIONS\")) { this.doOptions(req, resp); } else if (method.equals(\"TRACE\")) { this.doTrace(req, resp); } else { String errMsg = lStrings.getString(\"http.method_not_implemented\"); Object[] errArgs = new Object[]{method}; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(501, errMsg); } } 十八、HttpServletRequest接口HttpServletRequest表示Http环境中的Servlet请求。它扩展于javax.servlet.ServletRequest接口，并添加了几个方法。 String getContextPath();//返回请求上下文的请求URI部分 Cookie[] getCookies();//返回一个cookie对象数组 String getHeader(String var1);//返回指定HTTP标题的值 String getMethod();//返回生成这个请求HTTP的方法名称 String getQueryString();//返回请求URL中的查询字符串 HttpSession getSession();//返回与这个请求相关的会话对象 十九、HttpServletRequest内封装的请求因为Request代表请求，所以我们可以通过该对象分别获得HTTP请求的请求行，请求头和请求体。 关于HTTP具体的详细解释，可以参考我的另一篇博文：JavaWeb——HTTP。 二十、通过request获得请求行假设查询字符串为：username=zhangsan&amp;password=123 获得客户端的请求方式：String getMethod() 获得请求的资源： String getRequestURI() StringBuffer getRequestURL() String getContextPath() —web应用的名称 String getQueryString() —- get提交url地址后的参数字符串 二十一、通过request获得请求头long getDateHeader(String name) String getHeader(String name) Enumeration getHeaderNames() Enumeration getHeaders(String name) int getIntHeader(String name) referer头的作用：执行该此访问的的来源，做防盗链 二十二、通过request获得请求体请求体中的内容是通过post提交的请求参数，格式是： username=zhangsan&amp;password=123&amp;hobby=football&amp;hobby=basketball key ———————- value username [zhangsan] password [123] hobby [football，basketball] 以上面参数为例，通过一下方法获得请求参数： String getParameter(String name) String[] getParameterValues(String name) Enumeration getParameterNames() Map&lt;String,String[]&gt; getParameterMap() 注意：get请求方式的请求参数 上述的方法一样可以获得 二十三、Request乱码问题的解决方法在前面我们讲过，在service中使用的编码解码方式默认为：ISO-8859-1编码，但此编码并不支持中文，因此会出现乱码问题，所以我们需要手动修改编码方式为UTF-8编码，才能解决中文乱码问题，下面是发生乱码的具体细节： 解决post提交方式的乱码：request.setCharacterEncoding(“UTF-8”); 解决get提交的方式的乱码：parameter = newString(parameter.getbytes(“iso8859-1”),”utf-8”); 二十四、HttpServletResponse接口在Service API中，定义了一个HttpServletResponse接口，它继承自ServletResponse接口，专门用来封装HTTP响应消息。 由于HTTP请求消息分为状态行，响应消息头，响应消息体三部分，因此，在HttpServletResponse接口中定义了向客户端发送响应状态码，响应消息头，响应消息体的方法 二十五、HttpServletResponse内封装的响应 二十六、通过Response设置响应void addCookie(Cookie var1);//给这个响应添加一个cookie void addHeader(String var1, String var2);//给这个请求添加一个响应头 void sendRedirect(String var1) throws IOException;//发送一条响应码，讲浏览器跳转到指定的位置 void setStatus(int var1);//设置响应行的状态码 addHeader(String name, String value) addIntHeader(String name, int value) addDateHeader(String name, long date) setHeader(String name, String value) setDateHeader(String name, long date) setIntHeader(String name, int value) PrintWriter getWriter() 获得字符流，通过字符流的write(String s)方法可以将字符串设置到response 缓冲区中，随后Tomcat会将response缓冲区中的内容组装成Http响应返回给浏览器端。 ServletOutputStream getOutputStream() 获得字节流，通过该字节流的write(byte[] bytes)可以向response缓冲区中写入字节，再由Tomcat服务器将字节内容组成Http响应返回给浏览器。 注意：虽然response对象的getOutSream（）和getWriter（）方法都可以发送响应消息体，但是他们之间相互排斥，不可以同时使用，否则会发生异常。 二十七、Response的乱码问题 原因：response缓冲区的默认编码是iso8859-1，此码表中没有中文。所以需要更改response的编码方式： 通过更改response的编码方式为UTF-8，任然无法解决乱码问题，因为发送端服务端虽然改变了编码方式为UTF-8，但是接收端浏览器端仍然使用GB2312编码方式解码，还是无法还原正常的中文，因此还需要告知浏览器端使用UTF-8编码去解码。 上面通过调用两个方式分别改变服务端对于Response的编码方式以及浏览器的解码方式为同样的UTF-8编码来解决编码方式不一样发生乱码的问题。 response.setContentType(“text/html;charset=UTF-8”)这个方法包含了上面的两个方法的调用，因此在实际的开发中，只需要调用一个response.setContentType(“text/html;charset=UTF-8”)方法即可。 二十八、Response的工作流程 二十九、Servlet的工作流程 三十、编写第一个Servlet首先，我们来写一个简单的用户名，密码的登录界面的html文件： &lt;form action=\"/form\"method=\"get\"&gt; 该html文件在最后点击提交按钮时，把表单所有数据通过Get方式发送到/form虚拟路径下： &lt;!DOCTYPE html&gt; &lt;html lang=\"en\"&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=\"/form\" method=\"get\"&gt; &lt;span&gt;用户名&lt;/span&gt;&lt;input type=\"text\" name=\"username\"&gt;&lt;br&gt; &lt;span&gt;密码&lt;/span&gt;&lt;input type=\"password\" name=\"password\"&gt;&lt;br&gt; &lt;input type=\"submit\" name=\"submit\"&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 访问一下我们刚才写的这个简单的登录界面： 接下来，我们就开始写一个Servlet用来接收处理表单发送过来的请求，这个Servlet的名称就叫做FormServlet： public class FormServlet extends HttpServlet { private static final long serialVersionUID = -4186928407001085733L; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { //设置响应的编码格式为UTF-8编码，否则发生中文乱码现象 response.setContentType(\"text/html;charset=UTF-8\"); //1.获得请求方式 String method = request.getMethod(); //2.获得请求的资源相关的内容 String requestURI = request.getRequestURI();//获得请求URI StringBuffer requestURL = request.getRequestURL(); String webName = request.getContextPath();//获得应用路径（应用名称） String querryString = request.getQueryString();//获得查询字符串 response.getWriter().write(\"&lt;h1&gt;下面是获得的字符串&lt;/h1&gt;\"); response.getWriter().write(\"&lt;h1&gt;method(HTTP方法):&lt;h1&gt;\"); response.getWriter().write(\"&lt;h1&gt;\"+method+\"&lt;/h1&gt;&lt;br&gt;\"); response.getWriter().write(\"&lt;h1&gt;requestURi(请求URI）:&lt;/h1&gt;\"); response.getWriter().write(\"&lt;h1&gt;\" + requestURI + \"&lt;/h1&gt;&lt;br&gt;\"); response.getWriter().write(\"&lt;h1&gt;webname(应用名称):&lt;/h1&gt;\"); response.getWriter().write(\"&lt;h1&gt;\" + webName + \"&lt;/h1&gt;&lt;br&gt;\"); response.getWriter().write(\"&lt;h1&gt;querrystring(查询字符串):&lt;/h1&gt;\"); response.getWriter().write(\"&lt;h1&gt;\" + querryString + \"&lt;/h1&gt;\"); } } 该Servlet的作用是，接收form登录表单发送过来的HTTP请求，并解析出请求中封装的一些参数，然后在回写到response响应当中去，最后在浏览器端显示。 最后一步，我们在XML中配置好这个Servlet的映射关系： &lt;/servlet-mapping&gt; &lt;servlet&gt; &lt;servlet-name&gt;FormServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.javaee.util.FormServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;FormServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/form&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 接下来，启动tomcat，在浏览器中输入登录表单的地址： 填入用户名为：root，密码为：123,最后点击提交： 提交之后，表单数据将会发送到相应的Servlet进行处理，此时，浏览器的地址变成如下所示： 我们会发现，在地址栏中，多了后面的“?username=root&amp;password=123&amp;提交=提交”字符串，这其实就是我们开始填写的参数，以Get的方法发送过去，所以查询字符串会直接加在链接后面，如果采用的是Post方式则不会出现在链接中，因此，登录表单为了安全性大多采用Post方式提交。 我们来看看Servlet给我们返回了什么东西： 正如我们在Servlet中写的那样，Servlet把HTTP请求中的部分参数给解析出来了。 因此，可以再翻到上面的Servlet重新去理解一遍Servlet的工作原理，可能会有更清晰的认识。 三十一、Servlet的局限性我们已经看到，Servlet如果需要给客户端返回数据，比如像下面这样的一个HTML文件： Servlet内部需要这样写输出语句： PrintWriter writer = response.getWriter(); writer.write(\"&lt;!DOCTYPE html&gt;\\n\" + \"&lt;html&gt;\\n\" + \"\\t&lt;head&gt;\\n\" + \"\\t\\t&lt;meta charset=\\\"UTF-8\\\"&gt;\\n\" + \"\\t\\t&lt;title&gt;标题标签&lt;/title&gt;\\n\" + \"\\t&lt;/head&gt;\\n\" + \"\\t&lt;body&gt;\\n\" + \"\\t\\t&lt;!--标题标签--&gt;\\n\" + \"\\t\\t&lt;h1&gt;公司简介&lt;/h1&gt;&lt;br /&gt;\\n\" + \"\\t\\t&lt;h2&gt;公司简介&lt;/h2&gt;&lt;br /&gt;\\n\" + \"\\t\\t&lt;h3&gt;公司简介&lt;/h3&gt;&lt;br /&gt;\\n\" + \"\\t\\t&lt;h4&gt;公司简介&lt;/h4&gt;&lt;br /&gt;\\n\" + \"\\t\\t\\n\" + \"\\t\\t&lt;!--加入一条水平线--&gt;\\n\" + \"\\t\\t&lt;hr /&gt;\\n\" + \"\\t\\t\\n\" + \"\\t\\t&lt;h5&gt;公司简介&lt;/h5&gt;&lt;br /&gt;\\n\" + \"\\t\\t&lt;h6&gt;公司简介&lt;/h7&gt;&lt;br /&gt;\\n\" + \"\\t\\t&lt;h100&gt;公司简介&lt;/h100&gt;\\n\" + \"\\t&lt;/body&gt;\\n\" + \"&lt;/html&gt;\\n\"); 即一行一行的把HTML语句给用Writer输出，早期简单的网页还能应付得住，但是随着互联网的不断发展，网站的内容和功能越来越强大，一个普通的HTML文件可能就达到好几百行，如果在采用使用Servlet去一行一行的输出HTML代码的话，将会非常的繁琐并且浪费大量的时间，且在当时，出现了PHP这种可以内嵌到HTML文件的动态语言，使得制作动态网页变得异常的简单和轻松，因此大量的程序员转上了PHP语言的道路，JAVA的份额急剧减小，当时JAVA的开发者Sun公司为了解决这个问题，也开发出了自己的动态网页生成技术，使得同样可以在HTML文件里内嵌JAVA代码，这就是现在的JSP技术，关于JSP技术的具体内容，我们将留到下一节进行讲解。 三十二、ServletContextListener（Servlet全局监听器）首先要说明的是，ServletContextListener是一个接口，我们随便写一个类，只要这个类实现了ServletContextListener接口，那么这个类就实现了【监听ServletContext】的功能。那么，这个神奇的接口是如何定义的呢？我们来看一下这个接口的内部情况： package javax.servlet; import java.util.EventListener; public interface ServletContextListener extends EventListener { void contextInitialized(ServletContextEvent var1); void contextDestroyed(ServletContextEvent var1); } 我们发现，在这个接口中只声明了两个方法，分别是void contextInitialized(ServletContextEvent var1)和void contextDestroyed(ServletContextEvent var1)方法，所以，我们很容易的就能猜测到，ServletContext的生命只有两种，分别是： 1.ServletContext初始化。（应用start时）———-&gt;Servlet容器调用void contextInitialized(ServletContextEvent var1) 2.ServletContext销毁。（应用stop时）———-&gt;Servlet容器调用 void contextDestroyed(ServletContextEvent var1) 因此，我们大概能够猜到ServletContextListener的工作机制了，当应用启动时，ServletContext进行初始化，然后Servlet容器会自动调用正在监听ServletContext的ServletContextListener的void contextInitialized(ServletContextEvent var1)方法，并向其传入一个ServletContextEvent对象。当应用停止时，ServletContext被销毁，此时Servlet容器也会自动地调用正在监听ServletContext的ServletContextListener的void contextDestroyed(ServletContextEvent var1)方法。 为了验证我们的猜测，我们来随便写一个类，并且实现ServletContextListener接口，即实现监听ServletContext的功能： import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; public class MyListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent servletContextEvent) { System.out.println(\"ServletContextListener.contextInitialized方法被调用\"); } @Override public void contextDestroyed(ServletContextEvent servletContextEvent) { System.out.println(\"ServletContextListener.contextDestroyed方法被调用\"); } } 然后，在web.xml中注册我们自己写的这个MyListener: &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\" version=\"4.0\"&gt; &lt;listener&gt; &lt;listener-class&gt;MyListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;/web-app&gt; 接下来，让我们启动一下Tomcat，看一看会发生什么吧！控制台打印信息如下： 我们发现，当应用启动时，ServletContextListener.contextInitialized（）方法被调用了。这其实是Servlet容器偷偷干的事情。那么，当我们停止Tomcat时，按照猜想，Servlet容器应该也会偷偷调用void contextDestroyed(ServletContextEvent var1)方法，来通知ServletContextListener监听器：ServletContext已经被销毁了。那么，事实是不是和我们猜想的一模一样呢？让我们来停止Tomcat的运行，看一看控制台的情况吧：我们发现，void contextDestroyed(ServletContextEvent var1)方法确实被Servlet容器调用了。因此，我们的猜想得到了证实。 三十三、【进阶】ServletContextListener在Spring中的应用如果基础好一点的童鞋，或者已经学过Spring框架的同学，建议阅读下面的内容，没有学过Spring也没有关系，可以先学或者学完之后再回头来看一看，Spring容器是如何借用ServletContextListener这个接口来实例化的。 首先让我们再来回顾一下ServletContext的概念，ServletContext翻译成中文叫做“Servlet上下文”或者“Servlet全局”，但是这个翻译我认为翻译的实在是有点牵强，也导致了许多的开发者不明白这个变量到底具体代表了什么。其实ServletContext就是一个“域对象”，它存在于整个应用中，并在在整个应用中有且仅有1份，它表示了当前整个应用的“状态”，你也可以理解为某个时刻的ServletContext代表了这个应用在某个时刻的“一张快照”，这张“快照”里面包含了有关应用的许多信息，应用的所有组件都可以从ServletContext获取当前应用的状态信息。ServletContext随着程序的启动而创建，随着程序的停止而销毁。通俗点说，我们可以往这个ServletContext域对象中“存东西”，然后也可以在别的地方中“取出来”。 我们知道，Spring容器可以通过： ApplicationContext ctx=new ClassPathXmlApplicationContext(“配置文件的路径”）; 显示地实例化一个Spring IOC容器。也可以像下面一样，在web.xml中注册Spring IOC容器： &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:applicationContext.xml &lt;/param-value&gt; &lt;/context-param&gt; 其中的监听器类【org.springframework.web.context.ContextLoaderListener】实现了ServletContextListener接口，能够监听ServletContext的生命周期中的“初始化”和“销毁”。注意，这个【org.springframework.web.context.ContextLoaderListener】监听器类当然不是我们自己写的哦，是人家Spring团队写的，我们只要拿来用就行了。当然，别忘记导入相关的Jar包。（spring-web-4.2.4.RELEASE.jar） 那么，Spring团队给我们提供的这个监听器类是如何实现：当ServletContext初始化后，Spring IOC容器也能跟着初始化的呢？怀着好奇心，让我们再来看一看【org.springframework.web.context.ContextLoaderListener】的内部实现情况吧。 package org.springframework.web.context; import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; public class ContextLoaderListener extends ContextLoader implements ServletContextListener { public ContextLoaderListener() { } public ContextLoaderListener(WebApplicationContext context) { super(context); } --------------------------------------------------------重点关注下面这里哦！----------------------------------------------------------------------- public void contextInitialized(ServletContextEvent event) { this.initWebApplicationContext(event.getServletContext()); } --------------------------------------------------------重点关注上面这里哦！----------------------------------------------------------------------- public void contextDestroyed(ServletContextEvent event) { this.closeWebApplicationContext(event.getServletContext()); ContextCleanupListener.cleanupAttributes(event.getServletContext()); } } 我们发现，【org.springframework.web.context.ContextLoaderListener】这个类实现了ServletContextListener接口中的两个方法，其中，当ServletContext初始化后， public void contextInitialized(ServletContextEvent event)方法被调用，接下来执行initWebApplicationContext(event.getServletContext())方法，但是我们发现这个方法并没有在这个类中声明，因此，我们再看一下其父类中是如何声明的： public WebApplicationContext initWebApplicationContext(ServletContext servletContext) { if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) { throw new IllegalStateException(\"Cannot initialize context because there is already a root application context present - check whether you have multiple ContextLoader* definitions in your web.xml!\"); } else { Log logger = LogFactory.getLog(ContextLoader.class); servletContext.log(\"Initializing Spring root WebApplicationContext\"); if (logger.isInfoEnabled()) { logger.info(\"Root WebApplicationContext: initialization started\"); } long startTime = System.currentTimeMillis(); try { if (this.context == null) { this.context = this.createWebApplicationContext(servletContext); } if (this.context instanceof ConfigurableWebApplicationContext) { ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext)this.context; if (!cwac.isActive()) { if (cwac.getParent() == null) { ApplicationContext parent = this.loadParentContext(servletContext); cwac.setParent(parent); } this.configureAndRefreshWebApplicationContext(cwac, servletContext); } } servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) { currentContext = this.context; } else if (ccl != null) { currentContextPerThread.put(ccl, this.context); } if (logger.isDebugEnabled()) { logger.debug(\"Published root WebApplicationContext as ServletContext attribute with name [\" + WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + \"]\"); } if (logger.isInfoEnabled()) { long elapsedTime = System.currentTimeMillis() - startTime; logger.info(\"Root WebApplicationContext: initialization completed in \" + elapsedTime + \" ms\"); } return this.context; } catch (RuntimeException var8) { logger.error(\"Context initialization failed\", var8); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, var8); throw var8; } catch (Error var9) { logger.error(\"Context initialization failed\", var9); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, var9); throw var9; } } } 分析到这一步，我们发现Spring容器在这个方法中被实例化了。接下来，就让我们整理一下整体的思路： 当Servlet容器启动时，ServletContext对象被初始化，然后Servlet容器调用web.xml中注册的监听器的 public void contextInitialized(ServletContextEvent event) 方法，而在监听器中，调用了this.initWebApplicationContext(event.getServletContext())方法，在这个方法中实例化了Spring IOC容器。即ApplicationContext对象。 因此，当ServletContext创建时我们可以创建applicationContext对象，当ServletContext销毁时，我们可以销毁applicationContext对象。这样applicationContext就和ServletContext“共生死了”。————————————————版权声明：本文为CSDN博主「刘扬俊」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_19782019/article/details/80292110","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"web","slug":"web","permalink":"https://13592491893.github.io/tags/web/"},{"name":"servlet","slug":"servlet","permalink":"https://13592491893.github.io/tags/servlet/"}]},{"title":"SQL语法","slug":"SQL语法","date":"2021-05-20T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/62230.html","link":"","permalink":"https://13592491893.github.io/posts/62230.html","excerpt":"","text":"SQL语法一、视图1. sqlserver:a.sys_sh_rzp_item_mate_view: SELECT a.EnterpriseId,b.ItemCode,c.ItemCode mateItemCode,a.MateItemTypeCode,a.MateItemTypeName,a.mateitemweight,a.DisPlayOrder,a.IsWeight,a.PreciseScope, a.MateItemTime,a.VersionCode,a.VersionName,d.OrgCode,a.IsEnabled,a.IsUsed,a.IsRepeat,a.IsWhole,a.IsUseBag,a.UseBagNumber FROM dbo.sys_sh_rzp_item_mate a LEFT JOIN dbo.sys_sh_item_master b ON b.EnterpriseId = a.EnterpriseId AND b.ItemId=a.ItemId LEFT JOIN sys_sh_item_master c ON c.EnterpriseId=a.EnterpriseId AND c.ItemId=a.MateItemId LEFT JOIN dbo.sys_sh_org d ON d.EnterpriseId=a.EnterpriseId AND d.OrgId=a.OrgId b.sys_sh_rzp_material_contrast_view: SELECT c.EnterpriseId, c.ExItemId, exitem.ItemCode ExItemCode, exitem.ItemName ExItemName, c.ItemId, item.ItemCode, item.ItemName, c.ItemType, c.DisPlayOrder, c.OrgId, o.OrgCode, o.OrgName, c.CreateUserCode, c.CreateUserName, c.CreateDateTime, c.EditUserCode, c.EditUserName, c.EditDateTime FROM sys_sh_rzp_material_contrast c LEFT JOIN sys_sh_item_master exitem ON exitem.EnterpriseId = c.EnterpriseId AND exitem.ItemId = c.ExItemId LEFT JOIN sys_sh_item_master item ON item.EnterpriseId=c.EnterpriseId AND item.ItemId = c.ItemId LEFT JOIN dbo.sys_sh_org o ON o.EnterpriseId=c.EnterpriseId AND o.OrgId=c.OrgId 二、函数1. sqlservera.funGetChildItemType: -- ============================================= -- Author: LH -- Create date: 2020-12-09 -- Description: 根据类别代码获取所有子类别 --SELECT * FROM funGetChildItemType(3,'RZP') -- ============================================= ALTER FUNCTION [dbo].[funGetChildItemType] ( @enterpriseId INTEGER,---企业ID @ParentItemTypeCode VARCHAR(30)---类别代码 ) RETURNS TABLE AS RETURN ( WITH cr AS (SELECT c.EnterpriseId,c.ItemTypeId,c.ItemTypeCode,c.ItemTypeName FROM sys_sh_item_type c WHERE c.EnterpriseId=@enterpriseId AND EXISTS(SELECT * FROM f_split(@ParentItemTypeCode,',') f WHERE f.item = c.ItemTypeCode ) UNION ALL SELECT c.EnterpriseId,c.ItemTypeId,c.ItemTypeCode,c.ItemTypeName FROM sys_sh_item_type c INNER JOIN cr ON cr.EnterpriseId = c.EnterpriseId AND c.ParentItemTypeId=cr.ItemTypeId ) SELECT cr.EnterpriseId,ItemTypeId,ItemTypeCode,ItemTypeName FROM cr ) 使用方法：在sql语句里直接使用 b.proc_commitflight:交班 /* Description： 生成交接班记录 Create By：LH Create Date：2021-06-02 Version:2021.7.6.1 EXEC proc_commitflight '2021/06/09','bc03','前夜','999','管理员' */ ALTER PROC proc_commitflight( @agitatedate varchar(10)='2021-06-02', --工作日期 @flightscode VARCHAR(10)='bc02', --班次代码 @flightsname VARCHAR(10)='白班', --班次名称 @usercode VARCHAR(10), @username VARCHAR(10), @warehousecode VARCHAR(10), @enterpriseId VARCHAR(10), @resultnum INT=0 OUTPUT ) AS BEGIN BEGIN TRAN SET NOCOUNT ON SET ANSI_WARNINGS OFF DECLARE @foreflightscode VARCHAR(10)='' DECLARE @foreagitatedate VARCHAR(10)=@agitatedate SELECT TOP 1 @foreagitatedate=agitateDate,@foreflightscode=flightscode FROM sys_sh_rzp_material_flightsstock WHERE warehouseCode=@warehousecode ORDER BY agitateDate DESC,flightscode DESC /* IF @flightscode = 'bc02' SET @foreflightscode='bc01' ELSE IF @flightscode = 'bc03' BEGIN SET @foreflightscode='bc02' END ELSE IF @flightscode = 'bc01' BEGIN SET @foreflightscode='bc03' SET @foreagitatedate=CONVERT(varchar(10),DATEADD(DAY,-1,CAST(@agitatedate as DATE)),111) END*/ CREATE TABLE #inventory(warehousecode VARCHAR(20),warehousename VARCHAR(20),flightscode VARCHAR(20),flightsname VARCHAR(20), itemcode VARCHAR(20),itemname VARCHAR(50),firstweight DECIMAL(18,4),inweight DECIMAL(18,4),outweight DECIMAL(18,4), stockweight DECIMAL(18,4),nowweight DECIMAL(18,4)) ---库存盘点 期初 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT warehousecode,warehousename,@flightscode flightscode,@flightsname flightsname,itemcode,itemname, ISNULL(stockweight,0.0000) firstweight,0.0000 inweight,0.0000 outweight,0.0000 stockweight,0.0000 nowweight FROM sys_sh_rzp_material_flightsstock m WHERE EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=m.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=m.itemcode) AND EnterpriseId=@enterpriseId AND agitatedate=@foreagitatedate AND flightscode=@foreflightscode AND warehouseCode=@warehousecode AND ISNULL(stockweight,0)&gt;0 --原料接收 入库 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT i.warehouseCode,i.warehousename,d.flightscode,d.flightsname,d.itemCode,d.ItemName, 0.0000,SUM(ISNULL(d.RealWeight,0.0000)),0.0000,0.0000,0.0000 FROM sys_sh_rzp_material_incept_detail d INNER JOIN sys_sh_rzp_material_incept i ON i.EnterpriseId = d.EnterpriseId AND i.seq = d.seq WHERE EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=d.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=d.itemcode) AND d.EnterpriseId=@enterpriseId AND d.agitatedate=@agitatedate AND d.flightscode=@flightscode AND i.warehouseCode=@warehousecode GROUP BY i.warehousecode,i.warehousename,d.flightscode,d.flightsname,d.itemCode,d.ItemName --原料移库 出库 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT OutWareHouseCode,OutWareHouseName,flightscode,flightsname,itemCode,ItemName, 0.0000,0.0000,SUM(ISNULL(netweight,0.0000)),0.0000,0.0000 FROM sys_sh_rzp_materialmove m WHERE isDown!='Y' AND EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=m.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=m.itemcode) AND EnterpriseId=@enterpriseId AND agitatedate=@agitatedate AND flightscode=@flightscode AND OutWareHouseCode=@warehousecode GROUP BY OutWareHouseCode,OutWareHouseName,flightscode,flightsname,itemCode,ItemName --原料移库 入库 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT InWareHouseCode,InWareHouseName,flightscode,flightsname,itemCode,ItemName, 0.0000,SUM(ISNULL(netweight,0.0000)),0.0000,0.0000,0.0000 FROM sys_sh_rzp_materialmove m WHERE isDown!='Y' AND EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=m.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=m.itemcode) AND EnterpriseId=@enterpriseId AND agitatedate=@agitatedate AND flightscode=@flightscode AND issign='Y' AND InWareHouseCode=@warehousecode GROUP BY InWareHouseCode,InWareHouseName,flightscode,flightsname,itemCode,ItemName --原料销售 出库 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT warehousecode,warehousename,flightscode,flightsname,itemCode,ItemName, 0,0,SUM(ISNULL(netweight,0.0000)),0.0000,0.0000 FROM sys_sh_rzp_materialsale m WHERE EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=m.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=m.itemcode) AND EnterpriseId=@enterpriseId AND agitatedate=@agitatedate AND flightscode=@flightscode AND warehousecode=@warehousecode GROUP BY warehousecode,warehousename,flightscode,flightsname,itemCode,ItemName --原料领用 出库 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT warehousecode,warehousename,flightscode,flightsname,itemCode,ItemName, 0.0000,0.0000,SUM(ISNULL(InputWeight,0.0000)),0.0000,0.0000 FROM sys_sh_rzp_material_xinput WHERE EnterpriseId=@enterpriseId AND agitatedate=@agitatedate AND flightscode=@flightscode AND warehousecode=@warehousecode GROUP BY warehousecode,warehousename,flightscode,flightsname,itemCode,ItemName --盘点库存 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT warehousecode,warehousename,@flightscode,@flightsname,itemCode,ItemName, 0.0000,0.0000,0.0000,SUM(ISNULL(qtyweight,0.0000)),0.0000 FROM dbo.sys_sh_rzp_inventory_pda m WHERE (iscancel IS NULL OR iscancel&lt;&gt;'Y') AND EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=m.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=m.itemcode) AND agitatedate=@agitatedate AND flightscode=@flightscode AND warehousecode=@warehousecode GROUP BY warehousecode,warehousename,itemCode,ItemName --实时库存 INSERT INTO #inventory(warehousecode,warehousename,flightscode,flightsname,itemcode,itemname, firstweight,inweight,outweight,stockweight,nowweight) SELECT warehousecode,warehousename,@flightscode,@flightsname,itemCode,ItemName, 0.0000,0.0000,0.0000,0.0000,SUM(ISNULL(nowWeight,0.0000)) FROM sys_sh_rzp_materialware m WHERE EXISTS(SELECT 1 FROM sys_sh_rzp_material_contrast_view c WHERE c.EnterpriseId=m.EnterpriseId AND c.itemtype='Z' AND c.exitemcode=m.itemcode) AND EnterpriseId=@enterpriseId AND nowweight&gt;0 AND warehousecode=@warehousecode GROUP BY warehousecode,warehousename,itemCode,ItemName INSERT INTO sys_sh_rzp_material_flightsstock(enterpriseId,warehousecode,warehousename,agitateDate,flightscode,flightsname,itemcode,itemname,exweight,inweight, outweight,StockWeight,theorystockweight,usercode,username,entryDate,entryTime) SELECT @enterpriseId,warehousecode,warehousename,@agitateDate,flightscode,flightsname,itemcode,itemname,SUM(ISNULL(firstweight,0.0000)),SUM(ISNULL(inweight,0.0000)), SUM(ISNULL(outweight,0.0000)),SUM(ISNULL(stockweight,0.0000)),SUM(ISNULL(nowweight,0.0000)),@usercode,@username,CONVERT(varchar(10),GETDATE(),111),CONVERT(varchar(100), GETDATE(), 8) FROM #inventory GROUP BY warehousecode,warehousename,flightscode,flightsname,itemCode,ItemName SET @resultnum=@@ROWCOUNT IF @@ERROR&lt;&gt;0 GOTO errhandle DROP TABLE #inventory COMMIT TRAN errhandle: IF @@ERROR&lt;&gt;0 BEGIN DROP TABLE #inventory SET @resultnum=-1 ROLLBACK TRAN END END ​ 使用方法： 三、建DBlink### 1.sqlserver 移库同步数据 EXEC sp_addlinkedserver @server='10.1.6.14',--被访问的服务器别名（习惯上直接使用目标服务器IP，或取个别名如：JOY） @srvproduct='', @provider='SQLOLEDB', @datasrc='10.1.6.14' --要访问的服务器 2. ```sql EXEC sp_addlinkedsrvlogin '10.1.6.14', --被访问的服务器别名（如果上面sp_addlinkedserver中使用别名JOY，则这里也是JOY） 'false', NULL, 'sa', --帐号 'shxxzx123!@#' --密码 建触发器 create trigger trig_move_insert on sys_sh_rzp_materialmove after insert as BEGIN declare @names nvarchar(50)=null; set @names=(select top(1) name from dbo.test1 order by id desc ); if(select @names) is not null begin insert into dbo.test2(name) values(@names); end END; 参考：https://www.cnblogs.com/vuenote/p/9765156.html 四、语法1.exists和in参考：https://www.cnblogs.com/emilyyoucan/p/7833769.html select a.* from A a where exists(select 1 from B b where a.id=b.id) 以上查询使用了exists语句,exists()会执行A.length次,它并不缓存exists()结果集,因为exists()结果集的内容并不重要,重要的是结果集中是否有记录,如果有则返回true,没有则返回false. 它的查询过程类似于以下过程 List resultSet=[]; Array A=(select * from A) for(int i=0;i&lt;A.length;i++) { if(exists(A[i].id) { //执行select 1 from B b where b.id=a.id是否有记录返回 resultSet.add(A[i]); } } return resultSet; 当B表比A表数据大时适合使用exists(),因为它没有那么遍历操作,只需要再执行一次查询就行. 如:A表有10000条记录,B表有1000000条记录,那么exists()会执行10000次去判断A表中的id是否与B表中的id相等. 如:A表有10000条记录,B表有100000000条记录,那么exists()还是执行10000次,因为它只执行A.length次,可见B表数据越多,越适合exists()发挥效果. 再如:A表有10000条记录,B表有100条记录,那么exists()还是执行10000次,还不如使用in()遍历10000*100次,因为in()是在内存里遍历比较,而exists()需要查询数据库,我们都知道查询数据库所消耗的性能更高,而内存比较很快. 结论:exists()适合B表比A表数据大的情况 当A表数据与B表数据一样大时,in与exists效率差不多,可任选一个使用. 2.rownuma.sqlserverrow_number函数 select row_number() over(order by seq) rownum from fix_sh_card; 此方法把括号里的查询结果放到变量:temp 里面( 我也不确定是不是变量), 并用row_number()函数进行一个行号跟踪, 再用over 函数进行一个列的排序规则( 是这必须的), 并指定列名为'rownum' b.oracle--直接使用rownum select rownum,id,name from student where rownum=1 扩展：oracle中rownum和row_number()row_number()over(partition by col1 order by col2)表示根据col1分组，在分组内部根据col2排序，而此函数计算的值就表示每组内部排序后的顺序编号（组内连续的唯一的）。 与rownum的区别在于：使用rownum进行排序的时候是先对结果集加入伪劣rownum然后再进行排序，而row_number()在包含排序从句后是先排序再计算行号码 详细查看：https://www.cnblogs.com/CandiceW/p/6869167.html 3.stuff和for xml path原表数据： 把isInventory分组，seq加起来用“，”隔开 select IsInventory, seqs = ( stuff( (select ',' + CAST (seq as varchar) from fix_sh_inventory_set where IsInventory = A.IsInventory for xml path('')), 1, 1, '' ) ) from fix_sh_inventory_set as A group by IsInventory 解析：stuff(param1, startIndex, length, param2)将param1中自startIndex(SQL中都是从1开始，而非0)起，删除length个字符，然后用param2替换删掉的字符。 select STUFF('abcdefg',1,0,'1234') --结果为'1234abcdefg' select STUFF('abcdefg',1,1,'1234') --结果为'1234bcdefg' select STUFF('abcdefg',2,1,'1234') --结果为'a1234cdefg' select STUFF('abcdefg',2,2,'1234') --结果为'a1234defg' for xml path是将查询结果集以XML形式展现 select ',' + CAST (seq as varchar) from fix_sh_inventory_set for xml path 比上面多个括号： select ',' + CAST (seq as varchar) from fix_sh_inventory_set for xml path('') 记住下面这个固定格式： 4.union all​ union all只是合并查询结果，并不会进行去重和排序操作，在没有去重的前提下，使用union all的执行效率要比union高 union: 对两个结果集进行并集操作, 不包括重复行,相当于distinct, 同时进行默认规则的排序; union all: 对两个结果集进行并集操作, 包括重复行, 即所有的结果全部显示, 不管是不是重复; union: 会对获取的结果进行排序操作 union all: 不会对获取的结果进行排序操作 5.查最大编号 老语句： SELECT IsNull(SUBSTRING(MAX(filmNo),LEN(MAX(filmNo)) ,1),0)+1 number 因为filmno是字符类型，直接用MAX函数比较最后一位值的时候永远是9大,就是最后一个’-‘后面是10或者11,用MAX函数取值的时候也是9,造成新生成的最大number一直是10 改后的语句: SELECT isnull(max(cast(REPLACE( SUBSTRING(filmNo, len(filmno)-1, 2) , '-', '') AS int)),0)+1 number 注意：max比较的时候要用CAST函数把字符转为int SELECT 'BK' + '3' + convert(varchar,getdate(),12) +RIGHT('000' + CAST(isnull(SUBSTRING(MAX(inwareSeq), len(MAX(inwareSeq)) - 3, 4), 0) + 1 AS nvarchar(5)), 4) from fix_sh_spareparts_inware where inwareSeq like 'BK3210524%' and enterpriseId = 3 6.convertSelect CONVERT(varchar(100), GETDATE(), 0) : 01 26 2021 11:32PM Select CONVERT(varchar(100), GETDATE(), 1) : 01/26/21 Select CONVERT(varchar(100), GETDATE(), 2) : 21.01.26 Select CONVERT(varchar(100), GETDATE(), 3) : 26/01/21 Select CONVERT(varchar(100), GETDATE(), 4) : 26.01.21 Select CONVERT(varchar(100), GETDATE(), 5) : 26-01-21 Select CONVERT(varchar(100), GETDATE(), 6) : 26 01 21 Select CONVERT(varchar(100), GETDATE(), 7) : 01 26, 21 Select CONVERT(varchar(100), GETDATE(), 8) : 23:35:25 Select CONVERT(varchar(100), GETDATE(), 9) : 01 26 2021 11:35:34:873PM Select CONVERT(varchar(100), GETDATE(), 10) : 01-26-21 Select CONVERT(varchar(100), GETDATE(), 11) : 21/01/26 Select CONVERT(varchar(100), GETDATE(), 12) : 210126 Select CONVERT(varchar(100), GETDATE(), 13) : 26 01 2021 23:36:08:347 Select CONVERT(varchar(100), GETDATE(), 14) : 23:36:15:593 Select CONVERT(varchar(100), GETDATE(), 20) : 2021-01-26 23:36:22 Select CONVERT(varchar(100), GETDATE(), 21) : 2021-01-26 23:36:29.990 Select CONVERT(varchar(100), GETDATE(), 22) : 01/26/21 11:36:41 PM Select CONVERT(varchar(100), GETDATE(), 23) : 2021-01-26 Select CONVERT(varchar(100), GETDATE(), 24) : 23:36:58 Select CONVERT(varchar(100), GETDATE(), 25) : 2021-01-26 23:37:05.717 Select CONVERT(varchar(100), GETDATE(), 100) : 01 26 2021 11:37PM Select CONVERT(varchar(100), GETDATE(), 101) : 01/26/2021 Select CONVERT(varchar(100), GETDATE(), 102) : 2021.01.26 Select CONVERT(varchar(100), GETDATE(), 103) : 26/01/2021 Select CONVERT(varchar(100), GETDATE(), 104) : 26.01.2021 Select CONVERT(varchar(100), GETDATE(), 105) : 26-01-2021 Select CONVERT(varchar(100), GETDATE(), 106) : 26 01 2021 Select CONVERT(varchar(100), GETDATE(), 107) : 01 26, 2021 Select CONVERT(varchar(100), GETDATE(), 108) : 23:38:38 Select CONVERT(varchar(100), GETDATE(), 109) : 01 26 2021 11:38:45:950PM Select CONVERT(varchar(100), GETDATE(), 110) : 01-26-2021 Select CONVERT(varchar(100), GETDATE(), 111) : 2021/01/26 Select CONVERT(varchar(100), GETDATE(), 112) : 20210126 Select CONVERT(varchar(100), GETDATE(), 113) : 26 01 2021 23:39:18:693 Select CONVERT(varchar(100), GETDATE(), 114) : 23:39:25:183 Select CONVERT(varchar(100), GETDATE(), 120) : 2021-01-26 23:39:45 Select CONVERT(varchar(100), GETDATE(), 121) : 2021-01-26 23:39:53.440 Select CONVERT(varchar(100), GETDATE(), 126) : 2021-01-26T23:40:00.727 Select CONVERT(varchar(100), GETDATE(), 130) : 13 ????? ??????? 1442 11:40:07:847PM Select CONVERT(varchar(100), GETDATE(), 131) : 13/06/1442 11:40:14:917PM 7.cast语法：CAST (expression AS data_type) 8.DATEADDDATEADD() 函数在日期中添加或减去指定的时间间隔。语法：DATEADD(datepart,number,date) 9.with aswith cte1 as ( select * from table1 where name like 'abc%' ), cte2 as ( select * from table2 where id &gt; 20 ), cte3 as ( select * from table3 where price &lt; 100 ) select a.* from cte1 a, cte2 b, cte3 c where a.id = b.id and a.id = c.id --CTE后面必须直接跟使用CTE的SQL语句（如select、insert、update等），否则，CTE将失效。如下面的SQL语句将无法正常使用CTE： 10.left join一对多只保留一条结果的解决方法select * from （select user_id from tabel_1）a left join (select user_id from tabel_2 group by user_id) b on a.user_id = b.user_id","categories":[{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/categories/sql/"}],"tags":[{"name":"命令","slug":"命令","permalink":"https://13592491893.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/tags/sql/"},{"name":"语法","slug":"语法","permalink":"https://13592491893.github.io/tags/%E8%AF%AD%E6%B3%95/"}]},{"title":"spring日志","slug":"spring日志","date":"2021-04-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/5502.html","link":"","permalink":"https://13592491893.github.io/posts/5502.html","excerpt":"","text":"spring日志今天来谈一谈日志，主要是说一说springboot的日志，因为最近在学习springboot。首先在写代码的时候，要养成记日志的习惯，这点真的很重要，因为之前吃了很多亏。过去我对日志很不在意，该有的日志没有，不该有的日志却随意输出。新换的工作，上司对日志有严格的要求，也就慢慢开始注意了。 一般而言，一个接口或者说一段程序，其入口要有日志，记录传入的数据是什么；部分重要的处理逻辑要有日志输出；程序出口也要有日志，记录其最终的处理结果。这样在解决生产上的问题时，可以很快的定位问题的位置，是传入数据的问题还是我们代码逻辑写错了，总比凭空想象的好，要相信计算机，日志是不会骗人的。 还有一点，在生产上严禁使用System.out输出，性能太低，原因是System.out输出会导致线程等待（同步），而使用Logger输出线程不等待日志的输出（异步），而继续执行。 接下来看一看springboot的日志配置,说一下把日志记录到文件中的配置方式。 一、工具/原料 springboot 日志 二、方法/步骤 springboot推荐的日志类库是slf4j、日志系统为logback,确实我回头一看项目中使用的都是slf4j，说明这个东西确实有他的优点。 上文中也说了一点，slf4j有个接口叫Logger，提供了丰富的日志输出方法，包含了所有日志级别的输出。使用方式也是特别的简单，用slf4j的工厂类获取一个logger ，然后就可以输出日志了，默认情况下，日志只会输出到控制台。 通过在application.properties文件中配置logging.file、logging.path可以控制日志文件的输出路径和文件名。 不过有些细节需要注意，否则配置不生效，我测试了几种情况。 如果，两者都配置了：logging.file=myLog.log、logging.path=D:/data/mylog，注意windos的路径（后面配置文件中也是/），此时并不会在d盘下生成日志文件，只会在项目的根目录下创建一个myLog.log的文件（workspace中，此项目的根目录）。 其原因是，没有logback-spring.xml配置文件，系统只认识logging.file，不认识logging.path。 所以要配置logback-spring.xml，spring boot会默认加载此文件，为什么不配置logback.xml,因为logback.xml会先application.properties加载，而logback-spring.xml会后于application.properties加载，这样我们在application.properties文中设置日志文件名称和文件路径才能生效。 且看logback-spring.xml的配置详情。注意${LOG_PATH}和${LOG_FILE}分别是获取配置文件中的路径和文件名称，必须使用这两个全局的配置去获取。然后重启项目，发现在配置的目录下，有了相应的日志文件。 日志文件的配置结构： （1）FILE_LOG_PATTERN：日志输出格式变量，在控制台输出和文件中输出的append中都引用了此变量。（2）consoleLog：定义一个控制台的appender（3）fileLog：定义一个日志文件的appender，这就是文件输出的详细配置，是日志文件的输出地址：必须要为${LOG_PATH}/${LOG_FILE}，这样我们在application.properties中的配置才有效。level标签：如果我们设置了level为info,只会输出info的日志信息，其他日志级别的日志就会过滤掉，建议不配置level属性。 （4）logger：其name就是项目中对应的包路径，appender-ref是appender的引用，在本配置文件中，意思就是com.example.xyx.MySpringBootTest包下文件的日志，按照fileLog的配置去输出，即按照FILE_LOG_PATTERN的格式，输出到D:/data/mylog/myLog.log文件中。 标签level=”debug”是设置日志级别：作用是debug级别及其以上级别的日志会输出（debug、info、warn、error,,,），注意此处的level是一个下线，比其日志级别高的日志信息也会输出，很重要。 additivity=”false”是配置此logger是否提交给其他的logger或者root节点，如果true，则root也会执行或者其他的可以拦截到的logger节点,且logger的level优先级高;否则不会执行，在本配置文件中即控制台不会输出com.example.xyx.MySpringBootTest包下文件的日志。 （5）root：根节点，在logback-spring.xml中只引用了控制台日志输出配置，不会输出到文件，如果想输出到文件，可以写再写一个引用。level=info，在控制台输出into级别及其以上级别的日志。会拦截所有包下的日志，但是其输出会受到logger的影响，即注意logger中的additivity属性，如果为false，com.example.xyx.MySpringBootTest包下的日志不会输出到控制台。 三、肉制品同步程序的logback文件 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;configuration scan=\"false\"&gt; &lt;conversionRule conversionWord=\"seq\" converterClass=\"com.shys.iiot.rzp.sync.utils.LocalSeqNumberConverter\"/&gt; &lt;property resource=\"application.properties\"/&gt; &lt;!--日志文件的名称，根据系统自动追加日期和后缀 --&gt; &lt;property name=\"LOG_FILE_NAME\" value=\"shys-iiot-rzp-sync\"/&gt; &lt;!-- 彩色日志依赖的渲染类 &lt;conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\" /&gt; &lt;conversionRule conversionWord=\"wex\" converterClass=\"org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter\" /&gt; &lt;conversionRule conversionWord=\"wEx\" converterClass=\"org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter\" /&gt; --&gt; &lt;!-- %-5p %是格式修饰符， -指左对齐 5指最小字符 --&gt; &lt;!-- &lt;property name=\"CONSOLE_LOG_PATTERN\" value=\"%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(-){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}\" /&gt; --&gt; &lt;!-- 默认Sleuth会在MDC中添加[appname,traceId,spanId,exportable]： spanId - the id of a specific operation that took place appname - the name of the application that logged the span traceId - the id of the latency graph that contains the span exportable - whether the log should be exported to Zipkin or not. Whenwould you like the span not to be exportable? In the case in which you want towrap some operation in a Span and have it written to the logs only. --&gt; &lt;property name=\"CONSOLE_LOG_PATTERN\" value=\"%d{yyyy-MM-dd HH:mm:ss.SSS} %-2seq %-5p --- serviceId:[${spring.application.name:-}] user[%X{userSessionId},%X{enterpriseID}] traceId:[%X{X-B3-TraceId}] [%15.15t] %-40.40logger{39} : %m%n\"/&gt; &lt;!-- 控制台输出，开发调试可以在下文root中添加项 --&gt; &lt;appender name=\"stdout\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder&gt; &lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--dev环境 输出到LogStash property(\"apollo.env\").equalsIgnoreCase(\"dev\") &lt;if condition='property(\"apollo.env\").equalsIgnoreCase(\"dev\")'&gt; &lt;then&gt; &lt;/then&gt; &lt;/if&gt; &lt;appender name=\"logstash\" class=\"net.logstash.logback.appender.LogstashTcpSocketAppender\"&gt; &lt;destination&gt;10.1.5.20:5000&lt;/destination&gt; &lt;encoder class=\"net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\"&gt; &lt;providers&gt; &lt;pattern&gt; &lt;pattern&gt; { \"timestamp\": \"%date{\\\"yyyy-MM-dd'T'HH:mm:ss,SSSZ\\\"}\", \"seq\":\"#asLong{%seq}\", \"serviceId\": \"${spring.application.name}\", \"traceId\": \"%X{X-B3-TraceId}\", \"userId\": \"%X{userSessionId}\", \"enterpriseID\": \"%X{enterpriseID}\", \"log_level\": \"%level\", \"thread\": \"%thread\", \"class_name\": \"%class\", \"message\": \"%message\", \"stack_trace\": \"%exception{5}\" } &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;/encoder&gt; &lt;/appender&gt; --&gt; &lt;!-- 每天生成一个日志文件 --&gt; &lt;appender name=\"logfile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;/home/logs/${LOG_FILE_NAME}/${LOG_FILE_NAME}.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;!--日志文件输出的文件路径和文件名，不要修改 --&gt; &lt;FileNamePattern&gt;/home/logs/${LOG_FILE_NAME}/${LOG_FILE_NAME}.%d{yyyy-MM-dd}.%i.log &lt;/FileNamePattern&gt; &lt;!--日志文件保留天数，默认30天 --&gt; &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"errorfile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;/home/logs/${LOG_FILE_NAME}/${LOG_FILE_NAME}_ERROR.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!--日志文件输出的文件路径和文件名，不要修改 --&gt; &lt;FileNamePattern&gt;/home/logs/${LOG_FILE_NAME}_ERROR/${LOG_FILE_NAME}.%d{yyyy-MM-dd}.log &lt;/FileNamePattern&gt; &lt;!--日志文件保留天数，默认30天 --&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;${CONSOLE_LOG_PATTERN}&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;logger name=\"com.netflix.discovery\" level=\"WARN\"/&gt; &lt;logger name=\"com.netflix.loadbalancer\" level=\"WARN\"/&gt; &lt;logger name=\"org.springframework\" level=\"INFO\"/&gt; &lt;logger name=\"org.apache\" level=\"WARN\"/&gt; &lt;logger name=\"com.shys\" level=\"DEBUG\"/&gt; &lt;logger name=\"druid.sql\" level=\"DEBUG\"&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;appender-ref ref=\"logfile\"/&gt; &lt;/logger&gt; &lt;!--&lt;logger name=\"com.ctrip\" level=\"INFO\"&gt; &lt;/logger&gt; --&gt; &lt;!-- &lt;logger name=\"org.apache.ibatis\" level=\"ERROR\"&gt; &lt;appender-ref ref=\"stdout\" /&gt; &lt;appender-ref ref=\"logfile\" /&gt; &lt;/logger&gt; 就是这个监控了mybatis日志输出，配合上面的“dao” &lt;logger name=\"mybatisDao\" level=\"DEBUG\"/&gt; --&gt; &lt;!-- &lt;logger name=\"java.sql.PreparedStatement\" level=\"DEBUG\"&gt; &lt;appender-ref ref=\"stdout\" /&gt; &lt;appender-ref ref=\"logfile\" /&gt; &lt;/logger&gt; --&gt; &lt;!-- &lt;logger name=\"org.spring.springboot.dao\" level=\"DEBUG\"&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;/logger&gt; --&gt; &lt;logger name=\"org.redisson\" level=\"ERROR\"&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;/logger&gt; &lt;root level=\"DEBUG\"&gt; &lt;appender-ref ref=\"stdout\"/&gt; &lt;appender-ref ref=\"logfile\"/&gt; &lt;appender-ref ref=\"errorfile\"/&gt; &lt;!-- &lt;appender-ref ref=\"logstash\" /&gt; --&gt; &lt;/root&gt; &lt;/configuration&gt; package com.shys.iiot.rzp.sync.utils; import ch.qos.logback.classic.pattern.ClassicConverter; import ch.qos.logback.classic.spi.ILoggingEvent; import java.util.concurrent.atomic.AtomicInteger; public class LocalSeqNumberConverter extends ClassicConverter { long lastTimestamp = 0; AtomicInteger sequenceNumber = new AtomicInteger(1); @Override public String convert(ILoggingEvent le) { long now = le.getTimeStamp(); if (lastTimestamp != now) { lastTimestamp = now; sequenceNumber.getAndSet(1); } return Long.toString(sequenceNumber.getAndIncrement()); } }","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://13592491893.github.io/tags/spring/"},{"name":"日志","slug":"日志","permalink":"https://13592491893.github.io/tags/%E6%97%A5%E5%BF%97/"},{"name":"log","slug":"log","permalink":"https://13592491893.github.io/tags/log/"}]},{"title":"Gitee快速设置","slug":"Gitee快速设置","date":"2021-04-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.623Z","comments":true,"path":"posts/49456.html","link":"","permalink":"https://13592491893.github.io/posts/49456.html","excerpt":"","text":"Gitee快速设置git全局设置git config --global user.name \"时鸿运\" git config --global user.email \"459173919@qq.com\" 创建 git 仓库mkdir tt cd tt git init touch README.md git add README.md git commit -m \"first commit\" git remote add origin https://gitee.com/gsshy/tt.git git push -u origin master 已有仓库?cd existing_git_repo git remote add origin https://gitee.com/gsshy/tt.git git push -u origin master","categories":[{"name":"git","slug":"git","permalink":"https://13592491893.github.io/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://13592491893.github.io/tags/git/"},{"name":"gitee","slug":"gitee","permalink":"https://13592491893.github.io/tags/gitee/"},{"name":"config","slug":"config","permalink":"https://13592491893.github.io/tags/config/"}]},{"title":"feign使用","slug":"feign使用","date":"2021-03-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/8056.html","link":"","permalink":"https://13592491893.github.io/posts/8056.html","excerpt":"","text":"feign使用代码： 提供方： 调用方： Spring Cloud Feign中的@RequestParam,@RequestBody,@PathVariable：https://faceghost.com/article/217761","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"rpc","slug":"rpc","permalink":"https://13592491893.github.io/tags/rpc/"},{"name":"远程调用","slug":"远程调用","permalink":"https://13592491893.github.io/tags/%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"feign","slug":"feign","permalink":"https://13592491893.github.io/tags/feign/"}]},{"title":"Executor框架","slug":"Executor框架","date":"2021-02-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.623Z","comments":true,"path":"posts/54338.html","link":"","permalink":"https://13592491893.github.io/posts/54338.html","excerpt":"","text":"Executor框架一、Executor、Executors、ExecutorService在Java 5之后，并发编程引入了一堆新的启动、调度和管理线程的API。Executor框架便是Java 5中引入的，其内部使用了线程池机制，它在java.util.cocurrent 包下，通过该框架来控制线程的启动、执行和关闭，可以简化并发编程的操作。因此，在Java 5之后，通过Executor来启动线程比使用Thread的start方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免this逃逸问题——如果我们在构造器中启动一个线程，因为另一个任务可能会在构造器结束之前开始执行，此时可能会访问到初始化了一半的对象用Executor在构造器中。Eexecutor作为灵活且强大的异步执行框架，其支持多种不同类型的任务执行策略，提供了一种标准的方法将任务的提交过程和执行过程解耦开发，基于生产者-消费者模式，其提交任务的线程相当于生产者，执行任务的线程相当于消费者，并用Runnable来表示任务，Executor的实现还提供了对生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。 1、Executor的UML图：（常用的几个接口和子类） Executor框架包括：线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。 2、Executor和ExecutorServiceExecutor：一个接口，其定义了一个接收Runnable对象的方法executor，其方法签名为executor(Runnable command),该方法接收一个Runable实例，它用来执行一个任务，任务即一个实现了Runnable接口的类，一般来说，Runnable任务开辟在新线程中的使用方法为：new Thread(new RunnableTask())).start()，但在Executor中，可以使用Executor而不用显示地创建线程：executor.execute(new RunnableTask()); // 异步执行 ExecutorService：是一个比Executor使用更广泛的子类接口，其提供了生命周期管理的方法，返回 Future 对象，以及可跟踪一个或多个异步任务执行状况返回Future的方法；可以调用ExecutorService的shutdown（）方法来平滑地关闭 ExecutorService，调用该方法后，将导致ExecutorService停止接受任何新的任务且等待已经提交的任务执行完成(已经提交的任务会分两类：一类是已经在执行的，另一类是还没有开始执行的)，当所有已经提交的任务执行完毕后将会关闭ExecutorService。因此我们一般用该接口来实现和管理多线程。 通过 ExecutorService.submit() 方法返回的 Future 对象，可以调用isDone（）方法查询Future是否已经完成。当任务完成时，它具有一个结果，你可以调用get()方法来获取该结果。你也可以不用isDone（）进行检查就直接调用get()获取结果，在这种情况下，get()将阻塞，直至结果准备就绪，还可以取消任务的执行。Future 提供了 cancel() 方法用来取消执行 pending 中的任务。ExecutorService 部分代码如下： public interface ExecutorService extends Executor { void shutdown(); &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; } 3、Executors类： 主要用于提供线程池相关的操作Executors类，提供了一系列工厂方法用于创建线程池，返回的线程池都实现了ExecutorService接口。 public static ExecutorService newFiexedThreadPool(int Threads) 创建固定数目线程的线程池。 public static ExecutorService newCachedThreadPool()：创建一个可缓存的线程池，调用execute 将重用以前构造的线程（如果线程可用）。如果没有可用的线程，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程。 public static ExecutorService newSingleThreadExecutor()：创建一个单线程化的Executor。 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize)：创建一个支持定时及周期性的任务执行的线程池，多数情况下可用来替代Timer类。 4、Executor VS ExecutorService VS Executors正如上面所说，这三者均是 Executor 框架中的一部分。Java 开发者很有必要学习和理解他们，以便更高效的使用 Java 提供的不同类型的线程池。总结一下这三者间的区别，以便大家更好的理解： Executor 和 ExecutorService 这两个接口主要的区别是：ExecutorService 接口继承了 Executor 接口，是 Executor 的子接口Executor 和 ExecutorService 第二个区别是：Executor 接口定义了 execute()方法用来接收一个Runnable接口的对象，而 ExecutorService 接口中的 submit()方法可以接受Runnable和Callable接口的对象。Executor 和 ExecutorService 接口第三个区别是 Executor 中的 execute() 方法不返回任何结果，而 ExecutorService 中的 submit()方法可以通过一个 Future 对象返回运算结果。Executor 和 ExecutorService 接口第四个区别是除了允许客户端提交一个任务，ExecutorService 还提供用来控制线程池的方法。比如：调用 shutDown() 方法终止线程池。可以通过 《Java Concurrency in Practice》 一书了解更多关于关闭线程池和如何处理 pending 的任务的知识。Executors 类提供工厂方法用来创建不同类型的线程池。比如: newSingleThreadExecutor() 创建一个只有一个线程的线程池，newFixedThreadPool(int numOfThreads)来创建固定线程数的线程池，newCachedThreadPool()可以根据需要创建新的线程，但如果已有线程是空闲的会重用已有线程。下面给出一个Executor执行Callable任务的示例代码： import java.util.ArrayList; import java.util.List; import java.util.concurrent.*; public class CallableDemo{ public static void main(String[] args){ ExecutorService executorService = Executors.newCachedThreadPool(); List&lt;Future&lt;String&gt;&gt; resultList = new ArrayList&lt;Future&lt;String&gt;&gt;(); //创建10个任务并执行 for (int i = 0; i &lt; 10; i++){ //使用ExecutorService执行Callable类型的任务，并将结果保存在future变量中 Future&lt;String&gt; future = executorService.submit(new TaskWithResult(i)); //将任务执行结果存储到List中 resultList.add(future); } //遍历任务的结果 for (Future&lt;String&gt; fs : resultList){ try{ while(!fs.isDone);//Future返回如果没有完成，则一直循环等待，直到Future返回完成 System.out.println(fs.get()); //打印各个线程（任务）执行的结果 }catch(InterruptedException e){ e.printStackTrace(); }catch(ExecutionException e){ e.printStackTrace(); }finally{ //启动一次顺序关闭，执行以前提交的任务，但不接受新任务 executorService.shutdown(); } } } } class TaskWithResult implements Callable&lt;String&gt;{ private int id; public TaskWithResult(int id){ this.id = id; } /** * 任务的具体过程，一旦任务传给ExecutorService的submit方法， * 则该方法自动在一个线程上执行 */ public String call() throws Exception { System.out.println(\"call()方法被自动调用！！！ \" + Thread.currentThread().getName()); //该返回结果将被Future的get方法得到 return \"call()方法被自动调用，任务返回的结果是：\" + id + \" \" + Thread.currentThread().getName(); } } 5、自定义线程池自定义线程池，可以用ThreadPool Executor类创建，它有多个构造方法来创建线程池，用该类很容易实现自定义的线程池，这里先贴上示例程序： import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolTest{ public static void main(String[] args){ //创建等待队列 BlockingQueue&lt;Runnable&gt; bqueue = new ArrayBlockingQueue&lt;Runnable&gt;(20); //创建线程池，池中保存的线程数为3，允许的最大线程数为5 ThreadPoolExecutor pool = new ThreadPoolExecutor(3,5,50,TimeUnit.MILLISECONDS,bqueue); //创建七个任务 Runnable t1 = new MyThread(); Runnable t2 = new MyThread(); Runnable t3 = new MyThread(); Runnable t4 = new MyThread(); Runnable t5 = new MyThread(); Runnable t6 = new MyThread(); Runnable t7 = new MyThread(); //每个任务会在一个线程上执行 pool.execute(t1); pool.execute(t2); pool.execute(t3); pool.execute(t4); pool.execute(t5); pool.execute(t6); pool.execute(t7); //关闭线程池 pool.shutdown(); } } class MyThread implements Runnable{ @Override public void run(){ System.out.println(Thread.currentThread().getName() + \"正在执行。。。\"); try{ Thread.sleep(100); }catch(InterruptedException e){ e.printStackTrace(); } } } 运行结果如下： 6、比较Executor和new Thread()new Thread的弊端如下： 每次new Thread新建对象性能差。 线程缺乏统一管理，可能无限制新建线程，相互之间竞争，及可能占用过多系统资源导致死机或oom。 缺乏更多功能，如定时执行、定期执行、线程中断。 相比new Thread，Java提供的四种线程池的好处在于： 重用存在的线程，减少对象创建、消亡的开销，性能佳。 可有效控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。 提供定时执行、定期执行、单线程、并发数控制等功能。 二、ExecutorService和ThreadPoolExecutor1、创建线程池有两种方式 通过ThreadPoolExecutor构造函数(推荐) import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i &lt; 10; i++) { //创建WorkerThread对象（WorkerThread类实现了Runnable 接口） Runnable worker = new MyRunnable(\"\" + i); //执行Runnable executor.execute(worker); } //终止线程池 executor.shutdown(); while (!executor.isTerminated()) { } System.out.println(\"Finished all threads\"); } } corePoolSize: 核心线程数为 5。maximumPoolSize ：最大线程数 10keepAliveTime : 等待时间为 1L。unit: 等待时间的单位为 TimeUnit.SECONDS。workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100;handler:饱和策略为 CallerRunsPolicy。 output pool-1-thread-3 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:47 CST 2020 通过Executor框架的Executors创建 Executors返回的是ExecutorService 2、ExecutorServiceJava.util.concurrent.ExecutorService接口代表一种异步执行机制，它能够在后台执行任务。因此ExecutorService与thread pool是非常相似的。事实上，在java.util.package包中ExecutorService的具体实现就是一个线程池的具体实现。下面是一个简单的例子： ExecutorService executorService = Executors.newFixedThreadPool(10); executorService.execute(new Runnable() { public void run() { System.out.println(\"Asynchronous task\"); } }); executorService.shutdown(); 首先，通过newFixedThreadPool()工厂方法创建一个ExecutorService的实例。这个方法创建了一个可以有10个线程执行任务的线程池。 第二，Runnable接口的匿名实现类作为参数被传递给execute()方法。Runable将会被ExecutorService中的一个线程来执行。 下面是委托任务给ExecutorService的一些不同的方式： execute(Runnable) submit(Runnable) submit(Callable) invokeAny(…) invokeAll(…) 下面来逐个看看这些方法。 execute(Runnable) execute(Runnable) 方法接受一个java.lang.Runable对象的实例，并异步执行之。下面是一个使用ExecutorService执行Runnable的例子： ExecutorService executorService = Executors.newSingleThreadExecutor(); executorService.execute(new Runnable() { public void run() { System.out.println(\"Asynchronous task\"); } }); executorService.shutdown(); 这种方式不能获得Runnable执行的结果，如果有这种需要，你将要使用Callable。 submit(Runnable) submit(Runnable) 方法也接收一个Runnable接口的具体实现，并返回一个Future对象。Future对象可以用来检测Runable是否执行完成。 Future future = executorService.submit(new Runnable() { public void run() { System.out.println(\"Asynchronous task\"); } }); future.get(); //returns null if the task has finished correctly. submit(Callable) submit(Callable)方法与submit(Runnable)方法相似，除了接收的参数有所不同。Callable实例非常类似于Runnable,不同的是call方法可以返回一个结果，Runnable.run()方法不能返回一个结果(因为是void类型)，就算线程执行完了，成功了future.get()也只是得到null 可以通过submit(Callable)方法返回的Future对象获取Callable的结果。下面是一个使用Callable的例子： Future future = executorService.submit(new Callable(){ public Object call() throws Exception { System.out.println(\"Asynchronous Callable\"); return \"Callable Result\"; } }); System.out.println(\"future.get() = \" + future.get()); 上面代码的输出结果是： Asynchronous Callable future.get() = Callable Result invokeAny(…) invokeAny()方法接收一个Callable对象或者Callable的子接口实例的集合作为参数，这个方法不会返回Future,但会返回集合中某一个Callable的结果。你不能确定你得到是哪个Callable的结果。只是已执行完成的Callable中的一个。 如果一个任务已经完成（或者抛出了异常），剩余的Callable任务将被取消。下面是示例代码： ExecutorService executorService = Executors.newSingleThreadExecutor(); Set&lt;Callable&lt;String&gt;&gt; callables = new HashSet&lt;Callable&lt;String&gt;&gt;(); callables.add(new Callable&lt;String&gt;() { public String call() throws Exception { return \"Task 1\"; } }); callables.add(new Callable&lt;String&gt;() { public String call() throws Exception { return \"Task 2\"; } }); callables.add(new Callable&lt;String&gt;() { public String call() throws Exception { return \"Task 3\"; } }); String result = executorService.invokeAny(callables); System.out.println(\"result = \" + result); executorService.shutdown(); 示例代码将会打印给定的Callable集合中一个Callable任务返回的结果。我尝试执行了多次，结果是变化的。有时候是“Task1”,有时候是“Task 2”等。 invokeAll(…) invokeAll()接收一个Callable对象的集合作为参数，该方法会调用你传给他的集合中的所有Callable对象。Invoke()会返回一个Future对象的列表，通过这个列表你可以获取每一个Callable执行的结果。 一个任务可能会因为一个异常而结束，因此这时任务并不是真正意义上执行成功了。这在Future上是没有办法来判断的。 处理一个任务的容器（collection），并返回一个Future的容器。两个容器具有相同的结构，这里提交的任务容器列表和返回的Future列表存在顺序对应的关系。 下面是示例代码： ExecutorService executorService = Executors.newSingleThreadExecutor(); Set&lt;Callable&lt;String&gt;&gt; callables = new HashSet&lt;Callable&lt;String&gt;&gt;(); callables.add(new Callable&lt;String&gt;() { public String call() throws Exception { return \"Task 1\"; } }); callables.add(new Callable&lt;String&gt;() { public String call() throws Exception { return \"Task 2\"; } }); callables.add(new Callable&lt;String&gt;() { public String call() throws Exception { return \"Task 3\"; } }); List&lt;Future&lt;String&gt;&gt; futures = executorService.invokeAll(callables); for(Future&lt;String&gt; future : futures){ System.out.println(\"future.get = \" + future.get()); } executorService.shutdown(); ExecutorService Shutdown 当你是使用完ExecutorService后，你应该关闭它，使得线程不能持续运行。例如，你的应用程序从main()方法开始并且你的主线程退出应用程序，这时如果存在激活状态的ExecutorService，你的应用程序将仍然会保持运行。ExecutorService中激活的线程会阻止JVM关闭。 为了终止ExecutorService中的线程，你需要调用shutdown()方法。ExecutorService不会立即关闭，但是它也不会接受新的任务，直到它里面的所有线程都执行完毕，ExecutorService才会关闭。所有提交到ExecutorService中的任务会在调用shutdown()方法之前被执行。 如果你想立即关闭ExecutorService,你可以调用shutdownNow()方法。这将会尝试立即停止所有正在执行的任务，并且忽略所有提交的但未被处理的任务。对于正在执行的任务是不能确定的，也许它们停止了，也行它们执行直到结束。 3、ThreadPoolExecutorJava.util.concurrent.ThreadPoolExecutor类是ExecutorSerivce接口的具体实现。ThreadPoolExecutor使用线程池中的一个线程来执行给定的任务（Runnable或者Runnable）。 ThreadPoolExecutor内部的线程池包含不定数量的线程。池中线程的数量由下面的这些变量决定： corePoolSize maximumPoolSize 当一个任务委托给线程池执行，此时如果池线程中线程数少于corePoolSize，即使池中有空闲的线程，线程池中也会创建一个新的线程。 如果任务队列是满的，corePoolSize个线程或者更多的且少于maximumPoolSize的线程正在运行，也会创建一个新的线程来执行任务。 下面图释ThreadPoolExecutor这种原理： 创建ThreadPoolExecutorThreadPoolExecutor有多种构造函数。例如： int corePoolSize = 5; int maxPoolSize = 10; long keepAliveTime = 5000; ExecutorService threadPoolExecutor = new ThreadPoolExecutor( corePoolSize, maxPoolSize, keepAliveTime, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;() ); 三、CompletionServicejava.util.concurrent.CompletionService 是对 ExecutorService 的一个功能增强封装，优化了获取异步操作结果的接口。 1、使用场景假设我们要向线程池提交一批任务，并获取任务结果。一般的方式是提交任务后，从线程池得到一批 Future 对象集合，然后依次调用其 get() 方法。 这里有个问题：因为我们会要按固定的顺序来遍历 Future 元素，而 get() 方法又是阻塞的，因此如果某个 Future 对象执行时间太长，会使得我们的遍历过程阻塞在该元素上，无法及时从后面早已完成的 Future 当中取得结果。 CompletionService 解决了这个问题。它本身不包含线程池，创建一个 CompletionService 需要先创建一个 Executor。下面是一个例子： ExecutorService executor = Executors.newFixedThreadPool(4); CompletionService&lt;String&gt; completionService = new ExecutorCompletionService&lt;&gt;(executor); 向 CompletionService 提交任务的方式与 ExecutorService 一样： completionService.submit(() -&gt; \"Hello\"); 当你需要获得结果的时候，就不同了。有了 CompletionService，你不需要再持有 Future 集合。如果要得到最早的执行结果，只需要像下面这样： String result = completionService.take().get(); 这个 take() 方法返回的是最早完成的任务的结果，这个就解决了一个任务被另一个任务阻塞的问题。下面是一个完整的例子： 2、示例（更好的可以看代码里的测试内容）public static void main(String[] args) throws Exception { ExecutorService executor; CompletionService&lt;String&gt; completionService; // 创建一个指定执行时长的任务的方法 BiFunction&lt;Integer, Integer, Callable&lt;String&gt;&gt; createTask = (id, duration) -&gt; () -&gt; { log(\"Task \" + id + \" started, duration=\" + duration); Thread.sleep(duration); log(\"Task \" + id + \" completed.\"); return \"Result of task \" + id; }; /////////////////////////////////////////////////////////////////// System.out.println(\"// 示例1：像使用 ExecutorService 一样使用 CompletionService\"); // 初始化 executor 和 completionService executor = Executors.newFixedThreadPool(4); completionService = new ExecutorCompletionService&lt;&gt;(executor); // 提交任务 List&lt;Future&lt;String&gt;&gt; results = Arrays.asList( completionService.submit(createTask.apply(1, 1000)), completionService.submit(createTask.apply(2, 800)), completionService.submit(createTask.apply(3, 600)), completionService.submit(createTask.apply(4, 400)) ); // 取结果 for (Future&lt;String&gt; result : results) { log(result.get()); } executor.shutdown(); /////////////////////////////////////////////////////////////////// System.out.println(\"// 示例2：按标准方式使用 CompletionService\"); // 初始化 executor 和 completionService executor = Executors.newFixedThreadPool(4); completionService = new ExecutorCompletionService&lt;&gt;(executor); // 提交任务 completionService.submit(createTask.apply(5, 1000)); completionService.submit(createTask.apply(6, 800)); completionService.submit(createTask.apply(7, 600)); completionService.submit(createTask.apply(8, 400)); // 取结果 for (int i = 0; i &lt; 4; i++) { log(completionService.take().get()); } /////////////////////////////////////////////////////////////////// executor.shutdown(); } 这个例子的执行结果如下所示： // 示例1：像使用 ExecutorService 一样使用 CompletionService 10:22:32:271 - Task 4 started, duration=400 10:22:32:271 - Task 3 started, duration=600 10:22:32:271 - Task 2 started, duration=800 10:22:32:271 - Task 1 started, duration=1000 10:22:32:687 - Task 4 completed. 10:22:32:888 - Task 3 completed. 10:22:33:089 - Task 2 completed. 10:22:33:303 - Task 1 completed. 10:22:33:303 - Result of task 1 10:22:33:303 - Result of task 2 10:22:33:303 - Result of task 3 10:22:33:303 - Result of task 4 // 示例2：按标准方式使用 CompletionService 10:22:33:305 - Task 5 started, duration=1000 10:22:33:305 - Task 7 started, duration=600 10:22:33:305 - Task 6 started, duration=800 10:22:33:305 - Task 8 started, duration=400 10:22:33:718 - Task 8 completed. 10:22:33:718 - Result of task 8 10:22:33:918 - Task 7 completed. 10:22:33:918 - Result of task 7 10:22:34:119 - Task 6 completed. 10:22:34:119 - Result of task 6 10:22:34:320 - Task 5 completed. 10:22:34:320 - Result of task 5 可以看出，在示例 1 中，虽然 Task 4 执行时间只有 400ms，但因为我们是按照 1-2-3-4 的顺序依次取结果，因此 Task 4 完成后并没有马上打印出结果来。而在示例 2 中，对每个 Task 都是在完成时立刻就将结果打印出来了。这就是 CompletionService 的优势所在。 3、原理解释CompletionService 之所以能够做到这点，是因为它没有采取依次遍历 Future 的方式，而是在中间加上了一个结果队列，任务完成后马上将结果放入队列，那么从队列中取到的就是最早完成的结果。 如果队列为空，那么 take() 方法会阻塞直到队列中出现结果为止。此外 CompletionService 还提供一个 poll() 方法，返回值与 take() 方法一样，不同之处在于它不会阻塞，如果队列为空则立刻返回 null。这算是给用户多一种选择。","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"Executor","slug":"Executor","permalink":"https://13592491893.github.io/tags/Executor/"},{"name":"多线程","slug":"多线程","permalink":"https://13592491893.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"线程","slug":"线程","permalink":"https://13592491893.github.io/tags/%E7%BA%BF%E7%A8%8B/"}]},{"title":"工作中遇到的代码问题及优化","slug":"工作中遇到的代码问题及优化","date":"2021-01-24T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/38067.html","link":"","permalink":"https://13592491893.github.io/posts/38067.html","excerpt":"","text":"工作中遇到的代码问题及优化一、获取当前时间Java 8通过发布新的Date-Time API (JSR 310)来进一步加强对日期与时间的处理。 在旧版的 Java 中，日期时间 API 存在诸多问题，其中有： 非线程安全 − java.util.Date 是非线程安全的，所有的日期类都是可变的，这是Java日期类最大的问题之一。 设计很差 − Java的日期/时间类的定义并不一致，在java.util和java.sql的包中都有日期类，此外用于格式化和解析的类在java.text包中定义。java.util.Date同时包含日期和时间，而java.sql.Date仅包含日期，将其纳入java.sql包并不合理。另外这两个类都有相同的名字，这本身就是一个非常糟糕的设计。 时区处理麻烦 − 日期类并不提供国际化，没有时区支持，因此Java引入了java.util.Calendar和java.util.TimeZone类，但他们同样存在上述所有的问题。 Java 8 在 java.time 包下提供了很多新的 API。以下为两个比较重要的 API： Local(本地) − 简化了日期时间的处理，没有时区的问题。 Zoned(时区) − 通过制定的时区处理日期时间。 新的java.time包涵盖了所有处理日期，时间，日期/时间，时区，时刻（instants），过程（during）与时钟（clock）的操作。 1.本地化日期时间 APILocalDate/LocalTime 和 LocalDateTime 类可以在处理时区不是必须的情况。代码如下： import java.time.LocalDate; import java.time.LocalTime; import java.time.LocalDateTime; import java.time.Month; public class Java8Tester { public static void main(String args[]){ Java8Tester java8tester = new Java8Tester(); java8tester.testLocalDateTime(); } public void testLocalDateTime(){ // 获取当前的日期时间 LocalDateTime currentTime = LocalDateTime.now(); System.out.println(\"当前时间: \" + currentTime); LocalDate date1 = currentTime.toLocalDate(); System.out.println(\"date1: \" + date1); Month month = currentTime.getMonth(); int day = currentTime.getDayOfMonth(); int seconds = currentTime.getSecond(); System.out.println(\"月: \" + month +\", 日: \" + day +\", 秒: \" + seconds); LocalDateTime date2 = currentTime.withDayOfMonth(10).withYear(2012); System.out.println(\"date2: \" + date2); // 12 december 2014 LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 12); System.out.println(\"date3: \" + date3); // 22 小时 15 分钟 LocalTime date4 = LocalTime.of(22, 15); System.out.println(\"date4: \" + date4); // 解析字符串 LocalTime date5 = LocalTime.parse(\"20:15:30\"); System.out.println(\"date5: \" + date5); } } //输出结果： /** $ javac Java8Tester.java $ java Java8Tester 当前时间: 2016-04-15T16:55:48.668 date1: 2016-04-15 月: APRIL, 日: 15, 秒: 48 date2: 2012-04-10T16:55:48.668 date3: 2014-12-12 date4: 22:15 date5: 20:15:30 */ 2.使用时区的日期时间API如果我们需要考虑到时区，就可以使用时区的日期时间API： import java.time.ZonedDateTime; import java.time.ZoneId; public class Java8Tester { public static void main(String args[]){ Java8Tester java8tester = new Java8Tester(); java8tester.testZonedDateTime(); } public void testZonedDateTime(){ // 获取当前时间日期 ZonedDateTime date1 = ZonedDateTime.parse(\"2015-12-03T10:15:30+05:30[Asia/Shanghai]\"); System.out.println(\"date1: \" + date1); ZoneId id = ZoneId.of(\"Europe/Paris\"); System.out.println(\"ZoneId: \" + id); ZoneId currentZone = ZoneId.systemDefault(); System.out.println(\"当期时区: \" + currentZone); } } //输出结果： /** $ javac Java8Tester.java $ java Java8Tester date1: 2015-12-03T10:15:30+08:00[Asia/Shanghai] ZoneId: Europe/Paris 当期时区: Asia/Shanghai */ 二、stream流​ 具体用法参考项目stream流文档 三、多线程/线程池/异步1、业务中要对上传的图片和视频压缩后再保存2、下载多张图片https://blog.csdn.net/qq_36898043/article/details/79733124?spm=1001.2014.3001.5506 四、根据配置的ip和端口号判断连接是否能通​ 通过try{创建链接连接目标端口}捕获异常来判断是否能通 五、包装类型比较是否相等的问题String a = \"6.00\"; String b = \"6.0\"; System.out.println(Double.valueOf(a)); //6.0 System.out.println(Double.valueOf(b)); //6.0 System.out.println(Double.valueOf(a)==(Double.valueOf(b))); //false,==比较的是两个对象的内存地址 System.out.println(Double.valueOf(a).equals(Double.valueOf(b))); //true，equals比较的是两个值是否相等 扩展：浮点类型比较大小(关于float的Java面试题) public void test1() { float a=1.0f-0.9f; float b=0.9f-0.8f; if (a == b){ System.out.println(\"true\"); }else { System.out.println(\"false\"); } } 这道题我乍一看，觉得没什么难度，结果肯定是true，但是将代码粘到idea时，发现出了一条警告，说if里的内容结果永远为false，这就很纳闷了。于是我将a,b打印出来，结果如下： false 0.100000024 0.099999964 看到这里我心里已经明白了，这是发生了精度丢失问题。于是我搜索了一波，发现了原因。原来在计算机中，浮点数都是以二进制数的形式存储的，这样就会产生一个问题，有许多的小数无法被准确的表示。如同十进制无法准确表示 1/3 那样，二进制也有许多无法精确表示的数，计算机只能进行舍入，取近似值来表示。 那既然float无法有精度问题，那么它的包装类Float有没有这样的问题呢： public void test2(){ Float a=1.0f-0.9f; Float b=0.9f-0.8f; System.out.println(a); System.out.println(b); } 运行结果： 0.100000024 0.099999964 结果与test1相同，可见Float同样没能解决精度问题，从Float的源码可以看出，它的equals()方法最终仍是使用“==”做比较，可见并没有进行精度的处理。 float是这样，那同为浮点数的double也不例外： public void testn3(){ Double a=1.0-0.9; System.out.println(a); } 运行结果： 0.09999999999999998 可以看到double同样存在精度问题，但是double是双精度，float是单精度，它们的有效数位不同，因此结果也不同。 那么问题就来了，假如以后需要做一些金融方面的项目，比如算汇率等等，这时候该怎么办？为了防止这种情况，Java就提供了BigDecimal()这个类，来解决精度问题： public void test4(){ BigDecimal decimal = new BigDecimal(\"0.9\"); BigDecimal decimal1 = new BigDecimal(\"0.8\"); System.out.println(decimal); System.out.println(decimal1); System.out.println(decimal.subtract(decimal1).doubleValue()); } 运行结果： 0.9 0.8 0.1 这个类需要传入的参数是String类型的，需要传入的值是多少，就以字符串的形式填进去，可以看到并没有发生精度问题。通过调用它的subtract()方法做个减法，得出的结果是0.1，也没有出现精度问题。 但是如果直接向BigDecimal中传入一个double类型的值，结果会依然有精度问题： public void test5(){ double a=1.0-0.9; BigDecimal decimal = new BigDecimal(a); System.out.println(decimal); } 运行结果: 0.09999999999999997779553950749686919152736663818359375 六、mybatis中对日期大小比较的写法如果数据库是时间类型而且你传的值也是时间类型，可以直接比较。 大于号要用&amp;gt;小于号要用&amp;lt; &lt;![CDATA[ and t.activity_begintime &gt;= #{beginTime} ]]&gt; 1.数据库时间为TimeStamp 传入的参数为 java.util.Date AND d_create_date &gt;= #{startDate}(需要转义) 2.数据库时间为TimeStamp 传入的参数为 String AND d_create_date &gt;= to_date(#{startDate},'yyyy-MM-dd HH:mm:ss') 你试试吧。 ## 七、查询树状结构 生产计划查询树状结构 ![image-20210412103720971](https://gitee.com/gsshy/picgo/raw/master/img/image-20210412103720971.png) 后端方法： 实体类Entity加上children ```java private ArrayList&lt;PlanEntity&gt; children; 实现层 public Message searchPlanTree(PlanEntity entity) throws BusinessException { String enterpriseId = SessionUtils.getEnterpriseId(); entity.setEnterpriseId(enterpriseId); ArrayList&lt;PlanEntity&gt; planList = planDao.searchPlan(entity); List&lt;String&gt; planNoList; //所有结果的planNo集合 planNoList = planList.stream().map(PlanEntity::getPlanNo).collect(Collectors.toList()); //1.找到所有的一级菜单 ArrayList&lt;PlanEntity&gt; oneLevelList = new ArrayList&lt;&gt;(); for (PlanEntity planEntity : planList) { //判断是否是月计划,如果是,则加进一级计划里,如果不是月计划,则判断此计划的父计划是否在查询结果里,如果不在,此计划则加进一级计划里 if(\"MonthPlan\".equals(planEntity.getPlanTypeCode())){ oneLevelList.add(planEntity); }else{ if(!planNoList.contains(planEntity.getParentPlanNo())){ oneLevelList.add(planEntity); } } } //为一级菜单设置子菜单 for (PlanEntity child : oneLevelList) { child.setChildren(getChild(child.getPlanNo(), planList)); } return Message.success(oneLevelList, \"查询成功\"); } private ArrayList&lt;PlanEntity&gt; getChild(String planNo, ArrayList&lt;PlanEntity&gt; planList) { ArrayList&lt;PlanEntity&gt; childrenList = new ArrayList&lt;PlanEntity&gt;(); for (PlanEntity entity : planList) { if (planNo.equals(entity.getParentPlanNo())) { childrenList.add(entity); } } //递归退出条件 if (childrenList.size() == 0) { return null; } // 把子计划再循环一遍 for (PlanEntity entity : childrenList) { entity.setChildren(getChild(entity.getPlanNo(), planList)); } return childrenList; } 八、延迟队列(DelayQueue)​ 需求：点开始混合的时候，保存开始混合时间，和结束混合时间，然后系统时间等于结束混合时间的时候，把混合状态改成Y ​ 做法：这个用延迟队列试试看行不，DelayQueue，保存开始混合时间和结束混合时间的时候计算出结束混合时间与当前时间的差值作为延迟时间，保存的同时加到延迟队列里，延迟时间到了也就是到结束混合时间了，再改混合状态 参考：1.https://www.jb51.net/article/111888.htm 2.尚硅谷gulimall有一集讲延迟队列 九、controller中service注入为null调用controller中的方法时发现自动注入的service为null，排查后发现方法修饰符写成了private，改成public就可以了 原因：容器扫描bean生成代理类的时候，public和protected方法可以被正常代理，而private方法的不会被代理，属性的注入也是在代理类中完成，所以public/protected方法获取的注入属性是完成注入的属性，private方法获取的是未完成注入时的属性，所以是null。 详解可以看：https://blog.csdn.net/liruichuan/article/details/101367819","categories":[{"name":"work","slug":"work","permalink":"https://13592491893.github.io/categories/work/"}],"tags":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/tags/sql/"},{"name":"web","slug":"web","permalink":"https://13592491893.github.io/tags/web/"}]},{"title":"CAP理论","slug":"CAP理论","date":"2021-01-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.623Z","comments":true,"path":"posts/44851.html","link":"","permalink":"https://13592491893.github.io/posts/44851.html","excerpt":"","text":"CAP理论引言​ 关于 CAP 为何你读了那么多文章都还是搞不明白呢？因为 CAP 理论来自学术界，而解读 CAP 理论的人尝试用工程师的方式去阐述它，这本身就有了问题。 CAP 本身基于状态，基于瞬态，是一个描述性的理论，它并不解决工程问题。但是，很多工程师却总是尝试为 CAP 做过多解读。比如，非要说 CAP 理论只能适合某某场景，非要说 CAP 理论里的一致性是非常强的一致性，把其和事务的一致性混为一谈。 由于 CAP 是学术理论，并不是工程理论，它会舍弃很多现实世界的现实问题。比如网络的时长，比如节点内部的处理速度不一致，比如节点间存储方式和速度的不一致。它说的一致性就是客户端是否能拿到最新数据，它说的可用性就是允许客户端拿不到最新数据。而这些东西被工程师们的过多脑补，导致了文章和文章说法不一样，解析不一样，阐述背景不一样。 在今天这篇文章中，我们只解释和说明，不脑补，不过多从工程角度解读，只说本质，只指核心，希望能真正说清楚、讲明白 CAP 理论。望本文能达到这个目的。 作者：四猿外链接：https://www.zhihu.com/question/54105974/answer/1643846752来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 1. CAP 的由来要理解 CAP，首先我们要清楚，为何会有人提出 CAP？他提出 CAP 是为了解决什么问题？ 时间回到 1985 年，彼时，后来证明了 CAP 理论的 Lynch 教授此时给当时的 IT 界来了一记惊雷： 她通过不可辩驳的证明告诉业界的工程师们，如果在一个不稳定（消息要么乱序要么丢了）的网络环境里（分布式异步模型），想始终保持数据一致是不可能的。 这是个什么概念呢？就是她打破了那些既想提供超高质量服务，又想提供超高性能服务的技术人员的幻想。 这本质是在告诉大家，在分布式系统里，需要妥协。 但是，如何妥协？分布式系统里到底应该怎么权衡这种 trade-off？ 我们可以想象一下，在 CAP 定理提出之前，没有这些方向性的指引，在设计和实施分布式系统时该有多么混乱。一套分布式系统是由多个模块组成的，这些模块本身可能由不同的开发人员去完成。然而，对于这些人，在公共层面，竟然没有一个原则去指导他们该怎么完成这套功能。 比如，我们在同步两个节点的数据时，如果发生了错误，到底我们应该怎么做呢？如果没有统一的标准和方向，那很可能在一套分布式系统中的不同模块，会出现不同的处理情况。 假设一套系统，由 A、B 两个模块构成。 A 模块的设计理念是：节点间出现了问题，它可能会选择不断的重试，一直等到节点通信恢复。 而 B 的设计理念是：节点间出现了问题，它断开就是了，可能最多就记录下状态，等以后处理。 可是，当 A、B 之间出现了通信怎么办？那会出现 A 往 B 发请求，出问题会不断重试。而 B 往 A 发请求，出问题则直接断开的情况。 当然，在后面我们会说明，CAP 的理念在实际工程中，会允许这种不一致。可是，那种不一致是提前设计好和规划好的，是根据实际数据的重要性和业务需求做的妥协，而不是这种混乱的妥协。 所以，IT 界的人们就一直在摸索，试图找到一些纲领去指导分布式系统的设计，这一找就找了 15 年。 2000 年时，Eric Brewer 教授在 PODC 会议上提出了 CAP 理论，但是由于没有被证明过，所以，当时只能被称为 CAP 猜想。这个猜想引起了巨大的反响，因为 CAP 很符合人们对设计纲领的预期。 在 2002 年后，经过 Seth Gilbert 和 Nancy Lynch 从理论上证明了 CAP 猜想后，CAP 理论正式成为了分布式系统理论的基石之一。 2. CAP 到底是什么CAP 定理表达了一个分布式系统里不可能同时满足以下的三个特性： 2.1. C：数据一致性什么是数据一致性？咋一看真的很让人糊涂，一致性是什么？是指数据能一起变化，是能让数据整齐划一。 那么问题又来了，数据何时会变化？数据怎么才能被称为一起变化？我们现在来回答这些问题，当我们搞清楚了这些问题，那么对数据一致性就会有了清晰的理解。 首先第一个问题，数据何时会一起变化？ 答案是：仅且仅当包含数据的服务，收到数据更新请求的时候，数据才会发生变化。而数据更新请求则仅包括数据的增、删、改这三种请求，而这三种请求又被统称为写请求。所以，数据只有在写请求的时候才会发生变化。 那我们来回答第二个问题，数据要怎么样才能被称为一起变化了？即谁来判断数据是最终变化了？是服务器对写请求的返回结果吗？告诉写请求成功，数据就一定发生一致性变化了？ NO，数据发生变化是否一致是需要经过读请求来做检验的。那么读请求判断的依据是什么呢？ 假设，我们的分布式存储系统有两个节点，每个节点都包含了一部分需要被变化的数据。如果经过一次写请求后，两个节点都发生了数据变化。然后，读请求把这些变化后的数据都读取到了，我们就把这次数据修改称为数据发生了一致性变化。 但是，这还不是完整的一致性。因为系统不可能永久的正常运行下去。 如果系统内部发生了问题从而导致系统的节点无法发生一致性变化会怎么样呢？当我们这样做的时候，就意味着想看到最新数据的读请求们，很可能会看到旧数据，或者说获取到不同版本的数据。此时，为了保证分布式系统对外的数据一致性，于是选择不返回任何数据。 这里需要注意一下，CAP 定理是在说在某种状态下的选择，和实际工程的理论是有差别的。上面描述的一致性和 ACID 事务中的一致性是两回事。事务中的一致性包含了实际工程对状态的后续处理。但是 CAP 定理并不涉及到状态的后续处理，对于这些问题，后续出现了 BASE 理论等工程结论去处理，目前，只需要明白 CAP 定理主要描述的是状态。 2.2. A：可用性奥维德曾经说过：“行动被人们遗忘，结果却将永存”。 这句话说明了结果的重要性，而可用性在 CAP 里就是对结果的要求。它要求系统内的节点们接收到了无论是写请求还是读请求，都要能处理并给回响应结果。只是它有两点必须满足的条件： 条件 1：返回结果必须在合理的时间以内，这个合理的时间是根据业务来定的。业务说必须 100 毫秒内返回，合理的时间就是 100 毫秒，需要 1 秒内返回，那就是 1 秒，如果业务定的 100 毫秒，结果却在 1 秒才返回，那么这个系统就不满足可用性。 条件 2：需要系统内能正常接收请求的所有节点都返回结果。这包含了两重含义： 如果节点不能正常接收请求了，比如宕机了，系统崩溃了，而其他节点依然能正常接收请求，那么，我们说系统依然是可用的，也就是说，部分宕机没事儿，不影响可用性指标。 如果节点能正常接收请求，但是发现节点内部数据有问题，那么也必须返回结果，哪怕返回的结果是有问题的。比如，系统有两个节点，其中有一个节点数据是三天前的，另一个节点是两分钟前的，如果，一个读请求跑到了包含了三天前数据的那个节点上，抱歉，这个节点不能拒绝，必须返回这个三天前的数据，即使它可能不太合理。 2.3. P：分区容忍性分布式的存储系统会有很多的节点，这些节点都是通过网络进行通信。而网络是不可靠的，当节点和节点之间的通信出现了问题，此时，就称当前的分布式存储系统出现了分区。但是，值得一提的是，分区并不一定是由网络故障引起的，也可能是因为机器故障。 比如，我们的分布式存储系统有 A、B 两个节点。那么，当 A、B 之间由于可能路由器、交换机等底层网络设备出现了故障，A 和 B 通信出现了问题，但是 A、B 依然都在运行，都在对外提供服务。这时候，就说 A 和 B 发生了分区。 还有一种情况也会发生分区，当 A 出现了宕机，A 和 B 节点之间通信也是出现了问题，那么我们也称 A 和 B 发生了分区。 综上，我们可以知道，只要在分布式系统中，节点通信出现了问题，那么就出现了分区。 那么，分区容忍性是指什么？ 它是说，如果出现了分区问题，我们的分布式存储系统还需要继续运行。不能因为出现了分区问题，整个分布式节点全部就熄火了，罢工了，不做事情了。 3. CAP 怎么选择我们上面已经知道了，在设计分布式系统时，架构师们在 C、A、P 这三种特性里，只能选择两种。 但是，这道 CAP 的选择题，就像别人在问你“小明的父亲有三个孩子，老大叫大朗，老二叫二郎，请问老三叫什么”一样。在以分布式存系统为限定条件的 CAP 世界里，P 是早已经确定的答案，P 是必须的。 因为，在分布式系统内，P 是必然的发生的，不选 P，一旦发生分区错误，整个分布式系统就完全无法使用了，这是不符合实际需要的。所以，对于分布式系统，我们只能能考虑当发生分区错误时，如何选择一致性和可用性。 而根据一致性和可用性的选择不同，开源的分布式系统往往又被分为 CP 系统和 AP 系统。 当一套系统在发生分区故障后，客户端的任何请求都被卡死或者超时，但是，系统的每个节点总是会返回一致的数据，则这套系统就是 CP 系统，经典的比如 Zookeeper。 如果一套系统发生分区故障后，客户端依然可以访问系统，但是获取的数据有的是新的数据，有的还是老数据，那么这套系统就是 AP 系统，经典的比如 Eureka。 说了这么多，其实 CAP 定理本质很简单，它就是一种分布式系统设计的不同理念概括，包括它说的一致性，可用性和分区容错性。这就类似一个大学的校训，是极度概念化的东西。 所以，大白话来形容下 CAP 吧，CAP 就是告诉程序员们当分布式系统出现内部问题了，你要做两种选择： 要么迁就外部服务，像外包公司。 要么让外部服务迁就你，像银行。 迁就外部服务就是我们不能因为我们自己的问题让外部服务的业务运行受到影响，所以要优先可用性。而让外部服务迁就我们，就要优先一致性。 4. 对 CAP 的常见误解误解一：分布式系统因为 CAP 定理放弃了 C 或者 A 中的其中一个很多人在没有对 CAP 做深入了解的情况下，听到很多人说分布式系统必须在 CAP 三个特性里选择两个，就觉得一套分布式系统肯定要么只有可用性要么只有一致性，不存在完整的可用性和一致性功能。 这种理解是大有问题的。因为，P 这种问题发生的概率非常低，所以： 当没有出现分区问题的时候，系统就应该有完美的数据一致性和可用性。 你什么时候见过一个系统，当内部没有问题的时候，会经常让外部请求卡一下的？要么就冷不丁的提供陈旧的老数据？那还能叫系统吗？ 误解二：C 和 A 之间的选择是针对整个分布式系统的，只能整体考虑 C 和 A 之间的选择这个理解也是不对的。当分区发生的时候，其实对一致性和可用性的抉择是局部性的，而不是针对整个系统的。 可能是在一些子系统做一些抉择，甚至很可能只需要对某个事件或者数据，做一致性和可用性的抉择而已。 比如，当我们做一套支付系统的时候，会员的财务相关像账户余额，账务流水是必须强一致性的。这时候，你就要考虑选 C。但是，会员的名字，会员的支付设置就不必考虑强一致性，可以选择可用性 A。 一套分布式系统的运行，就像人生一样，就是一次又一次的选择。在不同阶段，不同的时刻有不同的事件发生的时候，又怎么可能会有完全一样的选择呢？ 误解三：CAP 的三个特性只有是和否两种极端选择，而不是一个范围这种二元性的理解更是极其误导人。 CAP 理论的三种特性不是 Boolean 类型的，不是一致和不一致，可用和不可用，分区和没分区的这类二选一的选项。而是这三种特性都是范围类型。 拿可用性来说，就像我从银行取钱。当我目的是派发压岁钱的时候，我很可能就想全要新票子，但是，新票子很可能就还得多一个步骤，就是需要拿旧票子去换一些新票，此时，我可以多等会儿，能拿到新票子就好。而当我的目的就是做生活花销的时候，票子是新是旧，我根本不那么关心，快点拿到钱就行。这就是可用性的范围需求之一，对时延性的要求。 再比如，分区容错则由于探测机制的问题，可能还得各节点搞投票去协商分区是否存在，当某一台机器出现了问题，可能不影响业务的话，就会被机器投票认为分区不存在。然后一直等到多数机器出现了问题，才会投票确认出现了分区问题。这就好像新冠疫情，还会分低、中、高风险区呢，不是一出现通信故障就都被逻辑认定为分区问题。 5. CAP 理论的一些疑问疑问一：在遵从 CAP 定理的系统中是否适合任意的写请求首先，在 CAP 定理中，关于一致性会有多种说法，但是总的来说，都是在描述数据最新版本的可见性。而这些可见性往往代表的是读请求返回的数据的可见性。 那么问题来了，当我们要求读数据的可见性的时候，对写数据有什么要求吗？ 比如，我们系统有三个节点，一个客户端给这个系统发了一个写请求，要求系统写入一个值为 20 的数据。那么，如果要满足 CAP 定理中的一致性，就需要在写完 20 这个数据之后，当其他客户端请求读取这个值为 20 的数据之后，无论请求被转发到系统中任何节点都能返回这个值。 这就要求写入这个值为 20 的写请求必须成功写到三个节点上，此时，系统就满足了写一致性的。所以，我们可以说对于读一致性的要求是同时约束了写一致性的。 其次，在 CAP 定理中，可用性本身要求对读、写请求都要处理。如果我们以可用性作为标准的时候，在发生分区错误时，由于我们对读请求并没有强行要求返回完全准确的数据，所以，可能在本次读请求之前的最近一次写请求可能是部分失败的。 同样的例子，我们的分布式系统由三个节点组成，最近一次写请求想把值为 20 的数据写到三个节点上。但是，由于发生了分区问题，有一个节点通信故障，写请求写不过去，因此只有两个节点包含了值为 20 的数据。 此时，写请求会返回给客户端一个结果，可能会告诉客户端写入成功了，也可能告诉客户端写入部分成功。 这时候，当后续的读请求恰巧被发送到有通信故障的那个节点，系统可能只能返回一个空的结果。但是，由于系统处理和返回了读写请求，所以，系统是满足了 CAP 中的可用性的。 疑问二：数据分片和数据副本的分布式系统是否都遵守 CAP 定理我们知道，在一套大规模的分布式系统里，一定是既需要把海量数据做切分，存储到不同的机器上，也需要对这些存储了数据的机器做副本备份的。 那么，如果，一个分布式系统里只有数据分片存储或者只有数据副本存储，他们都会遵守 CAP 定理吗？ 答案是当数据分片时，也是要遵守 CAP 定理，但是，是种非常特殊的遵守。 当在一套分布式系统只有分片存储的时候，CAP 理论会表现成什么样？ 比如，我们有个分布式系统，由三个节点 a、b、c 组成。其中节点 a 存放了 A 表的数据，b 存放了 B 表的数据，c 存放了 C 表的数据。 如果有一个业务，它的意图是想往 A 表插入一条新数据，在 B 表删除一条已有数据，在 C 表更新一条老数据，这个分布式系统该怎么处理这种业务？ 技术上我们对这种一个意图想做多件事的情况往往会包装成一个事务。当我们包装成一个事务以后，我们可能会通过先在 a 节点执行，然后去 b 节点执行，最后去 c 节点执行，等到都成功了，才会返回成功。 但是，发生了分区以后怎么办？当在 a、b 节点都成功了，到 c 发现发生了通信故障？ 此时，根据 CAP 定理，你有两个选择，要么就直接返回一个部分成功的结果给客户端，要么直接卡死等客户端超时或者返回失败给客户端。当返回部分成功的时候，这就是选择了可用性（A），当卡死或者返回失败给客户端的时候，就是选择了一致性（C）。 可是，我们将请求包装成了事务，而事务是要求要么都成功，要么都失败……为了遵守这种要求，对于分布式只有分片的情况，迫于客观条件，只能选择C。所以分片的分布式系统，往往都是 CP 的系统。 可选择，但是无法选择是分布式系统只有分片数据存储的情况时，遵守 CAP 定理的特殊表现。 而当分布式系统是多个节点，每个节点存储了完整的一套数据，别的节点只是完整数据的备份的时候，即使事务只在一台机器上成功，当发生分区故障的时候，我们也是可以有充分的余地选择是单机事务的回退 or 就此认为写成功的。 单机事务的回退，就可以对外表现为选择了一致性。 就此认为写成功，则可以认为选择了可用性。 疑问三：为何有时候区分一个系统是 AP 还是 CP 是如此之难因为，就像我们前面讲过的，由于 AP 或者 CP 的选择，可能仅局限为整套系统的局部，甚至某些特殊的数据上，而我们又是用这种局部的特性去描述了整套系统，所以就导致了区分的困难。而这本身其实也日渐成为了 CAP 的一个大问题，从而被人诟病。 6. CAP 的不足 CAP 定理本身是没有考虑网络延迟的问题的，它认为一致性是立即生效的，但是，要保持一致性，是需要时间成本的，这就导致往往分布式系统多选择 AP 方式 由于时代的演变，CAP 定理在针对所有分布式系统的时候，出现了一些力不从心的情况，导致很多时候它自己会把以前很严谨的数学定义改成了比较松弛的业务定义，类似于我们看到，CAP 定理把一致性、可用性、分区容错都变成了一个范围属性，而这和 CAP 定理本身这种数学定理般的称呼是有冲突的，出现了不符合数学严谨定义的问题。 在实践中以及后来 CAP 定理的提出者也承认，一致性和可用性并不仅仅是二选一的问题，只是一些重要性的区别，当强调一致性的时候，并不表示可用性是完全不可用的状态。比如，Zookeeper 只是在 master 出现问题的时候，才可能出现几十秒的不可用状态，而别的时候，都会以各种方式保证系统的可用性。而强调可用性的时候，也往往会采用一些技术手段，去保证数据最终是一致的。CAP 定理并没有给出这些情况的具体描述。 CAP 理论从工程角度来看只是一种状态的描述，它告诉大家当有错的时候，分布式系统可能处在什么状态。但是，状态是可能变化的。状态间如何转换，如何修补，如何恢复是没有提供方向的。 7. 引申出来的 BASE正因为 CAP 以上的种种不足，epay 的架构师 Dan Pritchett 根据他自身在大规模分布式系统的实践经验，总结出了 BASE 理论。BASE 理论是对 CAP 理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。 BASE 理论是实践工程的理论，它弥补了CAP 理论过于抽象的问题，也同时解决了 AP 系统的总体工程实践思想，是分布式系统的核心理论之一，我们将在下一篇文章里，详细的讲解此套理论。 8. 大厂面试题在文章最后，来几道大厂关于 CAP 的面试真题，检验一下你的学习效果，hiahiahia 什么是 CAP 理论？ CAP 中的 P 是什么意思？ 为什么说分布式系统，只能在 C、A 中二选一？ 结合实际应用，CP、AP 该怎么选择？","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"cap","slug":"cap","permalink":"https://13592491893.github.io/tags/cap/"},{"name":"ap,cp","slug":"ap-cp","permalink":"https://13592491893.github.io/tags/ap-cp/"}]},{"title":"stream","slug":"stream","date":"2020-12-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/f0e9be1c.html","link":"","permalink":"https://13592491893.github.io/posts/f0e9be1c.html","excerpt":"","text":"streamJava8中提供了Stream对集合操作作出了极大的简化，学习了Stream之后，我们以后不用使用for循环就能对集合作出很好的操作。 一、流的初始化与转换 Java中的Stream的所有操作都是针对流的，所以，使用Stream必须要得到Stream对象： 1、初始化一个流： Stream stream = Stream.of(“a”, “b”, “c”); 2、数组转换为一个流： String [] strArray = new String[] {“a”, “b”, “c”}; stream = Stream.of(strArray); 或者 stream = Arrays.stream(strArray); 3、集合对象转换为一个流（Collections）： List list = Arrays.asList(strArray); stream = list.stream(); 二、流的操作流的操作可以归结为几种： 1.遍历操作(map)使用map操作可以遍历集合中的每个对象，并对其进行操作，map之后，用.collect(Collectors.toList())会得到操作后的集合。 map(T -&gt; R) 遍历转换为大写： List&lt;String&gt; output = wordList.stream(). map(String::toUpperCase). collect(Collectors.toList()); 平方数： List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4); List&lt;Integer&gt; squareNums = nums.stream(). map(n -&gt; n * n). collect(Collectors.toList()); 将流中的每一个元素 T 映射为 R（类似类型转换） List&lt;String&gt; newList = list.stream() .map(Person::getName) .collect(toList()); 2.过滤操作(filter)使用filter可以对象Stream中进行过滤，通过测试的元素将会留下来生成一个新的Stream。 filter(T-&gt;boolean):保留 boolean 为 true 的元素 得到其中不为空的String: List&lt;String&gt; filterLists = new ArrayList&lt;&gt;(); filterLists.add(\"\"); filterLists.add(\"a\"); filterLists.add(\"b\"); List afterFilterLists = filterLists.stream().filter(s -&gt; !s.isEmpty()) .collect(Collectors.toList()); 保留 boolean 为 true 的元素 保留年龄为 20 的 person 元素 list = list.stream() .filter(person -&gt; person.getAge() == 20) .collect(toList()); 打印输出 [Person{name='jack', age=20}] 3.循环操作如果只是想对流中的每个对象进行一些自定义的操作，可以使用forEach： List&lt;String&gt; forEachLists = new ArrayList&lt;&gt;(); forEachLists.add(\"a\"); forEachLists.add(\"b\"); forEachLists.add(\"c\"); forEachLists.stream().forEach(s-&gt; System.out.println(s)); 4.返回特定的结果结合limit 返回 Stream 的前面 n 个元素；skip 则是扔掉前 n 个元素: List&lt;String&gt; forEachLists = new ArrayList&lt;&gt;(); forEachLists.add(\"a\"); forEachLists.add(\"b\"); forEachLists.add(\"c\"); forEachLists.add(\"d\"); forEachLists.add(\"e\"); forEachLists.add(\"f\"); List&lt;String&gt; limitLists = forEachLists.stream().skip(2).limit(3).collect(Collectors.toList()); 注意skip与limit是有顺序关系的，比如使用skip(2)会跳过集合的前两个，返回的为c、d、e、f,然后调用limit(3)会返回前3个，所以最后返回的c,d,e 5.排序(sort/min/max/distinct)sort可以对集合中的所有元素进行排序。max，min可以寻找出流中最大或者最小的元素，而distinct可以寻找出不重复的元素 如果流中的元素的类实现了 Comparable 接口，即有自己的排序规则，那么可以直接调用 sorted() 方法对元素进行排序，如 Stream。反之, 需要调用 sorted((T, T) -&gt; int) 实现 Comparator 接口 sorted() / sorted((T, T) -&gt; int) 对一个集合进行排序： List&lt;Integer&gt; sortLists = new ArrayList&lt;&gt;(); sortLists.add(1); sortLists.add(4); sortLists.add(6); sortLists.add(3); sortLists.add(2); List&lt;Integer&gt; afterSortLists = sortLists.stream().sorted((In1,In2)-&gt; In1-In2).collect(Collectors.toList()); 得到其中长度最大的元素： List&lt;String&gt; maxLists = new ArrayList&lt;&gt;(); maxLists.add(\"a\"); maxLists.add(\"b\"); maxLists.add(\"c\"); maxLists.add(\"d\"); maxLists.add(\"e\"); maxLists.add(\"f\"); maxLists.add(\"hahaha\"); int maxLength = maxLists.stream().mapToInt(s-&gt;s.length()).max().getAsInt(); System.out.println(\"字符串长度最长的长度为\"+maxLength); 对一个集合进行查重： List&lt;String&gt; distinctList = new ArrayList&lt;&gt;(); distinctList.add(\"a\"); distinctList.add(\"a\"); distinctList.add(\"c\"); distinctList.add(\"d\"); List&lt;String&gt; afterDistinctList = distinctList.stream().distinct().collect(Collectors.toList()); 4.根据年龄大小来比较： 根据年龄大小来比较： list = list.stream() .sorted((p1, p2) -&gt; p1.getAge() - p2.getAge()) .collect(toList()); 当然这个可以简化为，推荐使用写法简单明了: list = list.stream() .sorted(Comparator.comparingInt(Person::getAge)) .collect(toList()); 其中的distinct()方法能找出stream中元素equal()，即相同的元素，并将相同的去除，上述返回即为a,c,d。 6.匹配(Match方法) 有的时候，我们只需要判断集合中是否全部满足条件，或者判断集合中是否有满足条件的元素，这时候就可以使用 match方法： allMatch：Stream 中全部元素符合传入的 predicate，返回 true anyMatch：Stream 中只要有一个元素符合传入的 predicate，返回 true noneMatch：Stream 中没有一个元素符合传入的 predicate，返回 true 判断集合中没有有为’c’的元素： List&lt;String&gt; matchList = new ArrayList&lt;&gt;(); matchList.add(\"a\"); matchList.add(\"a\"); matchList.add(\"c\"); matchList.add(\"d\"); boolean isExits = matchList.stream().anyMatch(s -&gt; s.equals(\"c\")); 判断集合中是否全不为空： List&lt;String&gt; matchList = new ArrayList&lt;&gt;(); matchList.add(\"a\"); matchList.add(\"\"); matchList.add(\"a\"); matchList.add(\"c\"); matchList.add(\"d\"); boolean isNotEmpty = matchList.stream().noneMatch(s -&gt; s.isEmpty()); 则返回的为false 7.列表属性求和produceNum = jsonList.stream().map(IngredientsDetailEntity::getCheckQty).reduce(BigDecimal.ZERO, BigDecimal::add);","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"},{"name":"stream","slug":"java/stream","permalink":"https://13592491893.github.io/categories/java/stream/"}],"tags":[{"name":"命令","slug":"命令","permalink":"https://13592491893.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"stream","slug":"stream","permalink":"https://13592491893.github.io/tags/stream/"}]},{"title":"集合","slug":"集合","date":"2020-11-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/d366874c.html","link":"","permalink":"https://13592491893.github.io/posts/d366874c.html","excerpt":"","text":"集合1、三种集合 1.1 List 1、List有序队列，并且可重复 2、ArrayList：底层是数组，可以认为ArrayList是一个可改变大小的数组。 随着越来越多的元素被添加到ArrayList中，其规模是动态增加的。 3、LinkedList：LinkedList底层是通过双向链表实现的。所以，LinkedList和ArrayList之前的区别主要就是数组和链表的 区别。 4、数组中查询和赋值比较快，因为可以直接通过数组下标访问指定位置。 链表中删除和增加比较快，因为可以直接通过修改链表的指针（Java中并无指针，这里可以简单理解为指针。 其实是通过Node节点中的变量指定进行元素的增删。 5、Vector：Vector和ArrayList一样，都是通过数组实现的，但是Vector是线程安全的。 和ArrayList相比，其中的很多方法都通过同步（synchronized）处理来保证线程安全。 6、如果你的程序不涉及到线程安全问题，那么使用ArrayList是更好的选择（因为Vector使用synchronized，必然会影响 效率）。 二者之间还有一个区别，就是扩容策略不一样。在List被第一次创建的时候，会有一个初始大小，随着不断向List中增加元素，当List认为容量不够的时候就会进行扩容。 Vector缺省情况下自动增长原来一倍的数组长度，ArrayList增长原来的50%。 1.2 Set Set继承于Collection接口，是一个不允许出现重复元素，并且无序的集合，主要有HashSet和TreeSet两大实现类 在判断重复元素的时候，Set集合会调用hashCode()和equal()方法来实现 public class HashSetTest { public static void main(String[] agrs){ //hashCode() 和 equals()测试： hashCodeAndEquals(); } public static void hashCodeAndEquals(){ //第一个 Set集合： Set&lt;String&gt; set1 = new HashSet&lt;String&gt;(); String str1 = new String(\"jiaboyan\"); String str2 = new String(\"jiaboyan\"); set1.add(str1); set1.add(str2); System.out.println(\"长度：\"+set1.size()+\",内容为：\"+set1); //第二个 Set集合： Set&lt;App&gt; set2 = new HashSet&lt;App&gt;(); App app1 = new App(); app1.setName(\"jiaboyan\"); App app2 = new App(); app2.setName(\"jiaboyan\"); set2.add(app1); set2.add(app2); System.out.println(\"长度：\"+set2.size()+\",内容为：\"+set2); //第三个 Set集合： Set&lt;App&gt; set3 = new HashSet&lt;App&gt;(); App app3 = new App(); app3.setName(\"jiaboyan\"); set3.add(app3); set3.add(app3); System.out.println(\"长度：\"+set3.size()+\",内容为：\"+set3); } } ​ 测试结果： 长度：1,内容为：[jiaboyan] 长度：2,内容为：[com.jiaboyan.collection.App@efb78af, com.jiaboyan.collection.App@5f3306ad] 长度：1,内容为：[com.jiaboyan.collection.App@1fb030d8] ​ 可以看到，第一个Set集合中最终只有一个元素；第二个Set集合保留了2个元素；第三个集合也只有1个元素； ​ 究竟是什么原因呢？ ​ 让我们来看看HashSet的add(E e)方法： public boolean add(E e) { return map.put(e, PRESENT)==null; } ​ 在底层HashSet调用了HashMap的put(K key, V value)方法: public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } ​ 通过查看以上的源码，我们可以了解到：实际的逻辑都是在HashMap的put()方法中 int hash = hash(key) 对传入的key计算hash值； int i = indexFor(hash, table.length) 对hash值进行转换，转换成数组的index(HashMap中底层存储使用了Entry&lt;K,V&gt;[]数组)； for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) 判断对应index下是否存在元素； 如果存在，则if(e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k)))判断； 如果不存在，则addEntry(hash, key, value, i)直接添加； ​ 简单概括如下： ​ 在向HashMap中添加元素时，先判断key的hashCode值是否相同，如果相同，则调用equals()、==进行判断， 若相同则覆盖原有元素；如果不同，则直接向Map中添加元素； ​ 反过来，我们在看下上面的例子： ​ 在第一个Set集合中,我们new了两个String对象,赋了相同的值。当传入到HashMap中时key均为”jingyanbao”所 以hash 和i的值都相同。进行if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k)))判断，由于 String 对象重写 了 equals()方法，所以在((k = e.key) == key || key.equals(k))判断时，返回了true，所以第二次的 插入并不会 增加 Set集合的长度； ​ 第二个Set集合中，也是new了两个对象，但没有重写equals()方法（底层调用的Object的equals()，也就是==判 断），所以会增加2个元素； ​ 第三个Set集合中，只new了一个对象，调用的两次add方法都添加的这个新new的对象，所以也只是保留了1个 元素； HashSet是哈希表结构，主要利用HashMap的key来存储元素，计算插入元素的hashCode来获取元素在集合中的位置 TreeSet是红黑树结构，每一个元素都是树中的一个节点，插入的元素都会进行排序； Set集合框架结构： HashSet: HashSet底层由HashMap实现，插入的元素被当做是HashMap的key，根据hashCode值来确定集合中的位置，由于Set集合中并没有角标的概念，所以并没有像List一样提供get（）方法。当获取HashSet中某个元素时，只能通过遍历集合的方式进行equals()比较来实现； 不允许出现重复因素； 允许插入Null值； 元素无序（添加顺序和遍历顺序不一致）； 线程不安全，若2个线程同时操作HashSet，必须通过代码实现同步； TreeSet: 从名字上可以看出，此集合的实现和树结构有关。与HashSet集合类似，TreeSet也是基于Map来实现，具体实现TreeMap(后面讲解)，其底层结构为红黑树（特殊的二叉查找树）； 与HashSet不同的是，TreeSet具有排序功能，分为自然排序(123456)和自定义排序两类，默认是自然排序；在程序中，我们可以按照任意顺序将元素插入到集合中，等到遍历时TreeSet会按照一定顺序输出–倒序或者升序； 具有如下特点： 对插入的元素进行排序，是一个有序的集合（主要与HashSet的区别）; 底层使用红黑树结构，而不是哈希表结构； 允许插入Null值； 不允许插入重复元素； 线程不安全； 基本操作： public class TreeSetTest { public static void main(String[] agrs){ TreeSet&lt;String&gt; treeSet = new TreeSet&lt;String&gt;(); System.out.println(\"TreeSet初始化容量大小：\"+treeSet.size()); //元素添加： treeSet.add(\"my\"); treeSet.add(\"name\"); treeSet.add(\"jiaboyan\"); treeSet.add(\"hello\"); treeSet.add(\"world\"); treeSet.add(\"1\"); treeSet.add(\"2\"); treeSet.add(\"3\"); System.out.println(\"TreeSet容量大小：\" + treeSet.size()); System.out.println(\"TreeSet元素顺序为：\" + treeSet.toString()); //增加for循环遍历： for(String str:treeSet){ System.out.println(\"遍历元素：\"+str); } //迭代器遍历：升序 Iterator&lt;String&gt; iteratorAesc = treeSet.iterator(); while(iteratorAesc.hasNext()){ String str = iteratorAesc.next(); System.out.println(\"遍历元素升序：\"+str); } //迭代器遍历：降序 Iterator&lt;String&gt; iteratorDesc = treeSet.descendingIterator(); while(iteratorDesc.hasNext()){ String str = iteratorDesc.next(); System.out.println(\"遍历元素降序：\"+str); } //元素获取:实现NavigableSet接口 String firstEle = treeSet.first();//获取TreeSet头节点： System.out.println(\"TreeSet头节点为：\" + firstEle); // 获取指定元素之前的所有元素集合：(不包含指定元素) SortedSet&lt;String&gt; headSet = treeSet.headSet(\"jiaboyan\"); System.out.println(\"jiaboyan节点之前的元素为：\"+headSet.toString()); //获取给定元素之间的集合：（包含头，不包含尾） SortedSet subSet = treeSet.subSet(\"1\",\"world\"); System.out.println(\"1--jiaboan之间节点元素为：\"+subSet.toString()); //集合判断： boolean isEmpty = treeSet.isEmpty(); System.out.println(\"TreeSet是否为空：\"+isEmpty); boolean isContain = treeSet.contains(\"who\"); System.out.println(\"TreeSet是否包含who元素：\"+isContain); //元素删除： boolean jiaboyanRemove = treeSet.remove(\"jiaboyan\"); System.out.println(\"jiaboyan元素是否被删除\"+jiaboyanRemove); //集合中不存在的元素，删除返回false boolean whoRemove = treeSet.remove(\"who\"); System.out.println(\"who元素是否被删除\"+whoRemove); //删除并返回第一个元素：如果set集合不存在元素，则返回null String pollFirst = treeSet.pollFirst(); System.out.println(\"删除的第一个元素：\"+pollFirst); //删除并返回最后一个元素：如果set集合不存在元素，则返回null String pollLast = treeSet.pollLast(); System.out.println(\"删除的最后一个元素：\"+pollLast); treeSet.clear();//清空集合: } } TreeSet元素排序：https://www.jianshu.com/p/d6cff3517688 1.3 Map 通过查看Map接口描述，看到Map有多个子类，这里我们主要讲解常用的HashMap集合、LinkedHashMap集合。 HashMap&lt;K,V&gt;：存储数据采用的哈希表结构，元素的存取顺序不能保证一致。由于要保证键的唯一、不重复，需要重写键的hashCode()方法、equals()方法。 LinkedHashMap&lt;K,V&gt;：HashMap下有个子类LinkedHashMap，存储数据采用的哈希表结构+链表结构。通过链表结构可以保证元素的存取顺序一致；通过哈希表结构可以保证的键的唯一、不重复，需要重写键的hashCode()方法、equals()方法。 注意：Map接口中的集合都有两个泛型变量&lt;K,V&gt;,在使用时，要为两个泛型变量赋予数据类型。两个泛型变量&lt;K,V&gt;的数据类型可以相同，也可以不同。 代码演示： public class MapDemo { public static void main(String[] args) { //创建Map对象 Map&lt;String, String&gt; map = new HashMap&lt;String,String&gt;(); //数据采用的哈希表结构 //给map中添加元素 map.put(\"星期一\", \"Monday\"); map.put(\"星期日\", \"Sunday\"); System.out.println(map); // {星期日=Sunday, 星期一=Monday} //当给Map中添加元素，会返回key对应的原来的value值，若key没有对应的值，返回null System.out.println(map.put(\"星期一\", \"Mon\")); // Monday System.out.println(map); // {星期日=Sunday, 星期一=Mon} //根据指定的key获取对应的value String en = map.get(\"星期日\"); System.out.println(en); // Sunday //根据key删除元素,会返回key对应的value值 String value = map.remove(\"星期日\"); System.out.println(value); // Sunday System.out.println(map); // {星期一=Mon} } } HashMap和HashTable的区别1、HashTable是线程安全的，HashMap是线程不安全的。 2、HashTable的键不能为null，HashMap的键可以为null。 3、HashTable继承自Dirctionary抽象类，HashMap继承自AbstractMap类。 4、扩容大小：HashTable两倍加一，HashMap两倍。 5、初始容量：HashTable为11，HashMap为16。 2、 Java常见集合的默认大小及扩容机制在面试后台开发的过程中，集合是面试的热话题，不仅要知道各集合的区别用法，还要知道集合的扩容机制，今天我们就来谈下ArrayList 和 HashMap的默认大小以及扩容机制。 在 Java 7 中，查看源码可以知道：ArrayList 的默认大小是 10 个元素，HashMap 的默认大小是16个元素（必须是2的幂，为什么呢？？？下文有解释）。这就是 Java 7 中 ArrayList 和 HashMap 类 的代码片段： // from ArrayList.java JDK 1.7 private static final int DEFAULT_CAPACITY = 10; //from HashMap.java JDK 7 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 这里要讨论这些常用的默认初始容量和扩容的原因是： 当底层实现涉及到扩容时，容器或重新分配一段更大的连续内存（如果是离散分配则不需要重新分配，离散分配都是插入新元素时动态分配内存），要将容器原来的数据全部复制到新的内存上， 这无疑使效率大大降低。加载因子的系数小于等于1，意指即当元素个数超过容量长度*加载因子的系数时，进行扩容。另外，扩容也是有默认的倍数的，不同的容器扩容情况不同。 List 元素是有序的、可重复 ArrayList、Vector默认初始容量为10 Vector：线程安全，但速度慢 底层数据结构是数组结构 加载因子为1：即当 元素个数 超过 容量长度 时，进行扩容 扩容增量：原容量的 1倍 如 Vector的容量为10，一次扩容后是容量为20 ArrayList：线程不安全，查询速度快 底层数据结构是数组结构 扩容增量：原容量的 0.5倍+1 如 ArrayList的容量为10，一次扩容后是容量为16 Set(集) 元素无序的、不可重复。 HashSet：线程不安全，存取速度快 底层实现是一个HashMap（保存数据），实现Set接口 默认初始容量为16（为何是16，见下方对HashMap的描述） 加载因子为0.75：即当 元素个数 超过 容量长度的0.75倍 时，进行扩容 扩容增量：原容量的 1 倍 如 HashSet的容量为16，一次扩容后是容量为32 Map是一个双列集合 HashMap：默认初始容量为16 （为何是16：16是2^4，可以提高查询效率，另外，32=16&lt;&lt;1） 加载因子为0.75：即当 元素个数 超过 容量长度的0.75倍 时，进行扩容 扩容增量：原容量的 1 倍 如 HashSet的容量为16，一次扩容后是容量为32 接下来我们来谈谈hashMap的数组长度为什么保持2的次幂？ hashMap的数组长度一定保持2的次幂，比如16的二进制表示为 10000，那么length-1就是15，二进制为01111，同理扩容后的数组长度为32，二进制表示为100000，length-1为31，二进制表示为011111。 这样会保证低位全为1，而扩容后只有一位差异，也就是多出了最左位的1，这样在通过 h&amp;(length-1)的时候，只要h对应的最左边的那一个差异位为0，就能保证得到的新的数组索引和老数组索引一致(大大减少了 之前已经散列良好的老数组的数据位置重新调换)，还有，数组长度保持2的次幂，length-1的低位都为1，会使得获得的数组索引index更加均匀。 static int indexFor(int h, int length) { return h &amp; (length-1); } 首先算得key得hashcode值，然后跟数组的长度-1做一次“与”运算（&amp;）。看上去很简单，其实比较有玄机。比如数组的长度是2的4次方，那么hashcode就会和2的4次方-1做“与”运算。很多人都有这个疑问， 为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高，我以2的4次方举例，来解释一下为什么数组大小为2的幂时hashmap访问的性能最高。 看下图，左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同 一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的 值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组 长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率. 所以说，当数组长度为2的n次幂的时候，不同的key算得得index相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。 说到这里，我们再回头看一下hashmap中默认的数组大小是多少，查看源代码可以得知是16，为什么是16，而不是15，也不是20呢，看到上面的解释之后我们就清楚了吧，显然是因为16是2的整数次幂的原因， 在小数据量的情况下16比15和20更能减少key之间的碰撞，而加快查询的效率。 3.集合总结3.1、Java的4大接口1、Collection：顶级集合接口，其下有List和Set两大接口。 2、List：存储有序的、不唯一的数据。 3、Set：存储无序的、唯一的数据。 4、Map：以键值对的形式存储数据，以键取值。键不能重复，但值可以重复。 3.2、List接口是一个有序集合，继承自Collection接口。现已知常用实现类有：ArrayList、LinkedList、Vector。 1、ArrayList实现了一个长度可变的数组，在内存空间中开辟一串连续的空间。 2、LinkedList使用链表结构存储数据，在插入和删除元素是速度非常快。 3、Vector实现了一个动态数组，它是同步访问的。 3.3、如何遍历List1、for循环。 2、foreach遍历。 for(type element: array) { System.out.println(element); } 3、使用迭代器遍历列表。 3.4、Set接口是一个无序集合，继承自Collection接口。现已知常用实现类有：HashSet、LinkedHashSet、TreeSet。 1、HashSet：底层是HashMap的相关方法，传入数据后，根据数据的hashCode进行散列运算，得到一个散列值后再进行运算，确定元素在序列中存储的位置。所以，使用HashSet存数据必须在实体类中重写hashCode和equals方法！！ 2、LinkedHashSet：在HashSet的基础上，新增了一个链表。用链表来记录HashSet种元素放入的顺序；HashSet依然是无序的，但链表会按照存入的顺序存储。 3、TreeSet：将存入的数据，进行排序，然后输出。如果传入的是一个实体对象，那么需要传入比较器：实体类实现Comparable接口，并重写CompareTo方法。 3.5、Map接口它以键值对的形式存储数据，键不能重复，值可以重复。常用实现类有：HashMap、HashTable、LinkedHashMap、TreeMap。 1、LinkedHashMap：可以使用列表，记录数据放入的次序，输出的顺序与放入的顺序一致。 2、TreeMap：根据键的顺序，进行排序后输出。如果传入的是实体对象，必须重写比较函数。 3.6、HashMap和HashTable的区别1、HashTable是线程安全的，HashMap是线程不安全的。 2、HashTable的键不能为null，HashMap的键可以为null。 3、HashTable继承自Dirctionary抽象类，HashMap继承自AbstractMap类。 4、扩容大小：HashTable两倍加一，HashMap两倍。 5、初始容量：HashTable为11，HashMap为16。 4.集合面试题4.1、常见的集合有哪些1、Map接口和Collection接口是所有集合框架的父接口。2、Collection接口的子接口包括：Set接口和List接口3、Map接口的实现类主要有：HashMap、TreeMap、Hashtable、ConcurrentHashMap以及Properties等4、Set接口的实现类主要有：HashSet、TreeSet、LinkedHashSet等5、List接口的实现类主要有：ArrayList、LinkedList、Stack以及Vector等 4.2、HashMap与HashTable的区别1、HashMap没有考虑同步，是线程不安全的。Hashtable使用了synchronized关键字，是线程安全的。2、HashMap允许K/V都为null。后者K/V都不允许为null。3、HashMap继承自AbstractMap类。而Hashtable继承自Dictionary类。 4.3、线程安全和线程不安全的区别线程安全就是多个并行线程访问共享数据时，采用了加锁机制，当一个线程访问共享数据时，进行保护，其他线程不能进行访问，直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。线程不安全就是不提供共享数据访问保护，有可能出现多个线程先后更改共享数据，造成所得到的共享数据是脏数据。 4.4、什么是哈希Hash，一般翻译为“散列”，也有直接音译为“哈希”的，这就是把任意长度的输入通过散列算法，变换成固定长度的输出，该输出就是散列值(哈希值)。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。所有散列函数都有如下一个基本特性：根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。 4.5、什么是哈希冲突当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做碰撞(哈希碰撞)。 4.6、HashSet是如何保证数据不重复的HashSet的底层其实就是HashMap，由于HashMap的K值本身就不允许重复，并且在HashMap中如果K/V相同时，会用新的V覆盖掉旧的V，然后返回旧的V，那么在HashSet中执行这一句话始终会返回一个false，导致插入失败，这样就保证了数据的不可重复性。 4.7、Array和ArrayList的区别1、LinkedList实现了List和Deque接口，一般称为双向链表。ArrayList实现了List接口，动态数组。2、LinkedList在插入和删除数据时效率更高，ArrayList在查找某个index的数据时效率更高。3、LinkedList比ArrayList需要更多的内存。 4.8、Enumeration和Iterator接口的区别1、Enumeration的速度是Iterator的两倍，也使用更少的内存。2、与Enumeration相比，Iterator更加安全，因为当一个集合正在被遍历的时候，它会阻止其它线程去修改集合。3、迭代器取代了Java集合框架中的Enumeration。迭代器允许调用者从集合中移除元素，而Enumeration不能做到。","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"},{"name":"collection","slug":"java/collection","permalink":"https://13592491893.github.io/categories/java/collection/"}],"tags":[{"name":"list","slug":"list","permalink":"https://13592491893.github.io/tags/list/"},{"name":"set","slug":"set","permalink":"https://13592491893.github.io/tags/set/"},{"name":"map","slug":"map","permalink":"https://13592491893.github.io/tags/map/"}]},{"title":"vue实际工作常用方法","slug":"vue实际工作常用方法","date":"2020-10-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/8a416b2c.html","link":"","permalink":"https://13592491893.github.io/posts/8a416b2c.html","excerpt":"","text":"1.select选择框获取key和value&lt;el-select v-model=\"form.deviceType\" value-key=\"value\" @change=\"getDeviceTypeName\" :disabled=\"this.form.status == 'change'? true:false\"&gt; &lt;el-option v-for=\"item in deviceTypeList\" :key=\"item.listCode\" :label=\"item.listDesc\" :value=\"item.listCode\"&gt; &lt;/el-option&gt; &lt;/el-select&gt; getDeviceTypeName(value) { let obj = this.deviceTypeList.find(item =&gt; { //这里的oneData就是上面遍历的数据源 return item.listCode === value; //筛选出匹配数据 }); this.form.deviceTypeName = obj.listDesc; console.log(this.form.deviceTypeName); }, select标签绑定change事件 change事件里获取key和value的值 2.格式化列表字段&lt;el-table-column prop=\"isChange\" label=\"新增/更换\" :show-overflow-tooltip=\"true\" :formatter=\"isChangeFormatter\" align=\"center\"&gt; &lt;/el-table-column&gt; isChangeFormatter(row, column, cellValue) { if (row.isChange == 'Y') { return '更换'; } return '新增'; }, 3.webpack中alias配置中的“@”是什么意思如题所示，build文件夹下的webpack.base.conf.js resolve: { extensions: ['.js', '.vue', '.json'], alias: { 'vue$': 'vue/dist/vue.esm.js', '@': resolve('src') } } 其中的@的意思是：只是一个别名而已。这里设置别名是为了让后续引用的地方减少路径的复杂度 //例如 src - components - a.vue - router - home - index.vue //index.vue 里，正常引用 A 组件： import A from '../../components/a.vue' //如果设置了 alias 后。 alias: { 'vue$': 'vue/dist/vue.esm.js', '@': resolve('src') } //引用的地方路径就可以这样了 import A from '@/components/a.vue' //这里的 @ 就起到了【resolve('src')】路径的作用。 4.webpack proxyTable 代理跨域webpack 开发环境可以使用proxyTable 来代理跨域，生产环境的话可以根据各自的服务器进行配置代理跨域就行了。在我们的项目config/index.js 文件下可以看到有一个proxyTable的属性，我们对其简单的改写 proxyTable: { '/api': { target: 'http://api.douban.com/v2', changeOrigin: true, pathRewrite: { '^/api': '' } } } 这样当我们访问localhost:8080/api/movie的时候 其实我们访问的是http://api.douban.com/v2/movi…。 当然我们也可以根据具体的接口的后缀来匹配代理，如后缀为.shtml，代码如下： proxyTable: { '**/*.shtml': { target: 'http://192.168.198.111:8080/abc', changeOrigin: true } } 可参考地址：1.webpack 前后端分离开发接口调试解决方案，proxyTable解决方案2.http-proxy-middleware 5.vue2.x父子组件以及非父子组件之间的通信1.父组件传递数据给子组件父组件数据如何传递给子组件呢？可以通过props属性来实现 父组件： &lt;parent&gt; &lt;child :child-msg=\"msg\"&gt;&lt;/child&gt;//这里必须要用 - 代替驼峰 &lt;/parent&gt; data(){ return { msg: [1,2,3] }; } 子组件通过props来接收数据: 方式1： props: ['childMsg'] 方式2 : props: { childMsg: Array //这样可以指定传入的类型，如果类型不对，会警告 } 方式3： props: { childMsg: { type: Array, default: [0,0,0] //这样可以指定默认的值 } } 这样呢，就实现了父组件向子组件传递数据. 2.子组件与父组件通信子组件： &lt;template&gt; &lt;div @click=\"up\"&gt;&lt;/div&gt; &lt;/template&gt; methods: { up() { this.$emit('fun','这是一段内容'); //主动触发fun方法，'这是一段内容'为向父组件传递的数据 } } 父组件: &lt;div&gt; &lt;child @fun=\"change\" :msg=\"msg\"&gt;&lt;/child&gt; //监听子组件触发的fun事件,然后调用change方法 &lt;/div&gt; methods: { change(msg) { this.msg = msg; } } 3.非父子组件通信如果2个组件不是父子组件那么如何通信呢？这时可以通过eventHub来实现通信.所谓eventHub就是创建一个事件中心，相当于中转站，可以用它来传递事件和接收事件. let Hub = new Vue(); //创建事件中心 组件1触发： &lt;div @click=\"eve\"&gt;&lt;/div&gt; methods: { eve() { Hub.$emit('change','hehe'); //Hub触发事件 } } 组件2接收: &lt;div&gt;&lt;/div&gt; created() { Hub.$on('change', () =&gt; { //Hub接收事件 this.msg = 'hehe'; }); } 可参考：vue非父子组件怎么进行通信 6.select选择器回显问题 单位应该显示为千克 ,但是回显的后unitId是2,默认成string类型了,要手动设置一下: &lt;el-form-item label=\"单位\" prop=\"unitId\"&gt; &lt;el-select v-model=\"equimentEditeForm.unitId\" value-key=\"value\" size=\"small\" v-bind:disabled=\"diasabledInput\"&gt; &lt;el-option v-for=\"item in itemUnit\" :key=\"parseInt(item.unitId)\" :label=\"item.unitName\" :value=\"parseInt(item.unitId)\"&gt; &lt;/el-option&gt; &lt;/el-select&gt; &lt;/el-form-item&gt; 可参考:https://blog.csdn.net/qq_43779703/article/details/100693565 7.父子组件弹窗返回值问题 仪器参数定义开发时遇到问题:打断点时弹窗关闭 解决办法:vue点击弹框之外部位弹框消失,dialog加上属性：:close-on-click-modal=\"false\" 弹出框按x或者关闭的时候，下次再点击放大镜不能弹出 分析：弹窗是使用watch监听showInstrumentDefineSelectDialog值变化去控制窗口的打开和关闭，如果关闭的时候直接使用this.dialogTableVisible = false,showInstrumentDefineSelectDialog的值就不会变化(为true)，下次再点放大镜showInstrumentDefineSelectDialog的值还是true，没有变化，所以watch不会生效，正确关闭的时候应该使用this.$emit(\"update:showInstrumentDefineSelectDialog\", false,通过watch方法再改变dialogTableVisible的值 子组件关闭方法修改： // 关闭弹框 OnClose () { // this.dialogTableVisible = false this.$emit(\"update:showInstrumentDefineSelectDialog\", false) }, 正确的父子页面模板: ​ 父页面： ​ &lt;template&gt; &lt;div class=\"app-container el-main cardshadow\"&gt; &lt;el-form :model=\"queryParams\" ref=\"queryForm\" :inline=\"true\" v-show=\"showSearch\"&gt; &lt;el-form-item label=\"工具/仪器类型\" prop=\"deviceType\"&gt; &lt;el-input v-model=\"queryParams.deviceType\" placeholder=\"请输入仪器类型\" clearable size=\"small\" @keyup.enter.native=\"handleQuery\" /&gt; &lt;/el-form-item&gt; &lt;el-form-item label=\"类型代码\" prop=\"paraTypeCode\"&gt; &lt;el-input v-model=\"queryParams.paraTypeCode\" placeholder=\"请输入类型代码\" clearable size=\"small\" @keyup.enter.native=\"handleQuery\" /&gt; &lt;/el-form-item&gt; &lt;el-form-item label=\"类型名称\" prop=\"paraTypeName\"&gt; &lt;el-input v-model=\"queryParams.paraTypeName\" placeholder=\"请输入参数类型名称\" clearable size=\"small\" @keyup.enter.native=\"handleQuery\" /&gt; &lt;/el-form-item&gt; &lt;el-form-item label=\"参数代码\" prop=\"posParaCode\"&gt; &lt;el-input v-model=\"queryParams.posParaCode\" placeholder=\"请输入参数代码\" clearable size=\"small\" @keyup.enter.native=\"handleQuery\" /&gt; &lt;/el-form-item&gt; &lt;el-form-item&gt; &lt;el-button type=\"primary\" size=\"mini\" @click=\"handleQuery\"&gt;查询&lt;/el-button&gt; &lt;el-button type=\"primary\" size=\"mini\" @click=\"resetQuery\"&gt;重置&lt;/el-button&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;el-row :gutter=\"10\" class=\"mb8\"&gt; &lt;el-col :span=\"1.5\"&gt; &lt;el-button type=\"primary\" icon=\"el-icon-plus\" size=\"mini\" @click=\"handleAdd\"&gt;新增 &lt;/el-button&gt; &lt;/el-col&gt; &lt;/el-row&gt; &lt;el-table v-loading=\"loading\" border :data=\"posParaList\" size=\"mini\"&gt; &lt;el-table-column label=\"操作\" align=\"center\" class-name=\"small-padding fixed-width\"&gt; &lt;template slot-scope=\"scope\"&gt; &lt;el-button size=\"mini\" type=\"text\" icon=\"el-icon-edit\" @click=\"handleUpdate(scope.row)\"&gt;修改&lt;/el-button&gt; &lt;el-button size=\"mini\" type=\"text\" icon=\"el-icon-delete\" @click=\"handleDelete(scope.row)\"&gt;删除&lt;/el-button&gt; &lt;/template&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=\"deviceType\" label=\"工具/仪器类型\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;el-table-column prop=\"deviceTypeName\" label=\"工具/仪器类型名称\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;el-table-column prop=\"paraTypeCode\" label=\"类型代码\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;el-table-column prop=\"paraTypeName\" label=\"类型名称\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;el-table-column prop=\"posParaCode\" label=\"参数代码\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;el-table-column prop=\"posParaValue\" label=\"参数值\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;el-table-column prop=\"remark\" label=\"备注\" :show-overflow-tooltip=\"true\" align=\"center\"&gt;&lt;/el-table-column&gt; &lt;/el-table&gt; &lt;div class=\"pagination\"&gt; &lt;el-pagination background @size-change=\"handleSizeChange\" @current-change=\"handleCurrentChange\" :current-page=\"pagination.currentPage\" :page-sizes=\"[10, 20, 50, 100]\" :page-size=\"pagination.pageSize\" layout=\"total, sizes, prev, pager, next\" :total=\"pagination.total\"&gt; &lt;/el-pagination&gt; &lt;/div&gt; &lt;!-- 添加或修改菜单对话框 --&gt; &lt;el-dialog :title=\"title\" :visible.sync=\"open\" width=\"600px\" :close-on-click-modal=\"false\" append-to-body&gt; &lt;el-form ref=\"form\" :model=\"form\" :rules=\"rules\" label-width=\"120px\"&gt; &lt;el-row&gt; &lt;el-col :span=\"12\"&gt; &lt;el-form-item label=\"工具/仪器类型\" prop=\"deviceType\"&gt; &lt;!-- &lt;el-select v-model=\"form.deviceType\" value-key=\"value\" @change=\"getDeviceTypeName\" :disabled=\"this.form.status == 'change'? true:false\"&gt;--&gt; &lt;!-- &lt;el-option v-for=\"item in deviceTypeList\" :key=\"item.listCode\" :label=\"item.listDesc\" :value=\"item.listCode\"&gt;--&gt; &lt;!-- &lt;/el-option&gt;--&gt; &lt;!-- &lt;/el-select&gt;--&gt; &lt;el-input v-model=\"form.deviceTypeName\" v-show=\"false\"&gt;&lt;/el-input&gt; &lt;el-input v-model=\"form.deviceType\" maxlength=\"200\" :disabled=\"true\" /&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; &lt;el-col :span=\"12\"&gt; &lt;el-form-item label=\"类型代码\" prop=\"paraTypeCode\"&gt; &lt;!-- &lt;el-input v-model=\"form.paraTypeCode\" maxlength=\"60\" :disabled=\"this.form.status == 'change'? true:false\" placeholder=\"请输入类型代码\"&gt;&lt;/el-input&gt;--&gt; &lt;el-input v-model=\"form.paraTypeCode\" maxlength=\"60\" :disabled=\"disabledInput\" suffix-icon=\"el-icon-search\" @click.native=\"showInstrumentCheck\" placeholder=\"请输入类型代码\"&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; &lt;/el-row&gt; &lt;el-row&gt; &lt;el-col :span=\"12\"&gt; &lt;el-form-item label=\"类型名称\" prop=\"paraTypeName\"&gt; &lt;el-input v-model=\"form.paraTypeName\" maxlength=\"200\" :disabled=\"true\"/&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; &lt;el-col :span=\"12\"&gt; &lt;el-form-item label=\"参数代码\" prop=\"posParaCode\"&gt; &lt;el-input v-model=\"form.posParaCode\" maxlength=\"60\" placeholder=\"请输入参数代码\" /&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; &lt;/el-row&gt; &lt;el-row&gt; &lt;el-col :span=\"12\"&gt; &lt;el-form-item label=\"参数值\" prop=\"posParaValue\"&gt; &lt;el-input v-model=\"form.posParaValue\" maxlength=\"60\" placeholder=\"请输入参数值\" /&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; &lt;el-col :span=\"12\"&gt; &lt;el-form-item label=\"备注\" prop=\"remark\"&gt; &lt;el-input v-model=\"form.remark\" maxlength=\"30\" placeholder=\"请输入备注\" /&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; &lt;/el-row&gt; &lt;/el-form&gt; &lt;div slot=\"footer\" class=\"dialog-footer\"&gt; &lt;el-button type=\"primary\" @click=\"submitForm\"&gt;确 定&lt;/el-button&gt; &lt;el-button @click=\"cancel\"&gt;取 消&lt;/el-button&gt; &lt;/div&gt; &lt;instrument-define-select :showInstrumentDefineSelectDialog.sync=\"showInstrumentDefineSelectDialog\" :enable=\"enable\" @checkInstrument=\"checkInstrument\"&gt;&lt;/instrument-define-select&gt; &lt;/el-dialog&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import instrumentDefineSelect from \"../../../components/company/instrumentDefine/instrumentDefineSelect\" export default { name: \"posPara\", components: {instrumentDefineSelect}, data () { return { // 遮罩层 loading: true, // 显示搜索条件 showSearch: true, // 供应商类型表格数据 posParaList: [], deviceTypeList: [], showInstrumentDefineSelectDialog: false, disabledInput: false, enable: false, // 弹出层标题 title: \"\", // 是否显示弹出层 open: false, // 查询参数 queryParams: { deviceType: \"\", paraTypeCode: \"\", paraTypeName: \"\", posParaCode: \"\" }, // 表单参数 form: {}, pagination: { currentPage: 1, pageSize: 10, total: 0 }, // 表单校验 rules: { deviceType: [ { required: true, message: \"工具/仪器类型不能为空\", trigger: \"blur\" } ], paraTypeCode: [ { required: true, message: \"类型代码不能为空\", trigger: \"change\" }, { pattern: /^[A-Za-z0-9]{1,30}$/, message: \"请输入数字或者字母\", trigger: \"blur\" } ], paraTypeName: [ { required: true, message: \"类型名称不能为空\", trigger: \"blur\" } ], posParaCode: [ { required: true, message: \"参数代码不能为空\", trigger: \"blur\" }, { pattern: /^[A-Za-z0-9]{1,30}$/, message: \"请输入数字或者字母\", trigger: \"blur\" } ], posParaValue: [ { required: true, message: \"参数值不能为空\", trigger: \"blur\" } ] } } }, created () { this.getPosParaList() this.getDeviceTypeList() }, methods: { /** 查询参数列表 */ getPosParaList () { this.loading = true let that = this let datas = { deviceType: that.queryParams.deviceType, paraTypeCode: that.queryParams.paraTypeCode, paraTypeName: that.queryParams.paraTypeName, posParaCode: that.queryParams.posParaCode, pageNum: this.pagination.currentPage, pageSize: this.pagination.pageSize } this.$request .fetchGetPosPara(datas) .then(res =&gt; { that.posParaList = res.data.data.list that.pagination.total = res.data.data.total }) .catch(err =&gt; { console.log(err) }) this.loading = false }, /** 查询工具仪器类型 */ getDeviceTypeList () { let that = this let data = { listTypeCode: \"posDeviceType\" } this.$request .fetchSystemResourceList(data) .then(res =&gt; { that.deviceTypeList = res.data.data.list // that.fixStatus.put({listCode:'',listDesc:'全部'}); }) .catch(err =&gt; { console.log(err) }) }, getDeviceTypeName (value) { let obj = this.deviceTypeList.find(item =&gt; { // 这里的oneData就是上面遍历的数据源 return item.listCode === value // 筛选出匹配数据 }) this.form.deviceTypeName = obj.listDesc console.log(this.form.deviceTypeName) }, // 取消按钮 cancel () { this.open = false }, // 表单重置 reset () { this.form = { deviceType: \"\", paraTypeCode: \"\", paraTypeName: \"\", posParaCode: \"\", sorPosParaCode: \"\", posParaValue: \"\", remark: \"\" } this.resetForm(\"form\") }, /** 搜索按钮操作 */ handleQuery () { this.getPosParaList() }, /** 重置按钮操作 */ resetQuery () { this.resetForm(\"queryForm\") this.getPosParaList() }, /** 新增按钮操作 */ handleAdd (row) { this.reset() this.open = true this.title = \"仪器参数新增\" this.disabledInput = false }, /** 修改按钮操作 */ handleUpdate (row) { this.reset() this.form = JSON.parse(JSON.stringify(row)) this.form.sorPosParaCode = this.form.posParaCode this.form.status = \"change\" this.open = true this.disabledInput = true this.title = \"仪器参数修改\" }, /** 提交按钮 */ submitForm: function () { let from = this.form this.$refs[\"form\"].validate(valid =&gt; { if (valid) { if (this.form.status === \"change\") { let datas = { deviceType: from.deviceType, deviceTypeName: from.deviceTypeName, paraTypeCode: from.paraTypeCode, paraTypeName: from.paraTypeName, posParaCode: from.posParaCode, sorPosParaCode: from.sorPosParaCode, posParaValue: from.posParaValue, remark: from.remark } this.$request .fetchUpdatePosPara(datas) .then(res =&gt; { if (res.data.code === 0) { this.$message({ message: res.data.message, type: \"success\" }) } else { Message({ message: res.data.message, type: \"warning\" }) } this.open = false this.getPosParaList() }) .catch(err =&gt; { console.log(err) }) } else { let datas = { deviceType: from.deviceType, deviceTypeName: from.deviceTypeName, paraTypeCode: from.paraTypeCode, paraTypeName: from.paraTypeName, posParaCode: from.posParaCode, posParaValue: from.posParaValue, remark: from.remark } this.$request .fetchAddPosPara(datas) .then(res =&gt; { if (res.data.code === 0) { this.$message({ message: res.data.message, type: \"success\" }) } else { Message({ message: res.data.message, type: \"warning\" }) } this.open = false this.getPosParaList() }) .catch(err =&gt; { console.log(err) }) } } }) }, /** 删除按钮操作 */ handleDelete (row) { let that = this this.$confirm(\"是否删除工具/仪器类型\", \"警告\", { confirmButtonText: \"确定\", cancelButtonText: \"取消\", type: \"warning\" }) .then(function () { return that.delType(row) }) .catch(function () {}) }, // 删除工具/仪器类型 delType (row) { // let datas = { // deviceType: row.deviceType, // paraTypeCode: row.paraTypeCode, // posParaCode: row.posParaCode // }; this.$request .fetchDelPosPara(row) .then(res =&gt; { this.getPosParaList() if (res.data.code === 0) { this.$message({ message: res.data.message, type: \"success\" }) } else { Message({ message: res.data.message, type: \"warning\" }) } }) .catch(err =&gt; { console.log(err) }) }, // 界面改变 handleSizeChange (val) { let that = this that.pagination.pageSize = val that.getPosParaList() }, handleCurrentChange (val) { let that = this that.pagination.currentPage = val that.getPosParaList() }, checkInstrument (row) { console.log(row) let editForms = JSON.parse(JSON.stringify(this.form)) editForms.deviceType = row.deviceType editForms.deviceTypeName = row.deviceTypeName editForms.paraTypeCode = row.paraTypeCode editForms.paraTypeName = row.paraTypeName this.form = editForms this.showInstrumentDefineSelectDialog = false }, showInstrumentCheck () { if (this.disabledInput === false) { this.showInstrumentDefineSelectDialog = true } } } } &lt;/script&gt; &lt;style &gt; .pagination { padding-top: 20px; float: right; } &lt;/style&gt; 子页面： ​ &lt;template&gt; &lt;div&gt; &lt;el-dialog title=\"仪器选择\" :visible.sync=\"dialogTableVisible\" append-to-body width=\"800px\" @close=\"OnClose()\"&gt; &lt;el-form :inline=\"true\" :model=\"queryParams\" class=\"demo-form-inline\"&gt; &lt;el-form-item label=\"类型代码\" prop=\"paraTypeCode\" size=\"mini\"&gt; &lt;el-input v-model=\"queryParams.paraTypeCode\" placeholder=\"请输入类型代码\"&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=\"类型名称\" prop=\"paraTypeName\" size=\"mini\"&gt; &lt;el-input v-model=\"queryParams.paraTypeName\" placeholder=\"请输入类型名称\"&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item size=\"mini\"&gt; &lt;el-button type=\"primary\" size=\"mini\" @click=\"handleQuery\"&gt;查询&lt;/el-button&gt; &lt;el-button type=\"primary\" size=\"mini\" @click=\"resetQuery\"&gt;重置&lt;/el-button&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;div class=\"supplier-content\"&gt; &lt;div style=\"flex:1\"&gt; &lt;el-table :data=\"instrumentList\" highlight-current-row size=\"mini\" border @row-dblclick=\"rowDbClick\" @current-change=\"handleCurrentRow\"&gt; &lt;el-table-column prop=\"deviceType\" label=\"仪器类型\" align=\"center\"&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=\"deviceTypeName\" label=\"仪器类型名称\" align=\"center\"&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=\"paraTypeCode\" label=\"类型代码\" align=\"center\"&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=\"paraTypeName\" label=\"类型名称\" align=\"center\"&gt; &lt;/el-table-column&gt; &lt;/el-table&gt; &lt;div class=\"pagination\"&gt; &lt;el-pagination background @size-change=\"handleSizeChange\" @current-change=\"handleCurrentChange\" :current-page=\"pagination.currentPage\" :page-sizes=\"[10, 20, 50, 100]\" :page-size=\"pagination.pageSize\" layout=\"total, sizes, prev, pager, next\" :total=\"pagination.total\"&gt; &lt;/el-pagination&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div slot=\"footer\" class=\"dialog-footer\"&gt; &lt;el-button type=\"primary\" @click=\"submitForm\"&gt;确 定&lt;/el-button&gt; &lt;el-button @click=\"cancel\"&gt;取 消&lt;/el-button&gt; &lt;/div&gt; &lt;/el-dialog&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default { props: { showInstrumentDefineSelectDialog: false }, data () { return { dialogTableVisible: false, queryParams: { paraTypeCode: \"\", paraTypeName: \"\" }, // 翻页 pagination: { currentPage: 1, pageSize: 10, total: 0 }, instrumentList: [], currentRow: {} } }, created () { this.getInstrumentList() }, watch: { showInstrumentDefineSelectDialog (newValue, oldValue) { this.dialogTableVisible = newValue } }, methods: { /* 查询采集点列表 */ getInstrumentList () { this.loading = true let that = this let datas = { paraTypeCode: that.queryParams.paraTypeCode, paraTypeName: that.queryParams.paraTypeName, pageNum: that.pagination.currentPage, pageSize: that.pagination.pageSize } that.$request .fetchGetPosPara(datas) .then((res) =&gt; { that.instrumentList = res.data.data.list that.pagination.total = res.data.data.total }) .catch((err) =&gt; { console.log(err) }) this.loading = false }, /** 搜索按钮操作 */ handleQuery () { this.getInstrumentList() }, /** 重置按钮操作 */ resetQuery () { this.queryParams = { paraTypeCode: \"\", paraTypeName: \"\" } this.resetForm(\"queryForm\") this.handleQuery() }, // 界面改变 handleSizeChange (val) { let that = this that.pagination.pageSize = val that.getInstrumentList() }, handleCurrentChange (val) { let that = this that.pagination.currentPage = val that.getInstrumentList() }, // 双击行 rowDbClick (row) { this.dialogTableVisible = false // 子组件弹框隐藏 this.$emit(\"checkInstrument\", row) }, // 关闭弹框 OnClose () { // this.dialogTableVisible = false this.$emit(\"update:showInstrumentDefineSelectDialog\", false) }, // 选中某一行 handleCurrentRow (val) { this.currentRow = val }, // 确定按钮 submitForm () { this.dialogTableVisible = false // 子组件弹框隐藏 this.$emit(\"checkInstrument\", this.currentRow) }, // 取消按钮 cancel () { // this.dialogTableVisible = false this.$emit(\"update:showInstrumentDefineSelectDialog\", false) } } } &lt;/script&gt; &lt;style &gt; @import '../../../assets/css/ztree.css'; &lt;/style&gt; 8.弹出框点空白区域消失设定 &lt;el-dialog :title=\"title\" :visible.sync=\"open\" width=\"600px\" append-to-body :close-on-click-modal=\"false\"&gt; 9.后面要用到前面异步执行的结果值(异步变同步)用promise /** 查询点检明细 */ getFixCheckDetail (checkId) { return new Promise((resolve, reject) =&gt; { let that = this let data = { checkId: checkId } that.$request .fetchFixCheckDetail(data) .then((res) =&gt; { console.log(res) that.fixCheckDetail = res.data.data resolve() // that.fixStatus.put({listCode:'',listDesc:'全部'}); }) .catch((err) =&gt; { console.log(err) reject() }) }) } /** 详情按钮操作 */ handleShowDetail (row) { if (row.executeStatus === \"Y\" &amp;&amp; row.modeName === \"点检\") { let that = this let data = JSON.parse(JSON.stringify(row)) Promise.all([ that.getFixCheckDetail(data.checkId), that.getFixCheckAnomalousCount(data.checkId), that.getFixCheckImage(data.checkId) ]).then((values) =&gt; { that.checkEditeForm = data that.checkEditeForm.count = that.anomalousCount that.openMX = true that.titleMX = \"点检单详情\" }).catch((reason) =&gt; { console.log(reason) }) } }, 查百度好像还有一种方法：但是自己测试好像没生效 export function post_request(url, obj) { //返回一个promise实例。 return new Promise((resolve, reject) =&gt; { uni.request({ url: apiurl+url, data: obj, method:'POST', success: (result) =&gt; { resolve(result.data); }, fail: (e) =&gt; { reject(e); } }) }) } post_request只需要返回一个Promise实例即可，success中resolve(result.data) //请求数据 uni.login({ provider: 'weixin', success: async function (loginRes) { let code = loginRes.code let result= await post_request(\"/oauth/oauth\",{code:code}) console.log(result) } }) 使用的时候，在 “上级函数”中加上async。然后再使用的时候加上 let result = await promiseFun() 10.this.$refs介绍调用子组件的方法 11.element里面的trigger: ‘blur’和trigger: ‘change’有什么区别触发方式，blur失去焦点，change数据改变 12.table里面控制按钮权限班计划不能再排产： &lt;el-table-column label=\"操作\" align=\"center\" fixed class-name=\"small-padding fixed-width\" width=\"280\"&gt; &lt;template slot-scope=\"scope\"&gt; &lt;el-button size=\"mini\" type=\"text\" icon=\"el-icon-set-up\" :disabled=\"scope.row.planTypeCode==='FlightPlan'\" @click=\"handleProduct(scope.row)\"&gt;排产&lt;/el-button&gt; &lt;el-button size=\"mini\" type=\"text\" icon=\"el-icon-edit\" @click=\"handleUpdate(scope.row)\"&gt;修改&lt;/el-button&gt; &lt;el-button size=\"mini\" type=\"text\" icon=\"el-icon-delete\" @click=\"handleDel(scope.row)\"&gt;删除&lt;/el-button&gt; &lt;/template&gt; &lt;/el-table-column&gt; 13.上传文件图片及预览(员工证件档案功能)1.form表单里上传文件 &lt;el-col :span=\"24\"&gt; &lt;el-form-item label=\"文件上传\" prop=\"fileList\"&gt; &lt;el-upload class=\"upload-demo\" ref=\"upload\" :action=\"action\" :on-preview=\"handlePreview\" :on-remove=\"handleRemove\" accept=\".png,.jpg\" :on-exceed=\"handleExceed\" :limit=\"3\" :on-success=\"handleSuccess\" :on-error=\"handleError\" :before-upload=\"beforeImgUpload\" list-type=\"picture-card\" :file-list=\"photoList\"&gt; &lt;el-button size=\"small\" type=\"primary\"&gt;点击上传&lt;/el-button&gt; &lt;/el-upload&gt; &lt;!--图片预览的dialog--&gt; &lt;el-dialog :visible.sync=\"dialogVisible\"&gt; &lt;img width=\"100%\" :src=\"previewSrc\"&gt; &lt;/el-dialog&gt; &lt;/el-form-item&gt; &lt;/el-col&gt; return { // 搜索框 maxHeight: null, previewSrc: \"\", previewSrcList: [], dialogVisible: false, remindList: [], photoList: [], action: window.location.protocol + \"//\" + window.location.hostname + \":8081/company/interCertificate/saveFile?token=\" + store.getters.token, serverIp: localStorage.getItem(\"serverIp\"), fileList: [], } 新增/修改按钮：图片的回显问题 /** 新增按钮操作 */ handleAdd () { this.reset() this.form.status = \"add\" this.photoList = [] this.disabledInput = false this.title = \"人员证件档案新增\" this.open = true }, /** 修改按钮操作 */ handleUpdate (row) { this.reset() this.form = JSON.parse(JSON.stringify(row)) //把查出来的图片地址列表赋给anlage let anlage = row.anlageList this.form.status = \"change\" if (anlage !== null) { for (let i = 0; i &lt; anlage.length; i++) { let obj = new Object() obj.url = anlage[i] //photoList是绑定在el-upload里的fileList this.photoList.push(obj) } } this.disabledInput = true this.title = \"人员证件档案修改\" this.open = true }, 查询列表结果并修改路径值： this.$request .fetchSearchInterCertificateList(datas) .then(res =&gt; { let dataList = res.data.data.list dataList.forEach(obj =&gt; { console.log(obj.employeeName) let newAnlageList = [] if (obj.anlageList !== null) { obj.anlageList.forEach(obj2 =&gt; { //直接用obj2=that.serverIp+obj2不行，所以用newAnlageList.push方式 //如果不在查询结果这修改值的话也可以在v-for里动态改(列表里展示图片) obj2 = that.serverIp + obj2 newAnlageList.push(obj2) }) obj.anlageList = newAnlageList } }) this.interCertificateList = dataList this.pagination.total = res.data.data.total }) .catch(err =&gt; { console.log(err) }) 2.table里展示图片 &lt;el-table-column prop=\"anlageList\" label=\"附件\" width=\"150px\" align=\"center\" :show-overflow-tooltip=\"true\"&gt; &lt;template slot-scope=\"scope\"&gt; &lt;span v-for=\"(item,index) in scope.row.anlageList\" :key=\"index\"&gt; &lt;el-image style=\"width: 50px; height: 30px\" :src=\"item\" :preview-src-list=\"scope.row.anlageList\"&gt; &lt;/el-image&gt; &lt;/span&gt; &lt;/template&gt; &lt;/el-table-column&gt; :src=”item”动态加路径前缀也可以采用下面这种方法 14.vue Rules验证异常问题 人员已经输入了还提示员工不能为空 &lt;el-form-item label=\"人员\" prop=\"employeeName\"&gt; &lt;el-input v-model=\"form.employeeName\" placeholder=\"请输入内容\" suffix-icon=\"el-icon-search\" :disabled=\"disabledInput\" size=\"small\" @click.native=\"showEmployee\"&gt; &lt;/el-input&gt; &lt;/el-form-item&gt; // 表单校验 rules: { employeeName: [ { required: true, message: \"员工不能为空\", trigger: \"change\" } ] }, 问题原因： 这里定义的时候起初没有把employeeName加进去,把form表单里属性都定义一下就好了 小技巧，新建对象的时候直接let obj = new Object(),再用数组push进去，要不然会报对象属性错误的异常 15.js里replace方法替换所有问题： let a = \"aabbaacc\" let b = \"aa\" a = a.replace(b,\"dd\") //最后输入:ddbbaacc 正常replace并不会全文替换 要全文替换可以采用下面方式： string.replace(new RegExp(key,'g'),\"b\"); 可以参考：https://www.cnblogs.com/wenqiangit/p/10485245.html 16.强口领密码校验password: [ {required: true, validator: validatePass, trigger: \"blur\"}, { pattern: /^(?=.*\\d)(?=.*[,.?/\"&lt;&gt;!@#$%^&amp;*()\\-_=+\\\\|\\[\\]{};:'~`\\x22])(?=.*[a-z])(?=.*[A-Z]).{8,20}$/, message: '密码必须包含数字，小写字母，大写字母，特殊符号，长度为 8 - 20位' } ], let pwdRule = /^(?=.*\\d)(?=.*[,.?/\"&lt;&gt;!@#$%^&amp;*()\\-_=+\\\\|\\[\\]{};:'~`\\x22])(?=.*[a-z])(?=.*[A-Z]).{8,20}$/ if (!pwdRule.test(that.loginForm.password)) { this.$alert(\"用户口令密码弱,请修改密码后重新登录\", \"提示\", { confirmButtonText: \"确定\", callback: action =&gt; { this.dialogPassVisible = true } }); return }","categories":[{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/tags/vue/"},{"name":"前端","slug":"前端","permalink":"https://13592491893.github.io/tags/%E5%89%8D%E7%AB%AF/"},{"name":"工作","slug":"工作","permalink":"https://13592491893.github.io/tags/%E5%B7%A5%E4%BD%9C/"}]},{"title":"vue使用rules对表单字段进行校验","slug":"vue使用rules对表单字段进行校验","date":"2020-10-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/b0401a8b.html","link":"","permalink":"https://13592491893.github.io/posts/b0401a8b.html","excerpt":"","text":"vue使用rules对表单字段进行校验在实际开发过程中，发现以下的写法比较累赘，因为在后面的项目中，继续优化表单验证的方法，让代码更简洁。主要的修改是验证方法的修改和调用 以前验证表单字段的最大长度，description: [{ max: 200, message: ‘最大长度为200个字符’, trigger: ‘blur’ }] 现在写法：description: [validateLen(200)] 其中，validateLen是我封装的一个方法。 使用步骤： 1、在uitls文件夹中新建一个validate.js文件。 2、定义需要验证的方法 3、在需要使用的页面中引入import { validateLen, validateChart_ } from ‘@/utils/validate’ 4、在用于校验的rules中使用。例如：description: [validateLen(200)] 具体写法：参考如下。只是把旧的写法统一封装成了return 进行输出。少写很多代码，省事整洁 // 是否必填 export function validateNecessary(tipInfo = '请输入', trigger = 'blur', isNeces = true,) { return { required: isNeces, message: tipInfo, trigger } } // 验证最大长度 export function validateLen(len = 20) { return { max: len, message: '最大长度为' + len + '个字符', trigger: 'blur' } } // —————————–第一版———————- 基于element-ui 1、在代码中，添加属性：:rule &lt;el-form :model=\"form\" :rules=\"rules\" ref=\"form\" label-width=\"150px\"&gt;&lt;/el-form&gt; 并且，在&lt;el-form-item&gt;中添加prop属性，对应rules中的规则 2、新开一个文件夹（validate.js）定义验证规则 3、在页面(index.vue)中引入验证规则定义的文件，并在export default中定义rule规则，使用语法：{validator:验证方法,trigger:验证触发} 4、以下是validator.js文件的部分验证方法 /*是否合法IP地址*/ export function validateIP(rule, value,callback) { if(value==''||value==undefined||value==null){ callback(); }else { const reg = /^(\\d{1,2}|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d{1,2}|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d{1,2}|1\\d\\d|2[0-4]\\d|25[0-5])\\.(\\d{1,2}|1\\d\\d|2[0-4]\\d|25[0-5])$/; if ((!reg.test(value)) &amp;&amp; value != '') { callback(new Error('请输入正确的IP地址')); } else { callback(); } } } /* 是否手机号码或者固话*/ export function validatePhoneTwo(rule, value, callback) { const reg = /^((0\\d{2,3}-\\d{7,8})|(1[34578]\\d{9}))$/;; if (value == '' || value == undefined || value == null) { callback(); } else { if ((!reg.test(value)) &amp;&amp; value != '') { callback(new Error('请输入正确的电话号码或者固话号码')); } else { callback(); } } } /* 是否固话*/ export function validateTelphone(rule, value,callback) { const reg =/0\\d{2}-\\d{7,8}/; if(value==''||value==undefined||value==null){ callback(); }else { if ((!reg.test(value)) &amp;&amp; value != '') { callback(new Error('请输入正确的固话（格式：区号+号码,如010-1234567）')); } else { callback(); } } } /* 是否手机号码*/ export function validatePhone(rule, value,callback) { const reg =/^[1][3,4,5,7,8][0-9]{9}$/; if(value==''||value==undefined||value==null){ callback(); }else { if ((!reg.test(value)) &amp;&amp; value != '') { callback(new Error('请输入正确的电话号码')); } else { callback(); } } } /* 是否身份证号码*/ export function validateIdNo(rule, value,callback) { const reg = /(^\\d{15}$)|(^\\d{18}$)|(^\\d{17}(\\d|X|x)$)/; if(value==''||value==undefined||value==null){ callback(); }else { if ((!reg.test(value)) &amp;&amp; value != '') { callback(new Error('请输入正确的身份证号码')); } else { callback(); } } } /* 是否邮箱*/ export function validateEMail(rule, value,callback) { const reg =/^([a-zA-Z0-9]+[-_\\.]?)+@[a-zA-Z0-9]+\\.[a-z]+$/; if(value==''||value==undefined||value==null){ callback(); }else{ if (!reg.test(value)){ callback(new Error('请输入正确的邮箱地址')); } else { callback(); } } } /* 合法uri*/ export function validateURL(textval) { const urlregex = /^(https?|ftp):\\/\\/([a-zA-Z0-9.-]+(:[a-zA-Z0-9.&amp;%$-]+)*@)*((25[0-5]|2[0-4][0-9]|1[0-9]{2}|[1-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|1[0-9]{2}|[1-9]?[0-9])){3}|([a-zA-Z0-9-]+\\.)*[a-zA-Z0-9-]+\\.(com|edu|gov|int|mil|net|org|biz|arpa|info|name|pro|aero|coop|museum|[a-zA-Z]{2}))(:[0-9]+)*(\\/($|[a-zA-Z0-9.,?'\\\\+&amp;%$#=~_-]+))*$/; return urlregex.test(textval); } /*验证内容是否英文数字以及下划线*/ export function isPassword(rule, value, callback) { const reg =/^[_a-zA-Z0-9]+$/; if(value==''||value==undefined||value==null){ callback(); } else { if (!reg.test(value)){ callback(new Error('密码仅由英文字母，数字以及下划线组成')); } else { callback(); } } } /*自动检验数值的范围*/ export function checkMax20000(rule, value, callback) { if (value == '' || value == undefined || value == null) { callback(); } else if (!Number(value)) { callback(new Error('请输入[1,20000]之间的数字')); } else if (value &lt; 1 || value &gt; 20000) { callback(new Error('请输入[1,20000]之间的数字')); } else { callback(); } } //验证数字输入框最大数值,32767 export function checkMaxVal(rule, value,callback) { if (value &lt; 0 || value &gt; 32767) { callback(new Error('请输入[0,32767]之间的数字')); } else { callback(); } } //验证是否1-99之间 export function isOneToNinetyNine(rule, value, callback) { if (!value) { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入正整数')); } else { const re = /^[1-9][0-9]{0,1}$/; const rsCheck = re.test(value); if (!rsCheck) { callback(new Error('请输入正整数，值为【1,99】')); } else { callback(); } } }, 0); } // 验证是否整数 export function isInteger(rule, value, callback) { if (!value) { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入正整数')); } else { const re = /^[0-9]*[1-9][0-9]*$/; const rsCheck = re.test(value); if (!rsCheck) { callback(new Error('请输入正整数')); } else { callback(); } } }, 0); } // 验证是否整数,非必填 export function isIntegerNotMust(rule, value, callback) { if (!value) { callback(); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入正整数')); } else { const re = /^[0-9]*[1-9][0-9]*$/; const rsCheck = re.test(value); if (!rsCheck) { callback(new Error('请输入正整数')); } else { callback(); } } }, 1000); } // 验证是否是[0-1]的小数 export function isDecimal(rule, value, callback) { if (!value) { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入[0,1]之间的数字')); } else { if (value &lt; 0 || value &gt; 1) { callback(new Error('请输入[0,1]之间的数字')); } else { callback(); } } }, 100); } // 验证是否是[1-10]的小数,即不可以等于0 export function isBtnOneToTen(rule, value, callback) { if (typeof value == 'undefined') { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入正整数，值为[1,10]')); } else { if (!(value == '1' || value == '2' || value == '3' || value == '4' || value == '5' || value == '6' || value == '7' || value == '8' || value == '9' || value == '10')) { callback(new Error('请输入正整数，值为[1,10]')); } else { callback(); } } }, 100); } // 验证是否是[1-100]的小数,即不可以等于0 export function isBtnOneToHundred(rule, value, callback) { if (!value) { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入整数，值为[1,100]')); } else { if (value &lt; 1 || value &gt; 100) { callback(new Error('请输入整数，值为[1,100]')); } else { callback(); } } }, 100); } // 验证是否是[0-100]的小数 export function isBtnZeroToHundred(rule, value, callback) { if (!value) { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (!Number(value)) { callback(new Error('请输入[1,100]之间的数字')); } else { if (value &lt; 0 || value &gt; 100) { callback(new Error('请输入[1,100]之间的数字')); } else { callback(); } } }, 100); } // 验证端口是否在[0,65535]之间 export function isPort(rule, value, callback) { if (!value) { return callback(new Error('输入不可以为空')); } setTimeout(() =&gt; { if (value == '' || typeof(value) == undefined) { callback(new Error('请输入端口值')); } else { const re = /^([0-9]|[1-9]\\d|[1-9]\\d{2}|[1-9]\\d{3}|[1-5]\\d{4}|6[0-4]\\d{3}|65[0-4]\\d{2}|655[0-2]\\d|6553[0-5])$/; const rsCheck = re.test(value); if (!rsCheck) { callback(new Error('请输入在[0-65535]之间的端口值')); } else { callback(); } } }, 100); } // 验证端口是否在[0,65535]之间，非必填,isMust表示是否必填 export function isCheckPort(rule, value, callback) { if (!value) { callback(); } setTimeout(() =&gt; { if (value == '' || typeof(value) == undefined) { //callback(new Error('请输入端口值')); } else { const re = /^([0-9]|[1-9]\\d|[1-9]\\d{2}|[1-9]\\d{3}|[1-5]\\d{4}|6[0-4]\\d{3}|65[0-4]\\d{2}|655[0-2]\\d|6553[0-5])$/; const rsCheck = re.test(value); if (!rsCheck) { callback(new Error('请输入在[0-65535]之间的端口值')); } else { callback(); } } }, 100); } /* 小写字母*/ export function validateLowerCase(str) { const reg = /^[a-z]+$/; return reg.test(str); } /*保留2为小数*/ export function validatetoFixedNew(str) { return str ; } /* 验证key*/ // export function validateKey(str) { // var reg = /^[a-z_\\-:]+$/; // return reg.test(str); // } /* 大写字母*/ export function validateUpperCase(str) { const reg = /^[A-Z]+$/; return reg.test(str); } /* 大小写字母*/ export function validatAlphabets(str) { const reg = /^[A-Za-z]+$/; return reg.test(str); }","categories":[{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/categories/vue/"}],"tags":[{"name":"校验","slug":"校验","permalink":"https://13592491893.github.io/tags/%E6%A0%A1%E9%AA%8C/"},{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/tags/vue/"},{"name":"rules","slug":"rules","permalink":"https://13592491893.github.io/tags/rules/"}]},{"title":"vue开发-关于文件下载跨域的解决方法","slug":"vue开发----关于文件下载跨域的解决方法","date":"2020-10-25T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/a211f90a.html","link":"","permalink":"https://13592491893.github.io/posts/a211f90a.html","excerpt":"","text":"vue开发—-关于文件下载跨域的解决方法一、情况说明：项目将文件储存在第三方服务器（阿里云）上，在下载文件的时候，需要跨域，将处理过程记录如下。 回归到自己工作中：设备资料定义有pdf文档和word文档要下载，文档是保存在外网服务器上，如果没有跨域的话可以使用标签下载 downloadImg (informationLink, versionType, name) { let loadingInstance = Loading.service({ fullscreen: true,text:'下载中，请稍后！' }); let url = informationLink; // data:项目中获取的数据，包含文件url以及文件名等相关参数 let rightStr = informationLink.substring(informationLink.length - 3) let fileName = versionType + \"-\" + name; let xhr = new XMLHttpRequest(); xhr.open('GET', url, true); xhr.responseType = 'blob'; xhr.onload = (e) =&gt; { let url2 = window.URL.createObjectURL(xhr.response) let a = document.createElement('a'); a.href = url2 if(rightStr === 'zip'){ a.download = fileName + \".zip\"; }else { a.download = fileName + \".apk\"; } a.click() loadingInstance.close(); // const res = e.target.response; // this.saveAs(res, fileName); }; xhr.send(); }, 二、第一次尝试(还不是跨域问题)刚开始的时候，因为能获取到文件的URL地址，所以尝试以标签的形式下载，代码如下： // 下载按钮点击事件 fileDownload (url, fileName) { let ele = document.createElement('a'); ele.download = fileName; ele.href = url; ele.style.display = 'none'; document.body.appendChild(ele); ele.click(); document.body.removeChild(ele); }; 结果：虽然配置了download属性，但是因为url指向第三方资源，download会失效，表现和不使用download时一致——浏览器能打开的文件，浏览器会直接打开（可以手动下载）；不能打开的文件，会直接下载。 三、第二次尝试：（开始了）既然download属性无效，干脆通过xhr请求获取文件，然后下载到本地，代码如下： // 下载按钮点击事件 fileDownload() { let url = this.data.url; // data:项目中获取的数据，包含文件url以及文件名等相关参数 let fileName = this.data.file_name; let xhr = new XMLHttpRequest(); xhr.open('GET', url, true); xhr.responseType = 'blob'; xhr.onload = (e) =&gt; { const res = e.target.response; this.saveAs(res, fileName); }; xhr.send(); } // 导出文件函数 saveAs (obj, fileName) { let ele = document.createElement('a'); ele.download = fileName || '下载'; ele.href = URL.createObjectURL(obj); // 绑定a标签 ele.style.display = 'none'; document.body.appendChild(ele); // 兼容火狐浏览器 ele.click(); setTimeout(function () { // 延时释放 URL.revokeObjectURL(obj); // 用URL.revokeObjectURL()来释放这个object URL document.body.removeChild(ele);// 兼容火狐浏览器 }, 100); }; 结果：文件可以直接下载，but为虾米有的文件可以下载、有的文件告诉我跨域问题？？？刷新页面之后，刚刚还可以下载的文件又不可以下载了！！！崩溃ing…联系运维的老哥尝试修改一下第三方服务器的参数，发现没有卵用，只能再尝试一下了。 四、第三次尝试:(jsonp)既然是跨域的问题，网上的解决方案蛮多的，而且作为前端面试必考题，我背的也蛮熟的。首先使用jsonp来解决，步骤如下： 1.安装jsonp插件 npm install jsonp --save 2.在代码中使用jsonp import jsonp from 'jsonp'; // 导入插件 // 下载按钮点击事件 fileDownload () { let url = this.data.url; // data:项目中获取的数据，包含文件url以及文件名等相关参数 let fileName = this.data.file_name; // 先测试一下能不能跨域成功 jsonp(url, null, (err, data) =&gt; { if (err) { console.error(err.message); } else { console.log(data); } }) } 结果：可能是哪里使用不对，反正是没有请求数据成功，还是显示跨域问题，既然不行，那就再换方法。 3.卸载jsonp插件 npm uninstall jsonp 五、第四次尝试：（fetch跨域）废话不多说，直接上代码： // 下载按钮点击事件 fileDownload () { let url = this.data.url; // data:项目中获取的数据，包含文件url以及文件名等相关参数 let fileName = this.data.file_name; // 先测试一下能不能跨域成功 let myHeaders = new Headers({ 'Access-Control-Allow-Origin': '*', 'Content-Type': 'text/plain' }); fetch(url, { method:'GET', headers:myHeaders, mode:'cors' }).then(res=&gt;{ console.log(res); }); } 结果：依然没有解决跨域的问题，革命尚未成功，老子还得努力啊。（ps: fetch的mode属性设置为’no-cors‘的时候能请求成功，但是返回值无法使用，木的办法） 六、第五次尝试：（使用插件downloadjs下载文件）1.安装downloadjs插件 npm install downloadjs --save 2.使用downloadjs插件 import download from 'downloadjs'; // 引用插件 // 下载按钮点击事件 fileDownload () { let url = this.data.url; // data:项目中获取的数据，包含文件url以及文件名等相关参数 download(url); // 没看错，就是这么简单 } 结果：我在自己的电脑上发现完全可以下载文件，沾沾自喜了大概十分钟，通过别人的电脑访问我的IP测试，发现有的完全没问题，有的还是出现跨域的问题。。。。。。。。。。。要疯了有木有。。。。。。。。接着改吧。。。。。。。 3.卸载downloadjs插件 npm uninstall downloadjs 七、第六次尝试：（前端已经尽力了，让后端大佬帮忙吧）在使用了好多方法之后，在历时一整天的尝试之后，我，决定放下前端的骄傲，找后端大佬（php）商量一下，决定后端先将第三方服务器上的文件转换成二进制文件，然后通过一个接口返回给前端处理，上代码： // 下载按钮点击事件 async fileDownload () { let url = this.data.url; // data:项目中获取的数据，包含文件url以及文件名等相关参数 let fileName = this.data.file_name; const res = await getFile({ // 获取文件二进制数据的接口 oss_url: url }); this.saveAs(new Blob([res], { type: 'text/plain;charset=UTF-8' }), fileName); } // 导出文件函数 saveAs (obj, fileName) { let ele = document.createElement('a'); ele.download = fileName || '下载'; ele.href = URL.createObjectURL(obj); // 绑定a标签 ele.style.display = 'none'; document.body.appendChild(ele); // 兼容火狐浏览器 ele.click(); setTimeout(function () { // 延时释放 URL.revokeObjectURL(obj); // 用URL.revokeObjectURL()来释放这个object URL document.body.removeChild(ele);// 兼容火狐浏览器 }, 100); }; PS: (1).将二进制流转为Blob类型的时候，属性：{type: ‘text/plain;charset=UTF-8’}； (2).获取二进制文件的接口，我是使用项目里封装的axios方法，需注意设置属性：responseType: 'blob'。 结果：究极妥协之后终于看到了黎明的曙光，可以正常下载所需类型的文件了，大功告成！！！ 八、vue+java代码：vue： let loadingInstance = Loading.service({ fullscreen: true,text:'下载中，请稍后！' }); let datas = { informationName: informationName, informationLink: informationLink, }; this.$request .fetchGetFileOutputStream(datas) .then((res) =&gt; { loadingInstance.close(); const link = document.createElement(\"a\"); let blob = new Blob([res.data]); link.style.display = \"none\"; link.href = URL.createObjectURL(blob); link.download = informationName; //下载的文件名 // document.body.appendChild(link); link.click(); // document.body.removeChild(link); // self.dataToBase(informationName,data) }) .catch((error) =&gt; { loadingInstance.close(); self.$message({ message: '网络连接错误', type: 'warning' }); self.$Notice.error({ title: \"错误\", desc: \"网络连接错误\", }); console.log(error); }); java: @GetMapping(\"/getFileOutputStream\") public void getFileOutputStream(FixCardInformationEntity entity, HttpServletResponse response) { try { fixCardInformationService.getFileOutputStream(entity, response); } catch (Exception e) { e.printStackTrace(); logger.error(e.getMessage()); } } @Override public void getFileOutputStream(FixCardInformationEntity entity, HttpServletResponse response) throws Exception { response.setContentType(\"application/vnd.ms-excel\"); response.setCharacterEncoding(\"UTF-8\"); String fileName = URLEncoder.encode(\"未命名文件\", \"UTF-8\").replaceAll(\"\\\\+\", \"%20\"); if (!StringUtils.isEmpty(entity.getInformationName())) { fileName = URLEncoder.encode(entity.getInformationName(), \"UTF-8\").replaceAll(\"\\\\+\", \"%20\"); } response.setHeader(\"Content-disposition\", \"attachment;filename=\" + fileName); URL url = null; InputStream is = null; HttpURLConnection httpUrl = null; try { url = new URL(entity.getInformationLink()); httpUrl = (HttpURLConnection) url.openConnection(); httpUrl.connect(); is = httpUrl.getInputStream(); //创建一个Buffer字符串 byte[] buffer = new byte[1024]; //每次读取的字符串长度，如果为-1，代表全部读取完毕 int len = 0; //使用一个输入流从buffer里把数据读取出来 while ((len = is.read(buffer)) != -1) { //用输出流往buffer里写入数据，中间参数代表从哪个位置开始读，len代表读取的长度 response.getOutputStream().write(buffer, 0, len); } } catch (Exception e) { e.printStackTrace(); logger.info(e.getMessage()); } finally { if (is != null) { try { is.close(); } catch (IOException e) { e.printStackTrace(); } } if (httpUrl != null) { httpUrl.disconnect(); } } } 九、Java中HttpURLConnection使用详解、总结https://www.cnblogs.com/tfxz/p/12621611.html#GET_22","categories":[{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/categories/vue/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/tags/vue/"},{"name":"前端","slug":"前端","permalink":"https://13592491893.github.io/tags/%E5%89%8D%E7%AB%AF/"},{"name":"跨域","slug":"跨域","permalink":"https://13592491893.github.io/tags/%E8%B7%A8%E5%9F%9F/"}]},{"title":"ztree的模糊搜索过滤功能","slug":"ztree的模糊搜索过滤功能","date":"2020-10-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/5a57df06.html","link":"","permalink":"https://13592491893.github.io/posts/5a57df06.html","excerpt":"","text":"ztree的模糊搜索过滤功能1.问题加一个查询框可以模糊过滤 2.实现代码//搜索框 &lt;el-input v-model=\"itemTypeName\" placeholder=\"请输入设备类别名称\" size=\"mini\" clearable @keyup.enter.native=\"onSearch\"/&gt; &lt;el-scrollbar style=\"height: 35vh\"&gt; &lt;tree :nodes=\"itemTypeTreeData\" node-key=\"itemTypeId\" :setting=\"settingFixType\" @onClick=\"fixTreeClick\" @onCreated=\"handleCreated\"/&gt; &lt;/el-scrollbar&gt; handleCreated: function (ztreeObj) { this.ztreeObj = ztreeObj this.zTree = ztreeObj this.allNodes = ztreeObj.transformToArray(ztreeObj.getNodes()) ztreeObj.expandNode(ztreeObj.getNodes()[0], true, false, true) }, onSearch () { let that = this let zTreeObj = that.zTree if (that.itemTypeName === \"\") { //注意！！！showNodes时that.allNodes是getNodes转数组之后的，但是expandNode是直接用的getNodes()[0] zTreeObj.showNodes(that.allNodes) zTreeObj.expandAll(false) zTreeObj.expandNode(zTreeObj.getNodes()[0], true, false, true) return } that.nodeList = zTreeObj.getNodesByParamFuzzy(\"itemTypeName\", that.itemTypeName, null) that.updateNodes(that.nodeList) }, updateNodes (nodeList) { let that = this let zTreeObj = that.zTree let allNode = zTreeObj.transformToArray(zTreeObj.getNodes()) zTreeObj.hideNodes(allNode) for (let n in nodeList) { that.findParent(zTreeObj, nodeList[n]) } zTreeObj.showNodes(nodeList) }, findParent (zTree, node) { let that = this zTree.expandNode(node, true, false, false) let pNode = node.getParentNode() if (pNode != null) { that.nodeList.push(pNode) that.findParent(zTree, pNode) } },","categories":[{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/categories/vue/"}],"tags":[{"name":"ztree","slug":"ztree","permalink":"https://13592491893.github.io/tags/ztree/"},{"name":"模糊搜索","slug":"模糊搜索","permalink":"https://13592491893.github.io/tags/%E6%A8%A1%E7%B3%8A%E6%90%9C%E7%B4%A2/"},{"name":"工作","slug":"工作","permalink":"https://13592491893.github.io/tags/%E5%B7%A5%E4%BD%9C/"}]},{"title":"狂神说redis笔记","slug":"狂神说redis笔记","date":"2020-06-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/54863.html","link":"","permalink":"https://13592491893.github.io/posts/54863.html","excerpt":"","text":"狂神说redis笔记（一）一、Nosql概述1、单机Mysql时代90年代,一个网站的访问量一般不会太大，单个数据库完全够用。随着用户增多，网站出现以下问题： 数据量增加到一定程度，单机数据库就放不下了 数据的索引（B+ Tree）,一个机器内存也存放不下 访问量变大后（读写混合），一台服务器承受不住。 2、Memcached(缓存) + Mysql + 垂直拆分（读写分离）网站80%的情况都是在读，每次都要去查询数据库的话就十分的麻烦！所以说我们希望减轻数据库的压力，我们可以使用缓存来保证效率！ 优化过程经历了以下几个过程： 优化数据库的数据结构和索引(难度大) 文件缓存，通过IO流获取比每次都访问数据库效率略高，但是流量爆炸式增长时候，IO流也承受不了 MemCache,当时最热门的技术，通过在数据库和数据库访问层之间加上一层缓存，第一次访问时查询数据库，将结果保存到缓存，后续的查询先检查缓存，若有直接拿去使用，效率显著提升。 3、分库分表 + 水平拆分 + Mysql集群 4、如今最近的年代如今信息量井喷式增长，各种各样的数据出现（用户定位数据，图片数据等），大数据的背景下关系型数据库（RDBMS）无法满足大量数据要求。Nosql数据库就能轻松解决这些问题。目前一个基本的互联网项目： 5、为什么要用NoSQL ？用户的个人信息，社交网络，地理位置。用户自己产生的数据，用户日志等等爆发式增长！这时候我们就需要使用NoSQL数据库的，Nosql可以很好的处理以上的情况！ 什么是NosqlNoSQL = Not Only SQL（不仅仅是SQL） Not Only Structured Query Language 关系型数据库：列+行，同一个表下数据的结构是一样的。 非关系型数据库：数据存储没有固定的格式，并且可以进行横向扩展。 NoSQL泛指非关系型数据库，随着web2.0互联网的诞生，传统的关系型数据库很难对付web2.0时代！尤其是超大规模的高并发的社区，暴露出来很多难以克服的问题，NoSQL在当今大数据环境下发展的十分迅速，Redis是发展最快的。 Nosql特点1.方便扩展（数据之间没有关系，很好扩展！） 2.大数据量高性能（Redis一秒可以写8万次，读11万次，NoSQL的缓存记录级，是一种细粒度的缓存，性能会比较高！） 3.数据类型是多样型的！（不需要事先设计数据库，随取随用） 4.传统的 RDBMS 和 NoSQL 传统的 RDBMS(关系型数据库) 结构化组织SQL数据和关系都存在单独的表中 row col操作，数据定义语言严格的一致性基础的事务… Nosql 不仅仅是数据没有固定的查询语言键值对存储，列存储，文档存储，图形数据库（社交关系）最终一致性CAP定理和BASE高性能，高可用，高扩展… 5.大数据时代的3V ：主要是描述问题的 海量Velume 多样Variety 实时Velocity 6.大数据时代的3高 ： 主要是对程序的要求 高并发 高可扩 高性能 真正在公司中的实践：NoSQL + RDBMS 一起使用才是最强的。 二、Redis入门Redis是什么？Redis（Remote Dictionary Server )，即远程字典服务。 是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis能干什么？ 内存存储、持久化，内存是断电即失的，所以需要持久化（RDB、AOF） 高效率、用于高速缓冲 发布订阅系统 地图信息分析 计时器、计数器(eg：浏览量) 。。。 特性 多样的数据类型 持久化 集群 事务 … 环境搭建（略）性能测试redis-benchmark：Redis官方提供的性能测试工具，参数选项如下： 简单测试： # 测试：100个并发连接 100000请求 redis-benchmark -h localhost -p 6379 -c 100 -n 100000 结果： 基础知识redis默认有16个数据库 默认使用的第0个; 16个数据库为：DB 0~DB 15 默认使用DB 0 ，可以使用select n切换到DB n，dbsize可以查看当前数据库的大小，与key数量相关。 127.0.0.1:6379&gt; config get databases # 命令行查看数据库数量databases 1) \"databases\" 2) \"16\" 127.0.0.1:6379&gt; select 8 # 切换数据库 DB 8 OK 127.0.0.1:6379[8]&gt; dbsize # 查看数据库大小 (integer) 0 不同数据库之间 数据是不能互通的，并且dbsize 是根据库中key的个数。 127.0.0.1:6379&gt; set name sakura OK 127.0.0.1:6379&gt; SELECT 8 OK 127.0.0.1:6379[8]&gt; get name # db8中并不能获取db0中的键值对。 (nil) 127.0.0.1:6379[8]&gt; DBSIZE (integer) 0 127.0.0.1:6379[8]&gt; SELECT 0 OK 127.0.0.1:6379&gt; keys * \"counter:rand_int\" \"mylist\" \"name\" \"key:rand_int\" \"myset:rand_int\" 127.0.0.1:6379&gt; DBSIZE # size和key个数相关 (integer) 5 keys * ：查看当前数据库中所有的key。 flushdb：清空当前数据库中的键值对。 flushall：清空所有数据库的键值对。 Redis是单线程的，Redis是基于内存操作的。 所以Redis的性能瓶颈不是CPU,而是机器内存和网络带宽。 那么为什么Redis的速度如此快呢，性能这么高呢？QPS达到10W+ Redis为什么单线程还这么快？ 误区1：高性能的服务器一定是多线程的？ 误区2：多线程（CPU上下文会切换！）一定比单线程效率高！ 核心：Redis是将所有的数据放在内存中的，所以说使用单线程去操作效率就是最高的，多线程（CPU上下文会切换：耗时的操作！），对于内存系统来说，如果没有上下文切换效率就是最高的，多次读写都是在一个CPU上的，在内存存储数据情况下，单线程就是最佳的方案。 三、五大数据类型 Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它支持字符串、哈希表、列表、集合、有序集合，位图，hyperloglogs等数据类型。内置复制、Lua脚本、LRU收回、事务以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动分区。 Redis-key在redis中无论什么数据类型，在数据库中都是以key-value形式保存，通过进行对Redis-key的操作，来完成对数据库中数据的操作。 下面学习的命令： exists key：判断键是否存在 del key：删除键值对 move key db：将键值对移动到指定数据库 expire key second：设置键值对的过期时间 type key：查看value的数据类型 127.0.0.1:6379&gt; keys * # 查看当前数据库所有key (empty list or set) 127.0.0.1:6379&gt; set name qinjiang # set key OK 127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; keys * 1) \"age\" 2) \"name\" 127.0.0.1:6379&gt; move age 1 # 将键值对移动到指定数据库 (integer) 1 127.0.0.1:6379&gt; EXISTS age # 判断键是否存在 (integer) 0 # 不存在 127.0.0.1:6379&gt; EXISTS name (integer) 1 # 存在 127.0.0.1:6379&gt; SELECT 1 OK 127.0.0.1:6379[1]&gt; keys * 1) \"age\" 127.0.0.1:6379[1]&gt; del age # 删除键值对 (integer) 1 # 删除个数 127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; EXPIRE age 15 # 设置键值对的过期时间 (integer) 1 # 设置成功 开始计数 127.0.0.1:6379&gt; ttl age # 查看key的过期剩余时间 (integer) 13 127.0.0.1:6379&gt; ttl age (integer) 11 127.0.0.1:6379&gt; ttl age (integer) 9 127.0.0.1:6379&gt; ttl age (integer) -2 # -2 表示key过期，-1表示key未设置过期时间 127.0.0.1:6379&gt; get age # 过期的key 会被自动delete (nil) 127.0.0.1:6379&gt; keys * \"name\" 127.0.0.1:6379&gt; type name # 查看value的数据类型 string 关于TTL命令 Redis的key，通过TTL命令返回key的过期时间，一般来说有3种： 当前key没有设置过期时间，所以会返回-1. 当前key有设置过期时间，而且key已经过期，所以会返回-2. 当前key有设置过期时间，且key还没有过期，故会返回key的正常剩余时间. 关于重命名RENAME和RENAMENX RENAME key newkey修改 key 的名称 RENAMENX key newkey仅当 newkey 不存在时，将 key 改名为 newkey 。 String(字符串)普通的set、get直接略过。 常用命令及其示例： APPEND key value: 向指定的key的value后追加字符串 127.0.0.1:6379&gt; set msg hello OK 127.0.0.1:6379&gt; append msg \" world\" (integer) 11 127.0.0.1:6379&gt; get msg “hello world” DECR/INCR key: 将指定key的value数值进行+1/-1(仅对于数字) 127.0.0.1:6379&gt; set age 20 OK 127.0.0.1:6379&gt; incr age (integer) 21 127.0.0.1:6379&gt; decr age (integer) 20 INCRBY/DECRBY key n: 按指定的步长对数值进行加减 127.0.0.1:6379&gt; INCRBY age 5 (integer) 25 127.0.0.1:6379&gt; DECRBY age 10 (integer) 15 INCRBYFLOAT key n: 为数值加上浮点型数值 127.0.0.1:6379&gt; INCRBYFLOAT age 5.2 “20.2” STRLEN key: 获取key保存值的字符串长度 127.0.0.1:6379&gt; get msg “hello world” 127.0.0.1:6379&gt; STRLEN msg (integer) 11 GETRANGE key start end: 按起止位置获取字符串（闭区间，起止位置都取） 127.0.0.1:6379&gt; get msg “hello world” 127.0.0.1:6379&gt; GETRANGE msg 3 9 “lo worl” SETRANGE key offset value:用指定的value 替换key中 offset开始的值 127.0.0.1:6379&gt; set msg hello OK 127.0.0.1:6379&gt; setrange msg 2 hello (integer) 7 127.0.0.1:6379&gt; get msg \"hehello\" 127.0.0.1:6379&gt; set msg2 world OK 127.0.0.1:6379&gt; setrange msg2 2 ww (integer) 5 127.0.0.1:6379&gt; get msg2 \"wowwd\" GETSET key value: 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 127.0.0.1:6379&gt; GETSET msg test “hello world” SETNX key value: 仅当key不存在时进行set 127.0.0.1:6379&gt; SETNX msg test (integer) 0 127.0.0.1:6379&gt; SETNX name sakura (integer) 1 SETEX key seconds value: set 键值对并设置过期时间 127.0.0.1:6379&gt; setex name 10 root OK 127.0.0.1:6379&gt; get name (nil) MSET key1 value1 [key2 value2..]: 批量set键值对 127.0.0.1:6379&gt; MSET k1 v1 k2 v2 k3 v3 OK MSETNX key1 value1 [key2 value2..]: 批量设置键值对，仅当参数中所有的key都不存在时执行 127.0.0.1:6379&gt; MSETNX k1 v1 k4 v4 (integer) 0 MGET key1 [key2..]: 批量获取多个key保存的值 127.0.0.1:6379&gt; MGET k1 k2 k3 1) “v1” 2) “v2” 3) “v3” PSETEX key milliseconds value: 和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间 String类似的使用场景：value除了是字符串还可以是数字，用途举例： 计数器 统计多单位的数量：uid:123666：follow 0 粉丝数 对象存储缓存 List(列表)Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。 首先我们列表，可以经过规则定义将其变为队列、栈、双端队列等。 正如图Redis中List是可以进行双端操作的，所以命令也就分为了LXXX和RLLL两类，有时候L也表示List例如LLEN LPUSH/RPUSH key value1[value2..]从左边/右边向列表中PUSH值(一个或者多个)。 LRANGE key start end 获取list 起止元素==（索引从左往右 递增）== LPUSHX/RPUSHX key value 向已存在的列名中push值（一个或者多个） LINSERT key BEFORE|AFTER pivot value 在指定列表元素的前/后 插入value LLEN key 查看列表长度 LINDEX key index 通过索引获取列表元素 LSET key index value 通过索引为元素设值 LPOP/RPOP key 从最左边/最右边移除值 并返回 RPOPLPUSH source destination 将列表的尾部(右)最后一个值弹出，并返回，然后加到另一个列表的头部 LTRIM key start end 通过下标截取指定范围内的列表 LREM key count value List中是允许value重复的 count &gt; 0：从头部开始搜索 然后删除指定的value 至多删除count个 count &lt; 0：从尾部开始搜索… count = 0：删除列表中所有的指定value。 BLPOP/BRPOP key1[key2] timout 移出并获取列表的第一个/最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 BRPOPLPUSH source destination timeout 和RPOPLPUSH功能相同，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 代码示例： ---------------------------LPUSH---RPUSH---LRANGE-------------------------------- 127.0.0.1:6379&gt; LPUSH mylist k1 # LPUSH mylist=&gt;{1} (integer) 1 127.0.0.1:6379&gt; LPUSH mylist k2 # LPUSH mylist=&gt;{2,1} (integer) 2 127.0.0.1:6379&gt; RPUSH mylist k3 # RPUSH mylist=&gt;{2,1,3} (integer) 3 127.0.0.1:6379&gt; get mylist # 普通的get是无法获取list值的 (error) WRONGTYPE Operation against a key holding the wrong kind of value 127.0.0.1:6379&gt; LRANGE mylist 0 4 # LRANGE 获取起止位置范围内的元素 \"k2\" \"k1\" \"k3\" 127.0.0.1:6379&gt; LRANGE mylist 0 2 \"k2\" \"k1\" \"k3\" 127.0.0.1:6379&gt; LRANGE mylist 0 1 \"k2\" \"k1\" 127.0.0.1:6379&gt; LRANGE mylist 0 -1 # 获取全部元素 \"k2\" \"k1\" \"k3\" ---------------------------LPUSHX---RPUSHX----------------------------------- 127.0.0.1:6379&gt; LPUSHX list v1 # list不存在 LPUSHX失败 (integer) 0 127.0.0.1:6379&gt; LPUSHX list v1 v2 (integer) 0 127.0.0.1:6379&gt; LPUSHX mylist k4 k5 # 向mylist中 左边 PUSH k4 k5 (integer) 5 127.0.0.1:6379&gt; LRANGE mylist 0 -1 \"k5\" \"k4\" \"k2\" \"k1\" \"k3\" ---------------------------LINSERT--LLEN--LINDEX--LSET---------------------------- 127.0.0.1:6379&gt; LINSERT mylist after k2 ins_key1 # 在k2元素后 插入ins_key1 (integer) 6 127.0.0.1:6379&gt; LRANGE mylist 0 -1 \"k5\" \"k4\" \"k2\" \"ins_key1\" \"k1\" \"k3\" 127.0.0.1:6379&gt; LLEN mylist # 查看mylist的长度 (integer) 6 127.0.0.1:6379&gt; LINDEX mylist 3 # 获取下标为3的元素 \"ins_key1\" 127.0.0.1:6379&gt; LINDEX mylist 0 \"k5\" 127.0.0.1:6379&gt; LSET mylist 3 k6 # 将下标3的元素 set值为k6 OK 127.0.0.1:6379&gt; LRANGE mylist 0 -1 \"k5\" \"k4\" \"k2\" \"k6\" \"k1\" \"k3\" ---------------------------LPOP--RPOP-------------------------- 127.0.0.1:6379&gt; LPOP mylist # 左侧(头部)弹出 \"k5\" 127.0.0.1:6379&gt; RPOP mylist # 右侧(尾部)弹出 \"k3\" ---------------------------RPOPLPUSH-------------------------- 127.0.0.1:6379&gt; LRANGE mylist 0 -1 \"k4\" \"k2\" \"k6\" \"k1\" 127.0.0.1:6379&gt; RPOPLPUSH mylist newlist # 将mylist的最后一个值(k1)弹出，加入到newlist的头部 \"k1\" 127.0.0.1:6379&gt; LRANGE newlist 0 -1 \"k1\" 127.0.0.1:6379&gt; LRANGE mylist 0 -1 \"k4\" \"k2\" \"k6\" ---------------------------LTRIM-------------------------- 127.0.0.1:6379&gt; LTRIM mylist 0 1 # 截取mylist中的 0~1部分 OK 127.0.0.1:6379&gt; LRANGE mylist 0 -1 \"k4\" \"k2\" 初始 mylist: k2,k2,k2,k2,k2,k2,k4,k2,k2,k2,k2 ---------------------------LREM-------------------------- 127.0.0.1:6379&gt; LREM mylist 3 k2 # 从头部开始搜索 至多删除3个 k2 (integer) 3 删除后：mylist: k2,k2,k2,k4,k2,k2,k2,k2 127.0.0.1:6379&gt; LREM mylist -2 k2 #从尾部开始搜索 至多删除2个 k2 (integer) 2 删除后：mylist: k2,k2,k2,k4,k2,k2 ---------------------------BLPOP--BRPOP-------------------------- mylist: k2,k2,k2,k4,k2,k2 newlist: k1 127.0.0.1:6379&gt; BLPOP newlist mylist 30 # 从newlist中弹出第一个值，mylist作为候选 \"newlist\" # 弹出 \"k1\" 127.0.0.1:6379&gt; BLPOP newlist mylist 30 \"mylist\" # 由于newlist空了 从mylist中弹出 \"k2\" 127.0.0.1:6379&gt; BLPOP newlist 30 (30.10s) # 超时了 127.0.0.1:6379&gt; BLPOP newlist 30 # 我们连接另一个客户端向newlist中push了test, 阻塞被解决。 \"newlist\" \"test\" (12.54s) 小结 list实际上是一个链表，before Node after , left, right 都可以插入值 如果key不存在，则创建新的链表 如果key存在，新增内容 如果移除了所有值，空链表，也代表不存在 在两边插入或者改动值，效率最高！修改中间元素，效率相对较低 应用： 消息排队！消息队列（Lpush Rpop）,栈（Lpush Lpop） Set(集合）Redis的Set是string类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 Redis中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。 SADD key member1[member2..] 向集合中无序增加一个/多个成员 SCARD key 获取集合的成员数 SMEMBERS key 返回集合中所有的成员 SISMEMBER key member 查询member元素是否是集合的成员,结果是无序的 SRANDMEMBER key [count] 随机返回集合中count个成员，count缺省值为1 SPOP key [count] 随机移除并返回集合中count个成员，count缺省值为1 SMOVE source destination member 将source集合的成员member移动到destination集合 SREM key member1[member2..] 移除集合中一个/多个成员 SDIFF key1[key2..] 返回所有集合的差集 key1- key2 - … SDIFFSTORE destination key1[key2..] 在SDIFF的基础上，将结果保存到集合中==(覆盖)==。不能保存到其他类型key噢！ SINTER key1 [key2..] 返回所有集合的交集 SINTERSTORE destination key1[key2..] 在SINTER的基础上，存储结果到集合中。覆盖 SUNION key1 [key2..] 返回所有集合的并集 SUNIONSTORE destination key1 [key2..] 在SUNION的基础上，存储结果到及和张。覆盖 SSCAN KEY [MATCH pattern] [COUNT count] 在大量数据环境下，使用此命令遍历集合中元素，每次遍历部分 代码示例 ---------------SADD--SCARD--SMEMBERS--SISMEMBER-------------------- 127.0.0.1:6379&gt; SADD myset m1 m2 m3 m4 # 向myset中增加成员 m1~m4 (integer) 4 127.0.0.1:6379&gt; SCARD myset # 获取集合的成员数目 (integer) 4 127.0.0.1:6379&gt; smembers myset # 获取集合中所有成员 \"m4\" \"m3\" \"m2\" \"m1\" 127.0.0.1:6379&gt; SISMEMBER myset m5 # 查询m5是否是myset的成员 (integer) 0 # 不是，返回0 127.0.0.1:6379&gt; SISMEMBER myset m2 (integer) 1 # 是，返回1 127.0.0.1:6379&gt; SISMEMBER myset m3 (integer) 1 ---------------------SRANDMEMBER--SPOP---------------------------------- 127.0.0.1:6379&gt; SRANDMEMBER myset 3 # 随机返回3个成员 \"m2\" \"m3\" \"m4\" 127.0.0.1:6379&gt; SRANDMEMBER myset # 随机返回1个成员 \"m3\" 127.0.0.1:6379&gt; SPOP myset 2 # 随机移除并返回2个成员 \"m1\" \"m4\" 将set还原到{m1,m2,m3,m4} ---------------------SMOVE--SREM---------------------------------------- 127.0.0.1:6379&gt; SMOVE myset newset m3 # 将myset中m3成员移动到newset集合 (integer) 1 127.0.0.1:6379&gt; SMEMBERS myset \"m4\" \"m2\" \"m1\" 127.0.0.1:6379&gt; SMEMBERS newset \"m3\" 127.0.0.1:6379&gt; SREM newset m3 # 从newset中移除m3元素 (integer) 1 127.0.0.1:6379&gt; SMEMBERS newset (empty list or set) 下面开始是多集合操作,多集合操作中若只有一个参数默认和自身进行运算 setx=&gt;{m1,m2,m4,m6}, sety=&gt;{m2,m5,m6}, setz=&gt;{m1,m3,m6} -----------------------------SDIFF------------------------------------ 127.0.0.1:6379&gt; SDIFF setx sety setz # 等价于setx-sety-setz \"m4\" 127.0.0.1:6379&gt; SDIFF setx sety # setx - sety \"m4\" \"m1\" 127.0.0.1:6379&gt; SDIFF sety setx # sety - setx \"m5\" -------------------------SINTER--------------------------------------- 共同关注（交集） 127.0.0.1:6379&gt; SINTER setx sety setz # 求 setx、sety、setx的交集 \"m6\" 127.0.0.1:6379&gt; SINTER setx sety # 求setx sety的交集 \"m2\" \"m6\" -------------------------SUNION--------------------------------------- 127.0.0.1:6379&gt; SUNION setx sety setz # setx sety setz的并集 \"m4\" \"m6\" \"m3\" \"m2\" \"m1\" \"m5\" 127.0.0.1:6379&gt; SUNION setx sety # setx sety 并集 \"m4\" \"m6\" \"m2\" \"m1\" \"m5\" Hash（哈希）Redis hash 是一个string类型的field和value的映射表，hash特别适合用于存储对象。 Set就是一种简化的Hash,只变动key,而value使用默认值填充。可以将一个Hash表作为一个对象进行存储，表中存放对象的信息。 HSET key field value 将哈希表 key 中的字段 field 的值设为 value 。重复设置同一个field会覆盖,返回0 HMSET key field1 value1 [field2 value2..] 同时将多个 field-value (域-值)对设置到哈希表 key 中。 HSETNX key field value 只有在字段 field 不存在时，设置哈希表字段的值。 HEXISTS key field 查看哈希表 key 中，指定的字段是否存在。 HGET key field value 获取存储在哈希表中指定字段的值 HMGET key field1 [field2..] 获取所有给定字段的值 HGETALL key 获取在哈希表key 的所有字段和值 HKEYS key 获取哈希表key中所有的字段 HLEN key 获取哈希表中字段的数量 HVALS key 获取哈希表中所有值 HDEL key field1 [field2..] 删除哈希表key中一个/多个field字段 HINCRBY key field n 为哈希表 key 中的指定字段的整数值加上增量n，并返回增量后结果 一样只适用于整数型字段 HINCRBYFLOAT key field n 为哈希表 key 中的指定字段的浮点数值加上增量 n。 HSCAN key cursor [MATCH pattern] [COUNT count] 迭代哈希表中的键值对。 代码示例 ------------------------HSET--HMSET--HSETNX---------------- 127.0.0.1:6379&gt; HSET studentx name sakura # 将studentx哈希表作为一个对象，设置name为sakura (integer) 1 127.0.0.1:6379&gt; HSET studentx name gyc # 重复设置field进行覆盖，并返回0 (integer) 0 127.0.0.1:6379&gt; HSET studentx age 20 # 设置studentx的age为20 (integer) 1 127.0.0.1:6379&gt; HMSET studentx sex 1 tel 15623667886 # 设置sex为1，tel为15623667886 OK 127.0.0.1:6379&gt; HSETNX studentx name gyc # HSETNX 设置已存在的field (integer) 0 # 失败 127.0.0.1:6379&gt; HSETNX studentx email 12345@qq.com (integer) 1 # 成功 ----------------------HEXISTS-------------------------------- 127.0.0.1:6379&gt; HEXISTS studentx name # name字段在studentx中是否存在 (integer) 1 # 存在 127.0.0.1:6379&gt; HEXISTS studentx addr (integer) 0 # 不存在 -------------------HGET--HMGET--HGETALL----------- 127.0.0.1:6379&gt; HGET studentx name # 获取studentx中name字段的value \"gyc\" 127.0.0.1:6379&gt; HMGET studentx name age tel # 获取studentx中name、age、tel字段的value \"gyc\" \"20\" \"15623667886\" 127.0.0.1:6379&gt; HGETALL studentx # 获取studentx中所有的field及其value \"name\" \"gyc\" \"age\" \"20\" \"sex\" \"1\" \"tel\" \"15623667886\" \"email\" \"12345@qq.com\" --------------------HKEYS--HLEN--HVALS-------------- 127.0.0.1:6379&gt; HKEYS studentx # 查看studentx中所有的field \"name\" \"age\" \"sex\" \"tel\" \"email\" 127.0.0.1:6379&gt; HLEN studentx # 查看studentx中的字段数量 (integer) 5 127.0.0.1:6379&gt; HVALS studentx # 查看studentx中所有的value \"gyc\" \"20\" \"1\" \"15623667886\" \"12345@qq.com\" -------------------------HDEL-------------------------- 127.0.0.1:6379&gt; HDEL studentx sex tel # 删除studentx 中的sex、tel字段 (integer) 2 127.0.0.1:6379&gt; HKEYS studentx \"name\" \"age\" \"email\" -------------HINCRBY--HINCRBYFLOAT------------------------ 127.0.0.1:6379&gt; HINCRBY studentx age 1 # studentx的age字段数值+1 (integer) 21 127.0.0.1:6379&gt; HINCRBY studentx name 1 # 非整数字型字段不可用 (error) ERR hash value is not an integer 127.0.0.1:6379&gt; HINCRBYFLOAT studentx weight 0.6 # weight字段增加0.6 \"90.8\" Hash变更的数据user name age，尤其是用户信息之类的，经常变动的信息！Hash更适合于对象的存储，Sring更加适合字符串存储！ Zset（有序集合）不同的是每个元素都会关联一个double类型的分数（score）。redis正是通过分数来为集合中的成员进行从小到大的排序。 score相同：按字典顺序排序 有序集合的成员是唯一的,但分数(score)却可以重复。 ZADD key score member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 ZCARD key 获取有序集合的成员数 ZCOUNT key min max 计算在有序集合中指定区间score的成员数 ZINCRBY key n member 有序集合中对指定成员的分数加上增量 n ZSCORE key member 返回有序集中，成员的分数值 ZRANK key member 返回有序集合中指定成员的索引 ZRANGE key start end 通过索引区间返回有序集合成指定区间内的成员 ZRANGEBYLEX key min max 通过字典区间返回有序集合的成员 ZRANGEBYSCORE key min max 通过分数返回有序集合指定区间内的成员==-inf 和 +inf分别表示最小最大值，只支持开区间()== ZLEXCOUNT key min max 在有序集合中计算指定字典区间内成员数量 ZREM key member1 [member2..] 移除有序集合中一个/多个成员 ZREMRANGEBYLEX key min max 移除有序集合中给定的字典区间的所有成员 ZREMRANGEBYRANK key start stop 移除有序集合中给定的排名区间的所有成员 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 ZREVRANGE key start end 返回有序集中指定区间内的成员，通过索引，分数从高到底 ZREVRANGEBYSCORRE key max min 返回有序集中指定分数区间内的成员，分数从高到低排序 ZREVRANGEBYLEX key max min 返回有序集中指定字典区间内的成员，按字典顺序倒序 ZREVRANK key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 ZINTERSTORE destination numkeys key1 [key2 ..] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中，numkeys：表示参与运算的集合数，将score相加作为结果的score ZUNIONSTORE destination numkeys key1 [key2..] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 ZSCAN key cursor [MATCH pattern\\] [COUNT count] 迭代有序集合中的元素（包括元素成员和元素分值） 代码示例 -------------------ZADD--ZCARD--ZCOUNT-------------- 127.0.0.1:6379&gt; ZADD myzset 1 m1 2 m2 3 m3 # 向有序集合myzset中添加成员m1 score=1 以及成员m2 score=2.. (integer) 2 127.0.0.1:6379&gt; ZCARD myzset # 获取有序集合的成员数 (integer) 2 127.0.0.1:6379&gt; ZCOUNT myzset 0 1 # 获取score在 [0,1]区间的成员数量 (integer) 1 127.0.0.1:6379&gt; ZCOUNT myzset 0 2 (integer) 2 ----------------ZINCRBY--ZSCORE-------------------------- 127.0.0.1:6379&gt; ZINCRBY myzset 5 m2 # 将成员m2的score +5 \"7\" 127.0.0.1:6379&gt; ZSCORE myzset m1 # 获取成员m1的score \"1\" 127.0.0.1:6379&gt; ZSCORE myzset m2 \"7\" --------------ZRANK--ZRANGE----------------------------------- 127.0.0.1:6379&gt; ZRANK myzset m1 # 获取成员m1的索引，索引按照score排序，score相同索引值按字典顺序顺序增加 (integer) 0 127.0.0.1:6379&gt; ZRANK myzset m2 (integer) 2 127.0.0.1:6379&gt; ZRANGE myzset 0 1 # 获取索引在 0~1的成员 \"m1\" \"m3\" 127.0.0.1:6379&gt; ZRANGE myzset 0 -1 # 获取全部成员 \"m1\" \"m3\" \"m2\" testset=&gt;{abc,add,amaze,apple,back,java,redis} score均为0 ------------------ZRANGEBYLEX--------------------------------- 127.0.0.1:6379&gt; ZRANGEBYLEX testset - + # 返回所有成员 \"abc\" \"add\" \"amaze\" \"apple\" \"back\" \"java\" \"redis\" 127.0.0.1:6379&gt; ZRANGEBYLEX testset - + LIMIT 0 3 # 分页 按索引显示查询结果的 0,1,2条记录 \"abc\" \"add\" \"amaze\" 127.0.0.1:6379&gt; ZRANGEBYLEX testset - + LIMIT 3 3 # 显示 3,4,5条记录 \"apple\" \"back\" \"java\" 127.0.0.1:6379&gt; ZRANGEBYLEX testset (- [apple # 显示 (-,apple] 区间内的成员 \"abc\" \"add\" \"amaze\" \"apple\" 127.0.0.1:6379&gt; ZRANGEBYLEX testset [apple [java # 显示 [apple,java]字典区间的成员 \"apple\" \"back\" \"java\" -----------------------ZRANGEBYSCORE--------------------- 127.0.0.1:6379&gt; ZRANGEBYSCORE myzset 1 10 # 返回score在 [1,10]之间的的成员 \"m1\" \"m3\" \"m2\" 127.0.0.1:6379&gt; ZRANGEBYSCORE myzset 1 5 \"m1\" \"m3\" --------------------ZLEXCOUNT----------------------------- 127.0.0.1:6379&gt; ZLEXCOUNT testset - + (integer) 7 127.0.0.1:6379&gt; ZLEXCOUNT testset [apple [java (integer) 3 ------------------ZREM--ZREMRANGEBYLEX--ZREMRANGBYRANK--ZREMRANGEBYSCORE-------------------------------- 127.0.0.1:6379&gt; ZREM testset abc # 移除成员abc (integer) 1 127.0.0.1:6379&gt; ZREMRANGEBYLEX testset [apple [java # 移除字典区间[apple,java]中的所有成员 (integer) 3 127.0.0.1:6379&gt; ZREMRANGEBYRANK testset 0 1 # 移除排名0~1的所有成员 (integer) 2 127.0.0.1:6379&gt; ZREMRANGEBYSCORE myzset 0 3 # 移除score在 [0,3]的成员 (integer) 2 testset=&gt; {abc,add,apple,amaze,back,java,redis} score均为0 myzset=&gt; {(m1,1),(m2,2),(m3,3),(m4,4),(m7,7),(m9,9)} ----------------ZREVRANGE--ZREVRANGEBYSCORE--ZREVRANGEBYLEX----------- 127.0.0.1:6379&gt; ZREVRANGE myzset 0 3 # 按score递减排序，然后按索引，返回结果的 0~3 \"m9\" \"m7\" \"m4\" \"m3\" 127.0.0.1:6379&gt; ZREVRANGE myzset 2 4 # 返回排序结果的 索引的2~4 \"m4\" \"m3\" \"m2\" 127.0.0.1:6379&gt; ZREVRANGEBYSCORE myzset 6 2 # 按score递减顺序 返回集合中分数在[2,6]之间的成员 \"m4\" \"m3\" \"m2\" 127.0.0.1:6379&gt; ZREVRANGEBYLEX testset [java (add # 按字典倒序 返回集合中(add,java]字典区间的成员 \"java\" \"back\" \"apple\" \"amaze\" -------------------------ZREVRANK------------------------------ 127.0.0.1:6379&gt; ZREVRANK myzset m7 # 按score递减顺序，返回成员m7索引 (integer) 1 127.0.0.1:6379&gt; ZREVRANK myzset m2 (integer) 4 mathscore=&gt;{(xm,90),(xh,95),(xg,87)} 小明、小红、小刚的数学成绩 enscore=&gt;{(xm,70),(xh,93),(xg,90)} 小明、小红、小刚的英语成绩 -------------------ZINTERSTORE--ZUNIONSTORE----------------------------------- 127.0.0.1:6379&gt; ZINTERSTORE sumscore 2 mathscore enscore # 将mathscore enscore进行合并 结果存放到sumscore (integer) 3 127.0.0.1:6379&gt; ZRANGE sumscore 0 -1 withscores # 合并后的score是之前集合中所有score的和 \"xm\" \"160\" \"xg\" \"177\" \"xh\" \"188\" 127.0.0.1:6379&gt; ZUNIONSTORE lowestscore 2 mathscore enscore AGGREGATE MIN # 取两个集合的成员score最小值作为结果的 (integer) 3 127.0.0.1:6379&gt; ZRANGE lowestscore 0 -1 withscores \"xm\" \"70\" \"xg\" \"87\" \"xh\" \"93\" 应用案例： set排序 存储班级成绩表 工资表排序！ 普通消息，1.重要消息 2.带权重进行判断 排行榜应用实现，取Top N测试 四、三种特殊数据类型Geospatial(地理位置)使用经纬度定位地理坐标并用一个有序集合zset保存，所以zset命令也可以使用 geoadd key longitud(经度) latitude(纬度) member [..] 将具体经纬度的坐标存入一个有序集合 geopos key member [member..] 获取集合中的一个/多个成员坐标 geodist key member1 member2 [unit] 返回两个给定位置之间的距离。默认以米作为单位。 georadius key longitude latitude radius m|km|mi|ft [WITHCOORD][WITHDIST] [WITHHASH] [COUNT count] 以给定的经纬度为中心， 返回集合包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素。 GEORADIUSBYMEMBER key member radius... 功能与GEORADIUS相同，只是中心位置不是具体的经纬度，而是使用结合中已有的成员作为中心点。 geohash key member1 [member2..] 返回一个或多个位置元素的Geohash表示。使用Geohash位置52点整数编码。 有效经纬度 有效的经度从-180度到180度。 有效的纬度从-85.05112878度到85.05112878度。 指定单位的参数 unit 必须是以下单位的其中一个： m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 关于GEORADIUS的参数通过georadius就可以完成 附近的人功能 withcoord:带上坐标 withdist:带上距离，单位与半径单位相同 COUNT n : 只显示前n个(按距离递增排序) ----------------georadius--------------------- 127.0.0.1:6379&gt; GEORADIUS china:city 120 30 500 km withcoord withdist # 查询经纬度(120,30)坐标500km半径内的成员 1) 1) \"hangzhou\" 2) \"29.4151\" 3) 1) \"120.20000249147415\" 2) \"30.199999888333501\" 2) 1) \"shanghai\" 2) \"205.3611\" 3) 1) \"121.40000134706497\" 2) \"31.400000253193539\" ------------geohash--------------------------- 127.0.0.1:6379&gt; geohash china:city yichang shanghai # 获取成员经纬坐标的geohash表示 \"wmrjwbr5250\" \"wtw6ds0y300\" Hyperloglog(基数统计)Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。 因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 其底层使用string数据类型。 什么是基数？数据集中不重复的元素的个数。 应用场景：网页的访问量（UV）：一个用户多次访问，也只能算作一个人。 传统实现，存储用户的id,然后每次进行比较。当用户变多之后这种方式及其浪费空间，而我们的目的只是计数，Hyperloglog就能帮助我们利用最小的空间完成。 PFADD key element1 [elememt2..] 添加指定元素到 HyperLogLog中 PFCOUNT key [key] 返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey..] 将多个 HyperLogLog 合并为一个 HyperLogLog 代码示例 ----------PFADD--PFCOUNT--------------------- 127.0.0.1:6379&gt; PFADD myelemx a b c d e f g h i j k # 添加元素 (integer) 1 127.0.0.1:6379&gt; type myelemx # hyperloglog底层使用String string 127.0.0.1:6379&gt; PFCOUNT myelemx # 估算myelemx的基数 (integer) 11 127.0.0.1:6379&gt; PFADD myelemy i j k z m c b v p q s (integer) 1 127.0.0.1:6379&gt; PFCOUNT myelemy (integer) 11 ----------------PFMERGE----------------------- 127.0.0.1:6379&gt; PFMERGE myelemz myelemx myelemy # 合并myelemx和myelemy 成为myelemz OK 127.0.0.1:6379&gt; PFCOUNT myelemz # 估算基数 (integer) 17 BitMaps(位图)使用位存储，信息状态只有 0 和 1 Bitmap是一串连续的2进制数字（0或1），每一位所在的位置为偏移(offset)，在bitmap上可执行AND,OR,XOR,NOT以及其它位操作。 应用场景: 签到统计、状态统计 setbit key offset value 为指定key的offset位设置值 getbit key offset 获取offset位的值 bitcount key [start end] 统计字符串被设置为1的bit数，也可以指定统计范围按字节 bitop operration destkey key[key..] 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 BITPOS key bit [start] [end] 返回字符串里面第一个被设置为1或者0的bit位。start和end只能按字节,不能按位 代码示例 ------------setbit--getbit-------------- 127.0.0.1:6379&gt; setbit sign 0 1 # 设置sign的第0位为 1 (integer) 0 127.0.0.1:6379&gt; setbit sign 2 1 # 设置sign的第2位为 1 不设置默认 是0 (integer) 0 127.0.0.1:6379&gt; setbit sign 3 1 (integer) 0 127.0.0.1:6379&gt; setbit sign 5 1 (integer) 0 127.0.0.1:6379&gt; type sign string 127.0.0.1:6379&gt; getbit sign 2 # 获取第2位的数值 (integer) 1 127.0.0.1:6379&gt; getbit sign 3 (integer) 1 127.0.0.1:6379&gt; getbit sign 4 # 未设置默认是0 (integer) 0 -----------bitcount---------------------------- 127.0.0.1:6379&gt; BITCOUNT sign # 统计sign中为1的位数 (integer) 4 五、事务Redis的单条命令是保证原子性的，但是redis事务不能保证原子性 Redis事务本质：一组命令的集合。 ----------------- 队列 set set set 执行 ------------------- 事务中每条命令都会被序列化，执行过程中按顺序执行，不允许其他命令进行干扰。 一次性 顺序性 排他性 Redis事务没有隔离级别的概念 Redis单条命令是保证原子性的，但是事务不保证原子性！ Redis事务操作过程 开启事务（multi） 命令入队 执行事务（exec） 所以事务中的命令在加入时都没有被执行，直到提交时才会开始执行(Exec)一次性完成。 正常执行127.0.0.1:6379&gt; multi # 开启事务 OK 127.0.0.1:6379&gt; set k1 v1 # 命令入队 QUEUED 127.0.0.1:6379&gt; set k2 v2 # .. QUEUED 127.0.0.1:6379&gt; get k1 QUEUED 127.0.0.1:6379&gt; set k3 v3 QUEUED 127.0.0.1:6379&gt; keys * QUEUED 127.0.0.1:6379&gt; exec # 事务执行 1) OK 2) OK 3) \"v1\" 4) OK 5) 1) \"k3\" 2) \"k2\" 3) \"k1\" 取消事务(discurd)127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set k1 v1 QUEUED 127.0.0.1:6379&gt; set k2 v2 QUEUED 127.0.0.1:6379&gt; DISCARD # 放弃事务 OK 127.0.0.1:6379&gt; EXEC (error) ERR EXEC without MULTI # 当前未开启事务 127.0.0.1:6379&gt; get k1 # 被放弃事务中命令并未执行 (nil) 事务错误 代码语法错误（编译时异常）所有的命令都不执行 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set k1 v1 QUEUED 127.0.0.1:6379&gt; set k2 v2 QUEUED 127.0.0.1:6379&gt; error k1 # 这是一条语法错误命令 (error) ERR unknown command `error`, with args beginning with: `k1`, # 会报错但是不影响后续命令入队 127.0.0.1:6379&gt; get k2 QUEUED 127.0.0.1:6379&gt; EXEC (error) EXECABORT Transaction discarded because of previous errors. # 执行报错 127.0.0.1:6379&gt; get k1 (nil) # 其他命令并没有被执行 代码逻辑错误 (运行时异常) **其他命令可以正常执行 ** &gt;&gt;&gt; 所以不保证事务原子性 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; set k1 v1 QUEUED 127.0.0.1:6379&gt; set k2 v2 QUEUED 127.0.0.1:6379&gt; INCR k1 # 这条命令逻辑错误（对字符串进行增量） QUEUED 127.0.0.1:6379&gt; get k2 QUEUED 127.0.0.1:6379&gt; exec 1) OK 2) OK 3) (error) ERR value is not an integer or out of range # 运行时报错 4) \"v2\" # 其他命令正常执行 虽然中间有一条命令报错了，但是后面的指令依旧正常执行成功了。 所以说Redis单条指令保证原子性，但是Redis事务不能保证原子性。 监控悲观锁： 很悲观，认为什么时候都会出现问题，无论做什么都会加锁 乐观锁： 很乐观，认为什么时候都不会出现问题，所以不会上锁！更新数据的时候去判断一下，在此期间是否有人修改过这个数据 获取version 更新的时候比较version 使用watch key监控指定数据，相当于乐观锁加锁。 正常执行 127.0.0.1:6379&gt; set money 100 # 设置余额:100 OK 127.0.0.1:6379&gt; set use 0 # 支出使用:0 OK 127.0.0.1:6379&gt; watch money # 监视money (上锁) OK 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; DECRBY money 20 QUEUED 127.0.0.1:6379&gt; INCRBY use 20 QUEUED 127.0.0.1:6379&gt; exec # 监视值没有被中途修改，事务正常执行 1) (integer) 80 2) (integer) 20 测试多线程修改值，使用watch可以当做redis的乐观锁操作（相当于getversion） 我们启动另外一个客户端模拟插队线程。 线程1： 127.0.0.1:6379&gt; watch money # money上锁 OK 127.0.0.1:6379&gt; multi OK 127.0.0.1:6379&gt; DECRBY money 20 QUEUED 127.0.0.1:6379&gt; INCRBY use 20 QUEUED 127.0.0.1:6379&gt; # 此时事务并没有执行 模拟线程插队，线程2： 127.0.0.1:6379&gt; INCRBY money 500 # 修改了线程一中监视的money (integer) 600 回到线程1，执行事务： 127.0.0.1:6379&gt; EXEC # 执行之前，另一个线程修改了我们的值，这个时候就会导致事务执行失败 (nil) # 没有结果，说明事务执行失败 127.0.0.1:6379&gt; get money # 线程2 修改生效 \"600\" 127.0.0.1:6379&gt; get use # 线程1事务执行失败，数值没有被修改 \"0\" 解锁获取最新值，然后再加锁进行事务。 unwatch进行解锁。 注意：每次提交执行exec后都会自动释放锁，不管是否成功 六、Jedis使用Java来操作Redis，Jedis是Redis官方推荐使用的Java连接redis的客户端。 1.导入依赖 &lt;!--导入jredis的包--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--fastjson--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.70&lt;/version&gt; &lt;/dependency&gt; 2.编码测试 连接数据库操作命令断开连接 代码示例 public class TestPing { public static void main(String[] args) { Jedis jedis = new Jedis(\"192.168.xx.xxx\", 6379); String response = jedis.ping(); System.out.println(response); // PONG } } 输出PONG 常用的APIstring、list、set、hash、zset 所有的api命令，就是我们对应的上面学习的指令，一个都没有变化！ public class TestTX { public static void main(String[] args) { Jedis jedis = new Jedis(\"127.0.0.1\", 6379); jedis.flushDB(); JSONObject jsonObject = new JSONObject(); jsonObject.put(\"hello\",\"world\"); jsonObject.put(\"name\",\"kuangshen\"); // 开启事务 Transaction multi = jedis.multi(); String result = jsonObject.toJSONString(); // jedis.watch(result) try { multi.set(\"user1\",result); multi.set(\"user2\",result); int i = 1/0 ; // 代码抛出异常事务，执行失败！ multi.exec(); // 执行事务！ } catch (Exception e) { multi.discard(); // 放弃事务 e.printStackTrace(); } finally { System.out.println(jedis.get(\"user1\")); System.out.println(jedis.get(\"user2\")); jedis.close(); // 关闭连接 } } } 七、SpringBoot整合SpringBoot 操作数据：spring-data jpa jdbc mongodb redis！ SpringData 也是和 SpringBoot 齐名的项目！ 说明： 在 SpringBoot2.x 之后，原来使用的jedis 被替换为了 lettuce? jedis : 采用的直连，多个线程操作的话，是不安全的，如果想要避免不安全的，使用 jedis pool 连接池！ 更像 BIO 模式 lettuce : 采用netty，实例可以再多个线程中进行共享，不存在线程不安全的情况！可以减少线程数据了，更像 NIO 模式 源码分析： @Bean @ConditionalOnMissingBean(name = \"redisTemplate\") // 我们可以自己定义一个redisTemplate来替换这个默认的！ public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { // 默认的 RedisTemplate 没有过多的设置，redis 对象都是需要序列化！ // 两个泛型都是 Object, Object 的类型，我们后使用需要强制转换 &lt;String, Object&gt; RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; } @Bean @ConditionalOnMissingBean // 由于 String 是redis中最常使用的类型，所以说单独提出来了一个bean！ public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } 整合测试1.导入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; springboot 2.x后 ，原来使用的 Jedis 被 lettuce 替换。 jedis：采用的直连，多个线程操作的话，是不安全的。如果要避免不安全，使用jedis pool连接池！更像BIO模式 lettuce：采用netty，实例可以在多个线程中共享，不存在线程不安全的情况！可以减少线程数据了，更像NIO模式 我们在学习SpringBoot自动配置的原理时，整合一个组件并进行配置一定会有一个自动配置类xxxAutoConfiguration,并且在spring.factories中也一定能找到这个类的完全限定名。Redis也不例外。 那么就一定还存在一个RedisProperties类 @ConditionalOnClass注解中有两个类是默认不存在的，所以Jedis是无法生效的 然后再看Lettuce： 完美生效。 现在我们回到RedisAutoConfiguratio 只有两个简单的Bean RedisTemplate StringRedisTemplate 当看到xxTemplate时可以对比RestTemplat、SqlSessionTemplate,通过使用这些Template来间接操作组件。那么这俩也不会例外。分别用于操作Redis和Redis中的String数据类型。 在RedisTemplate上也有一个条件注解，说明我们是可以对其进行定制化的 说完这些，我们需要知道如何编写配置文件然后连接Redis，就需要阅读RedisProperties 这是一些基本的配置属性。 还有一些连接池相关的配置。注意使用时一定使用Lettuce的连接池。 2.编写配置文件 # 配置redis spring.redis.host=39.99.xxx.xx spring.redis.port=6379 3.使用RedisTemplate @SpringBootTest class Redis02SpringbootApplicationTests { @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() { // redisTemplate 操作不同的数据类型，api和我们的指令是一样的 // opsForValue 操作字符串 类似String // opsForList 操作List 类似List // opsForSet // opsForHash // opsForZSet // opsForGeo // opsForHyperLog // 除了基本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务和基本的CRUD // 获取连接对象 //RedisConnection connection = redisTemplate.getConnectionFactory().getConnection(); //connection.flushDb(); //connection.flushAll(); redisTemplate.opsForValue().set(\"mykey\",\"kuangshen\"); System.out.println(redisTemplate.opsForValue().get(\"mykey\")); } } 4.测试结果 此时我们回到Redis查看数据时候，惊奇发现全是乱码，可是程序中可以正常输出。这时候就关系到存储对象的序列化问题，在网络中传输的对象也是一样需要序列化，否者就全是乱码。 RedisTemplate内部的序列化配置是这样的 默认的序列化器是采用JDK序列化器 后续我们定制RedisTemplate就可以对其进行修改。 RedisSerializer提供了多种序列化方案： 我们来编写一个自己的 RedisTemplete import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; @Configuration public class RedisConfig { // 这是我给大家写好的一个固定模板，大家在企业中，拿去就可以直接使用！ // 自己定义了一个 RedisTemplate @Bean @SuppressWarnings(\"all\") public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) { // 我们为了自己开发方便，一般直接使用 &lt;String, Object&gt; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(factory); // Json序列化配置 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); // String 的序列化 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); // key采用String的序列化方式 template.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 template.setHashKeySerializer(stringRedisSerializer); // value序列化方式采用jackson template.setValueSerializer(jackson2JsonRedisSerializer); // hash的value序列化方式采用jackson template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; } } 所有的redis操作，其实对于java开发人员来说，十分的简单，更重要是要去理解redis的思想和每一种数据结构的用处和作用场景！ 八、Redis.conf容量单位不区分大小写，G和GB有区别 可以使用 include 组合多个配置问题 网络配置 日志# 日志 # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) 生产环境 # warning (only very important / critical messages are logged) loglevel notice logfile \"\" # 日志的文件位置名 databases 16 # 数据库的数量，默认是 16 个数据库 always-show-logo yes # 是否总是显示LOGO 日志输出级别 debugverbosenoticewaring 持久化规则持久化， 在规定的时间内，执行了多少次操作，则会持久化到文件 .rdb. aof redis 是内存数据库，如果没有持久化，那么数据断电及失！ 由于Redis是基于内存的数据库，需要将数据由内存持久化到文件中 持久化方式： RDBAOF # 如果900s内，如果至少有一个1 key进行了修改，我们及进行持久化操作 save 900 1 # 如果300s内，如果至少10 key进行了修改，我们及进行持久化操作 save 300 10 # 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作 save 60 10000 # 我们之后学习持久化，会自己定义这个测试！ stop-writes-on-bgsave-error yes # 持久化如果出错，是否还需要继续工作！ rdbcompression yes # 是否压缩 rdb 文件，需要消耗一些cpu资源！ rdbchecksum yes # 保存rdb文件的时候，进行错误的检查校验！ dir ./ # rdb 文件保存的目录！ SECURITY 安全可以在这里设置redis的密码，默认是没有密码！ 127.0.0.1:6379&gt; ping PONG 127.0.0.1:6379&gt; config get requirepass # 获取redis的密码 1) \"requirepass\" 2) \"\" 127.0.0.1:6379&gt; config set requirepass \"123456\" # 设置redis的密码 OK 127.0.0.1:6379&gt; config get requirepass # 发现所有的命令都没有权限了 (error) NOAUTH Authentication required. 127.0.0.1:6379&gt; ping (error) NOAUTH Authentication required. 127.0.0.1:6379&gt; auth 123456 # 使用密码进行登录！ OK 127.0.0.1:6379&gt; config get requirepass 1) \"requirepass\" 2) \"123456\" 限制 CLIENTS（客户端连接相关）maxclients 10000 # 设置能连接上redis的最大客户端的数量 maxmemory &lt;bytes&gt; # redis 配置最大的内存容量 maxmemory-policy noeviction # 内存到达上限之后的处理策略 1、volatile-lru：只对设置了过期时间的key进行LRU（默认值） 2、allkeys-lru ： 删除lru算法的key 3、volatile-random：随机删除即将过期key 4、allkeys-random：随机删除 5、volatile-ttl ： 删除即将过期的 6、noeviction ： 永不过期，返回错误 APPEND ONLY 模式 aof配置appendonly no # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下， rdb完全够用！ appendfilename \"appendonly.aof\" # 持久化的文件的名字 # appendfsync always # 每次修改都会 sync。消耗性能 appendfsync everysec # 每秒执行一次 sync，可能会丢失这1s的数据！ # appendfsync no # 不执行 sync，这个时候操作系统自己同步数据，速度最快！ 九、Redis持久化——RDB面试和工作，持久化都是重点！Redis 是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失。所以 Redis 提供了持久化功能！ 什么是RDB（Redis DataBase） 在指定时间间隔后，将内存中的数据集快照写入数据库 ；在恢复时候，直接读取快照文件，进行数据的恢复 ；默认情况下， Redis 将数据库快照保存在名字为 dump.rdb的二进制文件中。文件名可以在配置文件中进行自定义。 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的。这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。我们默认的就是RDB，一般情况下不需要修改这个配置！ 有时候在生产环境我们会将这个文件进行备份！rdb保存的文件是dump.rdb 都是在我们的配置文件中快照中进行配置的！ 工作原理在进行 RDB 的时候，redis 的主线程是不会做 io 操作的，主线程会 fork 一个子线程来完成该操作； Redis 调用forks。同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益(因为是使用子进程进行写操作，而父进程依然可以接收来自客户端的请求。) 触发机制 save的规则满足的情况下，会自动触发rdb原则 执行flushall命令，也会触发我们的rdb原则 退出redis，也会自动产生rdb文件 save使用 save 命令，会立刻对当前内存中的数据进行持久化 ,但是会阻塞，也就是不接受其他操作了； 由于 save 命令是同步命令，会占用Redis的主进程。若Redis数据非常多时，save命令执行速度会非常慢，阻塞所有客户端的请求。 示意图 flushall命令flushall 命令也会触发持久化 ； 触发持久化规则满足配置条件中的触发条件 ； 可以通过配置文件对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动进行数据集保存操作。 bgsavebgsave 是异步进行，进行持久化的时候，redis 还可以将继续响应客户端请求 ； bgsave和save对比 命令 save bgsave IO类型 同步 异步 阻塞？ 是 是（阻塞发生在fock()，通常非常快） 复杂度 O(n) O(n) 优点 不会消耗额外的内存 不阻塞客户端命令 缺点 阻塞客户端命令 需要fock子进程，消耗内存 如果恢复rdb文件！1、只需要将rdb文件放在我们redis启动目录就可以，redis启动的时候会自动检查dump.rdb 恢复其中的数据！2、查看需要存在的位置 127.0.0.1:6379&gt; config get dir 1) \"dir\" 2) \"/usr/local/bin\" # 如果在这个目录下存在 dump.rdb 文件，启动就会自动恢复其中的数据 优缺点 优点： 适合大规模的数据恢复 对数据的完整性要求不高 缺点： 需要一定的时间间隔进行操作，如果redis意外宕机了，这个最后一次修改的数据就没有了。 fork进程的时候，会占用一定的内容空间。 十、Redis持久化——AOFAppend Only File 将我们所有的命令都记录下来，history，恢复的时候就把这个文件全部再执行一遍 以日志的形式来记录每个写的操作，将Redis执行过的所有指令记录下来（读操作不记录），只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 什么是AOF 快照功能（RDB）并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、以及未保存到快照中的那些数据。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。 appendonly no yes则表示启用AOF 默认是不开启的，我们需要手动配置，然后重启redis，就可以生效了！ 如果这个aof文件有错位，这时候redis是启动不起来的，我需要修改这个aof文件 redis给我们提供了一个工具redis-check-aof –fix appendonly yes # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分的情况下，rdb完全够用 appendfilename \"appendonly.aof\" appendfsync always # 每次修改都会sync 消耗性能 appendfsync everysec # 每秒执行一次 sync 可能会丢失这一秒的数据 appendfsync no # 不执行 sync ,这时候操作系统自己同步数据，速度最快 优缺点优点 每一次修改都会同步，文件的完整性会更加好 没秒同步一次，可能会丢失一秒的数据 从不同步，效率最高 缺点 相对于数据文件来说，aof远远大于rdb，修复速度比rdb慢！ Aof运行效率也要比rdb慢，所以我们redis默认的配置就是rdb持久化 扩展：1、RDB 持久化方式能够在指定的时间间隔内对你的数据进行快照存储2、AOF 持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，AOF命令以Redis 协议追加保存每次写的操作到文件末尾，Redis还能对AOF文件进行后台重写，使得AOF文件的体积不至于过大。3、只做缓存，如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化4、同时开启两种持久化方式 在这种情况下，当redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。 RDB 的数据不实时，同时使用两者时服务器重启也只会找AOF文件，那要不要只使用AOF呢？作者建议不要，因为RDB更适合用于备份数据库（AOF在不断变化不好备份），快速重启，而且不会有AOF可能潜在的Bug，留着作为一个万一的手段。 5、性能建议 因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留 save 900 1 这条规则。 如果Enable AOF ，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了，代价一是带来了持续的IO，二是AOF rewrite 的最后将 rewrite 过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上，默认超过原大小100%大小重写可以改到适当的数值。 如果不Enable AOF ，仅靠 Master-Slave Repllcation 实现高可用性也可以，能省掉一大笔IO，也减少了rewrite时带来的系统波动。代价是如果Master/Slave 同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个 Master/Slave 中的 RDB文件，载入较新的那个，微博就是这种架构。 如何选择使用哪种持久化方式？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。 十一、Redis发布订阅Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。微信、 微博、关注系统！ Redis 客户端可以订阅任意数量的频道。 订阅/发布消息图： 第一个：消息发送者， 第二个：频道 第三个：消息订阅者！ 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 命令 PSUBSCRIBE pattern [pattern..] 订阅一个或多个符合给定模式的频道。 PUNSUBSCRIBE pattern [pattern..] 退订一个或多个符合给定模式的频道。 PUBSUB subcommand [argument[argument]] 查看订阅与发布系统状态。 PUBLISH channel message 向指定频道发布消息 SUBSCRIBE channel [channel..] 订阅给定的一个或多个频道。 UNSUBSCRIBE channel [channel..] 退订一个或多个频道 代码示例 ------------订阅端---------------------- 127.0.0.1:6379&gt; SUBSCRIBE sakura # 订阅sakura频道 Reading messages... (press Ctrl-C to quit) # 等待接收消息 1) \"subscribe\" # 订阅成功的消息 2) \"sakura\" 3) (integer) 1 1) \"message\" # 接收到来自sakura频道的消息 \"hello world\" 2) \"sakura\" 3) \"hello world\" 1) \"message\" # 接收到来自sakura频道的消息 \"hello i am sakura\" 2) \"sakura\" 3) \"hello i am sakura\" --------------消息发布端------------------- 127.0.0.1:6379&gt; PUBLISH sakura \"hello world\" # 发布消息到sakura频道 (integer) 1 127.0.0.1:6379&gt; PUBLISH sakura \"hello i am sakura\" # 发布消息 (integer) 1 -----------------查看活跃的频道------------ 127.0.0.1:6379&gt; PUBSUB channels \"sakura\" 原理Redis是使用C实现的，通过分析 Redis 源码里的 pubsub.c 文件，了解发布和订阅机制的底层实现，籍此加深对 Redis 的理解。 Redis 通过 PUBLISH 、SUBSCRIBE 和 PSUBSCRIBE 等命令实现发布和订阅功能。 每个 Redis 服务器进程都维持着一个表示服务器状态的 redis.h/redisServer 结构， 结构的 pubsub_channels 属性是一个字典， 这个字典就用于保存订阅频道的信息，其中，字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。 客户端订阅，就被链接到对应频道的链表的尾部，退订则就是将客户端节点从链表中移除。 缺点 如果一个客户端订阅了频道，但自己读取消息的速度却不够快的话，那么不断积压的消息会使redis输出缓冲区的体积变得越来越大，这可能使得redis本身的速度变慢，甚至直接崩溃。 这和数据传输可靠性有关，如果在订阅方断线，那么他将会丢失所有在短线期间发布者发布的消息。 应用 消息订阅：公众号订阅，微博关注等等（起始更多是使用消息队列来进行实现） 多人在线聊天室。 稍微复杂的场景，我们就会使用消息中间件MQ处理。 十二、Redis主从复制概念主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点（Master/Leader）,后者称为从节点（Slave/Follower）， 数据的复制是单向的！只能由主节点复制到从节点（主节点以写为主、从节点以读为主）。 默认情况下，每台Redis服务器都是主节点，一个主节点可以有0个或者多个从节点，但每个从节点只能由一个主节点。 作用 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余的方式。 故障恢复：当主节点故障时，从节点可以暂时替代主节点提供服务，是一种服务冗余的方式 负载均衡：在主从复制的基础上，配合读写分离，由主节点进行写操作，从节点进行读操作，分担服务器的负载；尤其是在多读少写的场景下，通过多个从节点分担负载，提高并发量。 高可用基石：主从复制还是哨兵和集群能够实施的基础。 为什么使用集群一般来说，要将Redis运用于工程项目中，只使用一台Redis是万万不能的（宕机），原因如下： 1、从结构上，单个Redis服务器会发生单点故障，并且一台服务器需要处理所有的请求负载，压力较大； 2、从容量上，单个Redis服务器内存容量有限，就算一台Redis服务器内存容量为256G，也不能将所有内存用作Redis存储内存，一般来说，单台Redis最大使用内存不应该超过20G。 电商网站上的商品，一般都是一次上传，无数次浏览的，说专业点也就是”多读少写”。 对于这种场景，我们可以使如下这种架构： 主从复制，读写分离！ 80% 的情况下都是在进行读操作！减缓服务器的压力！架构中经常使用！ 一主二从！ 只要在公司中，主从复制就是必须要使用的，因为在真实的项目中不可能单机使用Redis！ 总结 单台服务器难以负载大量的请求 单台服务器故障率高，系统崩坏概率大 单台服务器内存容量有限。 环境配置只配置从库，不用配置主库！ 127.0.0.1:6379&gt; info replication # Replication role:master # 角色 connected_slaves:0 # 从机数量 master_replid:3b54deef5b7b7b7f7dd8acefa23be48879b4fcff master_replid2:0000000000000000000000000000000000000000 master_repl_offset:0 second_repl_offset:-1 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 复制3个配置文件，然后修改对应的信息 端口 pid名字 log文件名 dump.rdb名字 启动单机多服务集群： 一主二从配置默认情况下，每台Redis服务器都是主节点；我们一般情况下只用配置从机就好了！ 认老大！一主（79）二从（80，81） 使用SLAVEOF host port就可以为从机配置主机了。 说明 SLAVEOF host 6379 找谁当自己的老大！ role:slave # 当前角色是从机 master_host:127.0.0.1 # 可以的看到主机的信息 然后主机上也能看到从机的状态： 说明 connected_slaves:1 # 多了从机的配置 slave0:ip=127.0.0.1,port=6380,state=online,offset=42,lag=1 # 多了从机的配置 真实的从主配置应该在配置文件中配置，这样的话是永久的，我们这里使用的是命令，暂时的！ 使用规则1.从机只能读，不能写，主机可读可写但是多用于写。 127.0.0.1:6381&gt; set name sakura # 从机6381写入失败 (error) READONLY You can't write against a read only replica. 127.0.0.1:6380&gt; set name sakura # 从机6380写入失败 (error) READONLY You can't write against a read only replica. 127.0.0.1:6379&gt; set name sakura OK 127.0.0.1:6379&gt; get name \"sakura\" 2.当主机断电宕机后，默认情况下从机的角色不会发生变化 ，集群中只是失去了写操作，当主机恢复以后，又会连接上从机恢复原状。 3.当从机断电宕机后，若不是使用配置文件配置的从机，再次启动后作为主机是无法获取之前主机的数据的，若此时重新配置称为从机，又可以获取到主机的所有数据。这里就要提到一个同步原理。 4.第二条中提到，默认情况下，主机故障后，不会出现新的主机，有两种方式可以产生新的主机： 从机手动执行命令slaveof no one,这样执行以后从机会独立出来成为一个主机 使用哨兵模式（自动选举） 如果没有老大了，这个时候能不能选择出来一个老大呢？手动！ 如果主机断开了连接，我们可以使用SLAVEOF no one让自己变成主机！其他的节点就可以手动连接到最新的主节点（手动）！如果这个时候老大修复了，那么就重新连接！ 复制原理Slave 启动成功连接到 master 后会发送一个sync同步命令 Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master将传送整个数据文件到slave，并完成一次完全同步。 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master 继续将新的所有收集到的修改命令依次传给slave，完成同步 但是只要是重新连接master，一次完全同步（全量复制）将被自动执行！ 我们的数据一定可以在从机中看到！ 十三、哨兵模式（自动选举老大的模式） 概述主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。Redis从2.8开始正式提供了Sentinel（哨兵） 架构来解决这个问题。能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 哨兵的作用： 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服务器，修改配置文件，让它们切换主机。 然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover[故障转移]操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为客观下线。 测试1、配置哨兵配置文件 sentinel.conf # sentinel monitor 被监控的名称 host port 1 sentinel monitor myredis 127.0.0.1 6379 1 后面的这个数字1，代表主机挂了，slave投票看让谁接替成为主机，票数最多的，就会成为主机！ 2、启动哨兵！ 3、此时哨兵监视着我们的主机6379，当我们断开主机后： 哨兵模式优缺点优点： 哨兵集群，基于主从复制模式，所有主从复制的优点，它都有 主从可以切换，故障可以转移，系统的可用性更好 哨兵模式是主从模式的升级，手动到自动，更加健壮 缺点： Redis不好在线扩容，集群容量一旦达到上限，在线扩容就十分麻烦 实现哨兵模式的配置其实是很麻烦的，里面有很多配置项 哨兵模式的全部配置完整的哨兵模式配置文件 sentinel.conf # Example sentinel.conf 哨兵sentinel实例运行的端口 默认26379 port 26379 哨兵sentinel的工作目录 dir /tmp 哨兵sentinel监控的redis主节点的 ip port master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符\".-_\"组成。 quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了 sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; sentinel monitor mymaster 127.0.0.1 6379 1 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码 sentinel auth-pass &lt;master-name&gt; &lt;password&gt; sentinel auth-pass mymaster MySUPER--secret-0123passw0rd 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒 sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt; sentinel down-after-milliseconds mymaster 30000 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步， 这个数字越小，完成failover所需的时间就越长， 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。 sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt; sentinel parallel-syncs mymaster 1 故障转移的超时时间 failover-timeout 可以用在以下这些方面： 1. 同一个sentinel对同一个master两次failover之间的间隔时间。 2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。 3.当想要取消一个正在进行的failover所需要的时间。 4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了 默认三分钟 sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt; sentinel failover-timeout mymaster 180000 SCRIPTS EXECUTION 配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。 对于脚本的运行结果有以下规则： 若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10 若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。 如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。 一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。 通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本， 这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数， 一个是事件的类型， 一个是事件的描述。 如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。 通知脚本 sentinel notification-script &lt;master-name&gt; &lt;script-path&gt; sentinel notification-script mymaster /var/redis/notify.sh 客户端重新配置主节点参数脚本 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。 以下参数将会在调用脚本时传给脚本: &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt; 目前&lt;state&gt;总是“failover”, &lt;role&gt;是“leader”或者“observer”中的一个。 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的 这个脚本应该是通用的，能被多次调用，不是针对性的。 sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt; sentinel client-reconfig-script mymaster /var/redis/reconfig.sh 十四、缓存穿透与雪崩缓存穿透（即查询不到）概念在默认情况下，用户请求数据时，会先在缓存(Redis)中查找，若没找到即缓存未命中，再在数据库中进行查找，数量少可能问题不大，可是一旦大量的请求数据（例如秒杀场景）缓存都没有命中的话，就会全部转移到数据库上，造成数据库极大的压力，就有可能导致数据库崩溃。网络安全中也有人恶意使用这种手段进行攻击被称为洪水攻击。 解决方案布隆过滤器 对所有可能查询的参数以Hash的形式存储，以便快速确定是否存在这个值，在控制层先进行拦截校验，校验不通过直接打回，减轻了存储系统的压力。 缓存空对象 一次请求若在缓存和数据库中都没找到，就在缓存中方一个空对象用于处理后续这个请求。 这样做有一个缺陷：存储空对象也需要空间，大量的空对象会耗费一定的空间，存储效率并不高。解决这个缺陷的方式就是设置较短过期时间 即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。 缓存击穿（即量太大，缓存过期）概念相较于缓存穿透，缓存击穿的目的性更强，一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。这就是缓存被击穿，只是针对其中某个key的缓存不可用而导致击穿，但是其他的key依然可以使用缓存响应。 比如热搜排行上，一个热点新闻被同时大量访问就可能导致缓存击穿。 解决方案1.设置热点数据永不过期 这样就不会出现热点数据过期的情况，但是当Redis内存空间满的时候也会清理部分数据，而且此种方案会占用空间，一旦热点数据多了起来，就会占用部分空间。 2.加互斥锁(分布式锁) 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。保证同时刻只有一个线程访问。这样对锁的要求就十分高。 缓存雪崩概念大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 缓存雪崩，是指在某一个时间段，缓存集中过期失效。Redis 宕机！ 产生雪崩的原因之一，比如在写本文的时候，马上就要到双十二零点，很快就会迎来一波抢购，这波商品时间比较集中的放入了缓存，假设缓存一个小时。那么到了凌晨一点钟的时候，这批商品的缓存就都过期了。而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。 其实集中过期，倒不是非常致命，比较致命的缓存雪崩，是缓存服务器某个节点宕机或断网。因为自然形成的缓存雪崩，一定是在某个时间段集中创建缓存，这个时候，数据库也是可以顶住压力的。无非就是对数据库产生周期性的压力而已。而缓存服务节点的宕机，对数据库服务器造成的压力是不可预知的，很有可能瞬间就把数据库压垮。 解决方案 redis高可用 这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续工作，其实就是搭建的集群 限流降级 这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 数据预热 数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。 十五、淘汰策略Redis的数据已经设置了TTL，不是过期就已经删除了吗？为什么还存在所谓的淘汰策略呢？这个原因我们需要从redis的过期策略聊起。 1.两种淘汰策略1.1、定期删除redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，以后会定期遍历这个字典来删除到期的 key。 Redis 默认会每秒进行十次过期扫描（100ms一次），过期扫描不会遍历过期字典中所有的 key，而是采用了一种简单的贪心策略。 1.从过期字典中随机 20 个 key； 2.删除这 20 个 key 中已经过期的 key； 3.如果过期的 key 比率超过 1/4，那就重复步骤 1； redis默认是每隔 100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载。 1.2、惰性删除所谓惰性策略就是在客户端访问这个key的时候，redis对key的过期时间进行检查，如果过期了就立即删除，不会给你返回任何东西。 定期删除可能会导致很多过期key到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，即当你主动去查过期的key时,如果发现key过期了,就立即进行删除,不返回任何东西. 总结：定期删除是集中处理，惰性删除是零散处理。 2.为什么需要淘汰策略有了以上过期策略的说明后，就很容易理解为什么需要淘汰策略了，因为不管是定期采样删除还是惰性删除都不是一种完全精准的删除，就还是会存在key没有被删除掉的场景，所以就需要内存淘汰策略进行补充。 2.1、内存淘汰策略 noeviction：当内存使用超过配置的时候会返回错误，不会驱逐任何键 allkeys-lru：加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键 volatile-lru：加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键 allkeys-random：加入键的时候如果过限，从所有key随机删除 volatile-random：加入键的时候如果过限，从过期键的集合中随机驱逐 volatile-ttl：从配置了过期时间的键中驱逐马上就要过期的键 volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键 allkeys-lfu：从所有键中驱逐使用频率最少的键 3、LRU3.1、标准LRU实现方式 新增key value的时候首先在链表结尾添加Node节点，如果超过LRU设置的阈值就淘汰队头的节点并删除掉HashMap中对应的节点。 修改key对应的值的时候先修改对应的Node中的值，然后把Node节点移动队尾。 访问key对应的值的时候把访问的Node节点移动到队尾即可。 3.2、Redis的LRU实现Redis维护了一个24位时钟，可以简单理解为当前系统的时间戳，每隔一定时间会更新这个时钟。每个key对象内部同样维护了一个24位的时钟，当新增key对象的时候会把系统的时钟赋值到这个内部对象时钟。比如我现在要进行LRU，那么首先拿到当前的全局时钟，然后再找到内部时钟与全局时钟距离时间最久的（差最大）进行淘汰，这里值得注意的是全局时钟只有24位，按秒为单位来表示才能存储194天，所以可能会出现key的时钟大于全局时钟的情况，如果这种情况出现那么就两个相加而不是相减来求最久的key。 struct redisServer { pid_t pid; char *configfile; //全局时钟 unsigned lruclock:LRU_BITS; ... }; typedef struct redisObject { unsigned type:4; unsigned encoding:4; /* key对象内部时钟 */ unsigned lru:LRU_BITS; int refcount; void *ptr; } robj; Redis中的LRU与常规的LRU实现并不相同，常规LRU会准确的淘汰掉队头的元素，但是Redis的LRU并不维护队列，只是根据配置的策略要么从所有的key中随机选择N个（N可以配置）要么从所有的设置了过期时间的key中选出N个键，然后再从这N个键中选出最久没有使用的一个key进行淘汰。 下图是常规LRU淘汰策略与Redis随机样本取一键淘汰策略的对比，浅灰色表示已经删除的键，深灰色表示没有被删除的键，绿色表示新加入的键，越往上表示键加入的时间越久。从图中可以看出，在redis 3中，设置样本数为10的时候能够很准确的淘汰掉最久没有使用的键，与常规LRU基本持平。 3.3、为什么要使用近似LRU？1、性能问题，由于近似LRU算法只是最多随机采样N个key并对其进行排序，如果精准需要对所有key进行排序，这样近似LRU性能更高 2、内存占用问题，redis对内存要求很高，会尽量降低内存使用率，如果是抽样排序可以有效降低内存的占用 3、实际效果基本相等，如果请求符合长尾法则，那么真实LRU与Redis LRU之间表现基本无差异 4、在近似情况下提供可自配置的取样率来提升精准度，例如通过 CONFIG SET maxmemory-samples 指令可以设置取样数，取样数越高越精准，如果你的CPU和内存有足够，可以提高取样数看命中率来探测最佳的采样比例。 4、LFULFU是在Redis4.0后出现的，LRU的最近最少使用实际上并不精确，考虑下面的情况，如果在|处删除，那么A距离的时间最久，但实际上A的使用频率要比B频繁，所以合理的淘汰策略应该是淘汰B。LFU就是为应对这种情况而生的。 AAAAAAAAA~~A~~~| B~~~~~B~~~~~B~~~~~B~~~~~~~~~~~~B| LFU把原来的key对象的内部时钟的24位分成两部分，前16位还代表时钟，后8位代表一个计数器。16位的情况下如果还按照秒为单位就会导致不够用，所以一般这里以时钟为单位。而后8位表示当前key对象的访问频率，8位只能代表255，但是redis并没有采用线性上升的方式，而是通过一个复杂的公式，通过配置如下两个参数来调整数据的递增速度。 lfu-log-factor 可以调整计数器counter的增长速度，lfu-log-factor越大，counter增长的越慢。 lfu-decay-time 是一个以分钟为单位的数值，可以调整counter的减少速度。 所以这两个因素就对应到了LFU的Counter减少策略和增长策略，它们实现逻辑分别如下。 4.1、降低LFUDecrAndReturn 先从高16位获取最近的降低时间ldt以及低8位的计数器counter值 计算当前时间now与ldt的差值（now-ldt），当ldt大于now时，那说明是过了一个周期，按照65535-ldt+now计算（16位一个周期最大65535） 使用第2步计算的差值除以lfu_decay_time，即LFUTimeElapsed(ldt) / server.lfu_decay_time，已过去n个lfu_decay_time，则将counter减少n。 4.2、增长LFULogIncr1、获取0-1的随机数r 2、计算0-1之间的控制因子p，它的计算逻辑如下 //LFU_INIT_VAL默认为5 baseval = counter - LFU_INIT_VAL; //计算控制因子 p = 1.0/(baseval*lfu_log_factor+1); 3、如果r小于p，counter增长1 p取决于当前counter值与lfu_log_factor因子，counter值与lfu_log_factor因子越大，p越小，r小于p的概率也越小，counter增长的概率也就越小。增长情况如下图： 从左到右表示key的命中次数，从上到下表示影响因子，在影响因子为100的条件下，经过10M次命中才能把后8位值加满到255. 4.3、新生KEY策略另外一个问题是，当创建新对象的时候，对象的counter如果为0，很容易就会被淘汰掉，还需要为新生key设置一个初始counter。counter会被初始化为LFU_INIT_VAL，默认5。","categories":[{"name":"redis","slug":"redis","permalink":"https://13592491893.github.io/categories/redis/"}],"tags":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"redis","slug":"redis","permalink":"https://13592491893.github.io/tags/redis/"},{"name":"nosql","slug":"nosql","permalink":"https://13592491893.github.io/tags/nosql/"}]},{"title":"微服务中远程调用Dubbo与Feign对比","slug":"微服务中远程调用Dubbo与Feign对比","date":"2020-03-21T16:00:00.000Z","updated":"2021-12-29T03:19:25.627Z","comments":true,"path":"posts/16094.html","link":"","permalink":"https://13592491893.github.io/posts/16094.html","excerpt":"","text":"微服务中远程调用Dubbo与Feign对比一、实现远程调用的方式Http接口（web接口、RestTemplate+Okhttp）、Feign、RPC调用（Dubbo、Socket编程）、Webservice。 二、什么是Feign？Feign是Spring Cloud提供的一个声明式的伪Http客户端，它使得调用远程服务就像调用本地服务一样简单，只需要创建一个接口并添加一个注解即可。 Nacos注册中心很好的兼容了Feign，Feign默认集成了Ribbon，所以在Nacos下使用Fegin默认就实现了负载均衡的效果。 三、什么是Dubbo？Dubbo是阿里巴巴开源的基于Java的高性能RPC分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。 Spring-cloud-alibaba-dubbo是基于SpringCloudAlibaba技术栈对dubbo技术的一种封装，目的在于实现基于RPC的服务调用。 四、Feign与Dubbo的对比Feign与Dubbo功能上有很多类似的地方，因为都是专注于远程调用这个动作。比如注册中心解耦、负载均衡、失败重试熔断、链路监控等。 Dubbo除了注册中心需要进行整合，其它功能都自己实现了，而Feign大部分功能都是依赖全家桶的组件来实现的。Dubbo小而专一，专注于远程调用。而Spring全家桶而言，远程调用只是一个重要的功能而已。 五、协议支持方面：Feign更加优雅简单。Feign是通过REST API实现的远程调用，基于Http传输协议，服务提供者需要对外暴露Http接口供消费者调用，服务粒度是http接口级的。通过短连接的方式进行通信，不适合高并发的访问。Feign追求的是简洁，少侵入（因为就服务端而言，在SpringCloud环境下，不需要做任何额外的操作，而Dubbo的服务端需要配置开放的Dubbo接口)。 Dubbo方式更灵活。Dubbo是通过RPC调用实现的远程调用，支持多传输协议(Dubbo、Rmi、http、redis等等)，可以根据业务场景选择最佳的方式，非常灵活。默认的Dubbo协议：利用Netty，TCP传输，单一、异步、长连接，适合数据量小、高并发和服务提供者远远少于消费者的场景。Dubbo通过TCP长连接的方式进行通信，服务粒度是方法级的。 从协议层选择看，Dubbo是配置化的，更加灵活。Dubbo协议更适合小数据高并发场景。 六、通信性能方面：从通信的性能上来分析，SpringCloud的通信采用Openfeign（feign）组件。 Feign基于Http传输协议，底层实现是rest。从OSI 7层模型上来看rest属于应用层。 在高并发场景下性能不够理想，成为性能瓶颈（虽然他是基于Ribbon以及带有熔断机制可以防止雪崩），需要改造。具体需要改造的内容需要时再研究。 Dubbo框架的通信协议采用RPC协议，属于传输层协议，性能上自然比rest高。提升了交互的性能，保持了长连接，高性能。 Dubbo性能更好，比如支持异步调用、Netty性能更好。Dubbo主要是配置而无需改造。 RPC REST 耦合性 强耦合 松耦合 消息协议 二进制 thrift/protobuf 文本 xml、jason 通信协议 TCP HTTP 接口契约IDL thrift/protobuf swagger 开发调试 消息不可读 可读，可调试 对外开放 一般作为内部各个系统的通信框架 对接外部系统 使用SpringCloud整合Dubbo，正所谓是强强联合。 七、负载均衡方面：Feign默认使用Ribbon作为负载均衡的组件。 Dubbo和Ribbon（Feign默认集成Ribbon）都支持负载均衡策略，但是Dubbo支持的更灵活。 Dubbo和Ribbon对比： Ribbon的负载均衡策略：随机、规则轮询、空闲策略、响应时间策略。 Dubbo的负载均衡策略：Dubbo支持4种算法，随机、权重轮询、最少活跃调用数、一致性Hash策略。而且算法里面引入权重的概念。 Dubbo可以使用路由策略，然后再进行负载均衡。 Dubbo配置的形式不仅支持代码配置，还支持Dubbo控制台灵活动态配置。 Dubbo负载均衡的算法可以精准到某个服务接口的某个方法，而Ribbon的算法是Client级别的。Ribbon需要进行全局配置，个性化配置比较麻烦。 八、容错机制方面：Feign默认使用Hystix作为服务熔断的组件。Hystix提供了服务降级，服务熔断，依赖隔离，监控（Hystrix Dashboard）等功能。Feign是利用熔断机制来实现容错的，与Dubbo处理的方式不一样。 Dubbo支持多种容错策略，FailOver、FailFast、Failsafe、FailBack、Aviailable、Broadcast、Forking策略等，以及Mock。也引入了retry次数，timeout等配置参数。Dubbo自带了失败重试的功能。 九、其他方面（以下方便并未进行详细整理仅做参考）：Dubbo附带了白名单功能、结果缓存、同步和异步调用的功能。 Dubbo支持更多更灵活的并发控制： 客户端配置actives参数，配置单个Cunsumer最大并发请求数，超出则线程阻塞等待，超时报错。 Provider可以配置executes参数来限制最大的并发线程数，超出报错。 Provider可以配置accepts参数来限制最大长连接数来限制最大的连接数。 Provider的通过配置任务线程池的类型和最大线程数来控制并发量，超负载直接丢弃。 路由、流量调度、ABtest方面： Ribbon需自己实现，应用不灵活。 Ribbon主要通过扩展AbstractLoadBalancerRule负载均衡的方法来实现，在负载均衡的部分还要进行改造升级。 Dubbo更加灵活方便。 Dubbo通过界面化、校本化配置路由规则，可以实现灰度发布、动态流量调度、容量计算等，方案成熟。 另外，Dubbo 还支持多版本调用。 Dubbo支持更完善的监控和管理界面，SC也有Actuator等工具进行监控，但是并不是针对远程调用这一块的 Dubbo支持客户端设置调用结果缓存，支持配置3种策略的结果缓存(LRU、LFU、FIO)，但是要自己实现超时管理。 十、总结Dubbo支持更多功能、更灵活、支持高并发的RPC框架。 SpringCloud全家桶里面（Feign、Ribbon、Hystrix），特点是非常方便。Ribbon、Hystrix、Feign在服务治理中，配合Spring Cloud做微服务，使用上有很多优势，社区也比较活跃，看将来更新发展。 业务发展影响着架构的选型，当服务数量不是很大时，使用普通的分布式RPC架构即可，当服务数量增长到一定数据，需要进行服务治理时，就需要考虑使用流式计算架构。Dubbo可以方便的做更精细化的流量调度，服务结构治理的方案成熟，适合生产上使用，虽然Dubbo是尘封后重新开启，但这并不影响其技术价值。 如果项目对性能要求不是很严格，可以选择使用Feign，它使用起来更方便。 如果需要提高性能，避开基于Http方式的性能瓶颈，可以使用Dubbo。 Dubbo Spring Cloud的出现，使得Dubbo既能够完全整合到Spring Cloud的技术栈中，享受Spring Cloud生态中的技术支持和标准化输出，又能够弥补Spring Cloud中服务治理这方面的短板。 十一、Example1.openfeignprovider： consumer： 2.dubbo 建一个接口模块，POM文件引入zookeeper，不需要启动类 建服务提供方，service目录与api里的保持一致 建消费方 十二、HttpClient、RestTemplate和Feign相关知识先了解一下HTTP 协议 史前时期 HTTP 协议在我们的生活中随处可见，打开手机或者电脑，只要你上网，不论是用 iPhone、Android、Windows 还是 Mac，不论是用浏览器还是 App，不论是看新闻、短视频还是听音乐、玩游戏，后面总会有 HTTP 在默默为你服务。 据 NetCraft 公司统计，目前全球至少有 16 亿个网站、2 亿多个独立域名，而这个庞大网络世界的底层运转机制就是 HTTP。 那么，在享受如此便捷舒适的网络生活时，你有没有想过，HTTP 协议是怎么来的？它最开始是什么样子的？又是如何一步一步发展到今天，几乎“统治”了整个互联网世界的呢？ 20 世纪 60 年代，美国国防部高等研究计划署（ARPA）建立了 ARPA 网，它有四个分布在各地的节点，被认为是如今互联网的“始祖”。 然后在 70 年代，基于对 ARPA 网的实践和思考，研究人员发明出了著名的 TCP/IP 协议。由于具有良好的分层结构和稳定的性能，TCP/IP 协议迅速战胜其他竞争对手流行起来，并在 80 年代中期进入了 UNIX 系统内核，促使更多的计算机接入了互联网。 创世纪 1989 年，任职于欧洲核子研究中心（CERN）的蒂姆·伯纳斯 - 李（Tim Berners-Lee）发表了一篇论文，提出了在互联网上构建超链接文档系统的构想。这篇论文中他确立了三项关键技术。 URI：即统一资源标识符，作为互联网上资源的唯一身份； HTML：即超文本标记语言，描述超文本文档； HTTP：即超文本传输协议，用来传输超文本。 所以在这一年，我们的HTTP诞生了。 HTTP/0.9 20 世纪 90 年代初期的互联网世界非常简陋，计算机处理能力低，存储容量小，网速很慢，还是一片“信息荒漠”。网络上绝大多数的资源都是纯文本，很多通信协议也都使用纯文本，所以 HTTP 的设计也不可避免地受到了时代的限制。 这一时期的 HTTP 被定义为 0.9 版，结构比较简单，为了便于服务器和客户端处理，它也采用了纯文本格式。蒂姆·伯纳斯 - 李最初设想的系统里的文档都是只读的，所以只允许用“GET”动作从服务器上获取 HTML 文档，并且在响应请求之后立即关闭连接，功能非常有限。 HTTP/0.9 虽然很简单，但它作为一个“原型”，充分验证了 Web 服务的可行性，而“简单”也正是它的优点，蕴含了进化和扩展的可能性，因为： “把简单的系统变复杂”，要比“把复杂的系统变简单”容易得多。 HTTP/1.0 1993 年，NCSA（美国国家超级计算应用中心）开发出了 Mosaic，是第一个可以图文混排的浏览器，随后又在 1995 年开发出了服务器软件 Apache，简化了 HTTP 服务器的搭建工作。 同一时期，计算机多媒体技术也有了新的发展：1992 年发明了 JPEG 图像格式，1995 年发明了 MP3 音乐格式。 这些新软件新技术一经推出立刻就吸引了广大网民的热情，更的多的人开始使用互联网，研究 HTTP 并提出改进意见，甚至实验性地往协议里添加各种特性，从用户需求的角度促进了 HTTP 的发展。 于是在这些已有实践的基础上，经过一系列的草案，HTTP/1.0 版本在 1996 年正式发布。它在多方面增强了 0.9 版，形式上已经和我们现在的 HTTP 差别不大了，例如： 增加了 HEAD、POST 等新方法 增加了响应状态码，标记可能的错误原因 引入了协议版本号概念 引入了 HTTP Header（头部）的概念，让 HTTP 处理请求和响应更加灵活 传输的数据不再仅限于文本 HTTP/1.1 1995 年，网景的 Netscape Navigator 和微软的 Internet Explorer 开始了著名的“浏览器大战”，都希望在互联网上占据主导地位。于是在“浏览器大战”结束之后的 1999 年，HTTP/1.1 发布了 RFC 文档，编号为 2616，正式确立了延续十余年的传奇。 HTTP/1.1 主要的变更点有： 增加了 PUT、DELETE 等新的方法； 增加了缓存管理和控制； 明确了连接管理，允许持久连接； 允许响应数据分块（chunked），利于传输大文件； 强制要求 Host 头，让互联网主机托管成为可能。 HTTP/2 HTTP/1.1 发布之后，整个互联网世界呈现出了爆发式的增长，度过了十多年的“快乐时光”，更涌现出了 Facebook、Twitter、淘宝、京东等互联网新贵。 这期间也出现了一些对 HTTP 不满的意见，主要就是连接慢，无法跟上迅猛发展的互联网，但 HTTP/1.1 标准一直“岿然不动”，无奈之下人们只好发明各式各样的“小花招”来缓解这些问题，比如以前常见的切图、JS 合并等网页优化手段。 终于有一天，搜索巨头 Google 忍不住了，首先开发了自己的浏览器 Chrome，然后推出了新的 SPDY 协议，并在 Chrome 里应用于自家的服务器，如同十多年前的网景与微软一样，从实际的用户方来“倒逼”HTTP 协议的变革，这也开启了第二次的“浏览器大战”。 历史再次重演，不过这次的胜利者是 Google，Chrome 目前的全球的占有率超过了 60%。Google 借此顺势把 SPDY 推上了标准的宝座，互联网标准化组织以 SPDY 为基础开始制定新版本的 HTTP 协议，最终在 2015 年发布了 HTTP/2，RFC 编号 7540。 SPDY（读作“SPeeDY”）是Google开发的基于TCP的会话层协议，用以最小化网络延迟，提升网络速度，优化用户的网络使用体验。 SPDY并不是一种用于替代HTTP的协议，而是对HTTP协议的增强。新协议的功能包括数据流的多路复用、请求优先级以及HTTP报头压缩。 谷歌表示，引入SPDY协议后，在实验室测试中页面加载速度比原先快64%。 HTTP/2 的制定充分考虑了现今互联网的现状：宽带、移动、不安全，在高度兼容 HTTP/1.1 的同时在性能改善方面做了很大努力，主要的特点有： 二进制协议，不再是纯文本 可发起多个请求，废弃了 1.1 里的管道 使用专用算法压缩头部，减少数据传输量 允许服务器主动向客户端推送数据 增强了安全性，“事实上”要求加密通信 虽然 HTTP/2 到今天已经五岁，也衍生出了 gRPC 等新协议，但由于 HTTP/1.1 实在是太过经典和强势，目前它的普及率还比较低，大多数网站使用的仍然还是 20 年前的 HTTP/1.1。 HTTP/3 在 HTTP/2 还处于草案之时，Google 又发明了一个新的协议，叫做 QUIC，而且还是相同的“套路”，继续在 Chrome 和自家服务器里试验着“玩”，依托它的庞大用户量和数据量，持续地推动 QUIC 协议成为互联网上的“既成事实”。 2018 年，互联网标准化组织 IETF 提议将“HTTP over QUIC”更名为“HTTP/3”并获得批准，HTTP/3 正式进入了标准化制订阶段，也许两三年后就会正式发布，到时候我们很可能会跳过 HTTP/2 直接进入 HTTP/3。 QUIC（Quick UDP Internet Connection）是谷歌制定的一种基于UDP的低时延的互联网传输层协议。 在2016年11月国际互联网工程任务组(IETF)召开了第一次QUIC工作组会议，受到了业界的广泛关注。 这也意味着QUIC开始了它的标准化过程，成为新一代传输层协议 了解这么多，那到底HTTP是什么呢？ 你可能会不假思索、脱口而出：“HTTP 就是超文本传输协议，也就是 HyperText Transfer Protocol。” 回答的也没错，但是太过简单。更准确的回答应该是“HTTP 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范” 关于HttpClient 简介 官网这样说 超文本传输协议（HTTP）可能是当今Internet上使用的最重要的协议。Web服务，支持网络的设备和网络计算的增长继续将HTTP协议的作用扩展到用户驱动的Web浏览器之外，同时增加了需要HTTP支持的应用程序的数量。 尽管java.net软件包提供了用于通过HTTP访问资源的基本功能，但它并未提供许多应用程序所需的全部灵活性或功能。HttpClient试图通过提供高效，最新且功能丰富的程序包来实现此空白，以实现最新HTTP标准和建议的客户端。 HttpClient是为扩展而设计的，同时提供了对基本HTTP协议的强大支持，对于构建HTTP感知的客户端应用程序（例如Web浏览器，Web服务客户端或利用或扩展HTTP协议进行分布式通信的系统）的任何人来说，HttpClient都可能会感兴趣。 HttpClient 是 Apache Jakarta Common 下的子项目，用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。HttpClient 已经应用在很多的项目中，比如 Apache Jakarta 上很著名的另外两个开源项目 Cactus 和 HTMLUnit 都使用了 HttpClient。 HttpClient 相比传统 JDK 自带的URLConnection，增加了易用性和灵活性，它不仅是客户端发送 HTTP 请求变得容易，而且也方便了开发人员测试接口（基于 HTTP 协议的），即提高了开发的效率，也方便提高代码的健壮性。因此熟练掌握 HttpClient 是很重要的必修内容，掌握 HttpClient 后，相信对于 HTTP 协议的了解会更加深入。 了解更新详情，参考官网http://hc.apache.org/index.html 特性 基于标准、纯净的 Java 语言。实现了 HTTP 1.0 和 HTTP 1.1 以可扩展的面向对象的结构实现了 HTTP 全部的方法（GET, POST, PUT, DELETE, HEAD, OPTIONS, and TRACE）。 支持 HTTPS 协议。 通过 HTTP 代理建立透明的连接。 利用 CONNECT 方法通过 HTTP 代理建立隧道的 HTTPS 连接。 Basic, Digest, NTLMv1, NTLMv2, NTLM2 Session, SNPNEGO/Kerberos 认证方案。 插件式的自定义认证方案。 便携可靠的套接字工厂使它更容易的使用第三方解决方案。 连接管理器支持多线程应用。支持设置最大连接数，同时支持设置每个主机的最大连接数，发现并关闭过期的连接。 自动处理 Set-Cookie 中的 Cookie。 插件式的自定义 Cookie 策略。 Request 的输出流可以避免流中内容直接缓冲到 Socket 服务器。 Response 的输入流可以有效的从 Socket 服务器直接读取相应内容。 在 HTTP 1.0 和 HTTP 1.1 中利用 KeepAlive 保持持久连接。 直接获取服务器发送的 response code 和 headers。 设置连接超时的能力。 实验性的支持 HTTP 1.1 response caching。 源代码基于 Apache License 可免费获取。 使用流程 创建 HttpClient 对象 创建请求方法的实例，并指定请求 URL。如果需要发送 GET 请求，创建 HttpGet 对象；如果需要发送 POST 请求，创建 HttpPost 对象 如果需要发送请求参数，可调用 HttpGet、HttpPost 共同的 setParams(HttpParams params) 方法来添加请求参数；对于 HttpPost 对象而言，也可调用 setEntity(HttpEntity entity) 方法来设置请求参数 调用 HttpClient 对象的 execute(HttpUriRequest request) 发送请求，该方法返回一个 HttpResponse 调用 HttpResponse 的 getAllHeaders()、getHeaders(String name) 等方法可获取服务器的响应头 调用 HttpResponse 的 getEntity() 方法可获取 HttpEntity 对象，该对象包装了服务器的响应内容。程序可通过该对象获取服务器的响应内容 释放连接。无论执行方法是否成功，都必须释放连接 使用用例 pom配置 &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.12&lt;/version&gt; &lt;/dependency&gt; 创建Get请求 @Test void testGet() { // 创建 HttpClient 客户端 CloseableHttpClient httpClient = HttpClients.createDefault(); // 创建 HttpGet 请求 HttpGet httpGet = new HttpGet(\"http://192.168.1.250:15005/dsm-ubm/base-role-info/list\"); // 设置长连接 httpGet.setHeader(\"Connection\", \"keep-alive\"); // 设置认证信息 httpGet.setHeader(\"Authorization\", \"eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ0ZXN0IiwiY3JlYXRlZCI6MTU4ODA2NjM1NTMxNiwiZXhwIjoxNTg4NjcxMTU1fQ.mfroxGQMf_QbHGViEBhQ0hzHoxdNM0TwpGWT64t3LPUl8Sn_ZSBFFKUAt0aKkywM3Lq8245LSXu6BYOptVwYZg\"); // 设置代理（模拟浏览器版本） httpGet.setHeader(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\"); CloseableHttpResponse httpResponse = null; try { // 请求并获得响应结果 httpResponse = httpClient.execute(httpGet); HttpEntity httpEntity = httpResponse.getEntity(); //打印结果 System.out.println(EntityUtils.toString(httpEntity)); } catch (IOException e) { e.printStackTrace(); } finally { if (httpResponse != null) { try { httpResponse.close(); } catch (IOException e) { e.printStackTrace(); } } if (httpClient != null) { try { httpClient.close(); } catch (IOException e) { e.printStackTrace(); } } } } keep-alive说明 keep-alive：从HTTP/1.1起，浏览器默认都开启了Keep-Alive，保持连接特性，客户端和服务器都能选择随时关闭连接，则请求头中为connection:close。 简单地说，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的TCP连接。 但是Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。 创建Post请求，content-type=application/json @Test void testPost() { // 创建 HttpClient 客户端 CloseableHttpClient httpClient = HttpClients.createDefault(); // 创建 HttpPost 请求 HttpPost httpPost = new HttpPost(\"http://192.168.1.250:15005/dsm-ubm/api/getInAreaPatientList\"); // 设置长连接 httpPost.setHeader(\"Connection\", \"keep-alive\"); // 设置认证信息 httpPost.setHeader(\"Authorization\", \"eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ0ZXN0IiwiY3JlYXRlZCI6MTU4ODA2NjM1NTMxNiwiZXhwIjoxNTg4NjcxMTU1fQ.mfroxGQMf_QbHGViEBhQ0hzHoxdNM0TwpGWT64t3LPUl8Sn_ZSBFFKUAt0aKkywM3Lq8245LSXu6BYOptVwYZg\"); // 设置代理（模拟浏览器版本） httpPost.setHeader(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\"); // 创建 HttpPost 参数 JSONObject jsonObject = new JSONObject(); jsonObject.put(\"FromTime\", \"\"); jsonObject.put(\"Ward\", \"4008\"); CloseableHttpResponse httpResponse = null; try { StringEntity entity = new StringEntity(jsonObject.toJSONString(), \"utf-8\"); entity.setContentType(\"application/json\"); httpPost.setEntity(entity); httpResponse = httpClient.execute(httpPost); HttpEntity httpEntity = httpResponse.getEntity(); //打印结果 System.out.println(EntityUtils.toString(httpEntity)); } catch (IOException e) { e.printStackTrace(); } finally { try { if (httpResponse != null) { httpResponse.close(); } } catch (IOException e) { e.printStackTrace(); } try { if (httpClient != null) { httpClient.close(); } } catch (IOException e) { e.printStackTrace(); } } } 创建Post请求，content-type=application/x-www-form-urlencoded @Test void testPost() { // 创建 HttpClient 客户端 CloseableHttpClient httpClient = HttpClients.createDefault(); // 创建 HttpPost 请求 HttpPost httpPost = new HttpPost(\"http://localhost:8080/hello\"); // 设置长连接 httpPost.setHeader(\"Connection\", \"keep-alive\"); // 设置认证信息 httpPost.setHeader(\"Authorization\", \"eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ0ZXN0IiwiY3JlYXRlZCI6MTU4ODA2NjM1NTMxNiwiZXhwIjoxNTg4NjcxMTU1fQ.mfroxGQMf_QbHGViEBhQ0hzHoxdNM0TwpGWT64t3LPUl8Sn_ZSBFFKUAt0aKkywM3Lq8245LSXu6BYOptVwYZg\"); // 设置代理（模拟浏览器版本） httpPost.setHeader(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\"); // 创建 HttpPost 参数 List&lt;BasicNameValuePair&gt; params = new ArrayList&lt;&gt;(); params.add(new BasicNameValuePair(\"message\", \"鹧鸪哨\")); CloseableHttpResponse httpResponse = null; try { httpPost.setEntity(new UrlEncodedFormEntity(params, \"utf-8\")); httpResponse = httpClient.execute(httpPost); HttpEntity httpEntity = httpResponse.getEntity(); //打印结果 System.out.println(EntityUtils.toString(httpEntity)); } catch (IOException e) { e.printStackTrace(); } finally { try { if (httpResponse != null) { httpResponse.close(); } } catch (IOException e) { e.printStackTrace(); } try { if (httpClient != null) { httpClient.close(); } } catch (IOException e) { e.printStackTrace(); } } } application/json与application/x-www-form-urlencoded区别 application/json和application/x-www-form-urlencoded都是表单数据发送时的编码类型。默认地，表单数据会编码为application/x-www-form-urlencoded。就是说，在发送到服务器之前，所有字符都会进行编码。 application/json，随着json规范的越来越流行，并且浏览器支持程度原来越好，许多开发人员将application/json作为请求content-type，告诉服务器请求的主体内容是json格式的字符串，服务器端会对json字符串进行解析，这种方式的好处就是前端人员不需要关心数据结构的复杂度，只要是标准的json格式就能提交成功，需要封装成对象的话，可以加上@RequestBody注解 application/x-www-form-urlencoded是Jquery的Ajax请求默认方式，这种方式的好处就是浏览器都支持，在请求发送过程中会对数据进行序列化处理，以键值对形式，数据拼接方式为key=value的方式，后台如果使用对象接收的话，可以自动封装成对象 @RequestBody和@RequestParam区别 @RequestParam 用来处理Content-Type: 为 application/x-www-form-urlencoded编码的内容。（Http协议中，如果不指定Content-Type，则默认传递的参数就是application/x-www-form-urlencoded类型）。 在Content-Type: application/x-www-form-urlencoded的请求中， get 方式中queryString的值，和post方式中 body data的值都会被Servlet接受到并转化到Request.getParameter()参数集中，所以@RequestParam可以获取的到。 @RequestBody 处理HttpEntity传递过来的数据，一般用来处理非Content-Type: application/x-www-form-urlencoded编码格式的数据。 GET请求中，因为没有HttpEntity，所以@RequestBody并不适用。 POST请求中，通过HttpEntity传递的参数，必须要在请求头中声明数据的类型Content-Type，SpringMVC通过使用HandlerAdapter 配置的HttpMessageConverters来解析HttpEntity中的数据，然后绑定到相应的bean上。 @ResponseBody 和 @RequestBody 区别 @ResponseBody是作用在方法上的，@ResponseBody 表示该方法的返回结果直接写入 HTTP response body 中，一般在异步获取数据时使用【也就是AJAX】，在使用 @RequestMapping后，返回值通常解析为跳转路径，但是加上 @ResponseBody 后返回结果不会被解析为跳转路径，而是直接写入 HTTP response body 中。 比如异步获取 json 数据，加上 @ResponseBody 后，会直接返回 json 数据。 @RequestBody 用于读取Request请求的body部分数据，使系统默认的HttpMessageConverter进行解析，然后把相应的数据绑定要返回的对象上，再把HttpMessageConverter返回的对象数据绑定到Controller方法的参数上。 #关于RestTemplate 介绍 RestTemplate 是从 Spring3.0 开始支持的一个 HTTP 请求工具，它提供了常见的REST请求方案的模版，例如 GET 请求、POST 请求、PUT 请求、DELETE 请求以及一些通用的请求执行方法 exchange 以及 execute。RestTemplate 继承自 InterceptingHttpAccessor 并且实现了 RestOperations 接口，其中 RestOperations 接口定义了基本的 RESTful 操作，这些操作在 RestTemplate 中都得到了实现。是比httpClient更优雅的Restful URL访问。 RestTemplate是Spring提供的用于访问Rest服务的客户端， RestTemplate提供了多种便捷访问远程Http服务的方法,能够大大提高客户端的编写效率。 调用RestTemplate的默认构造函数，RestTemplate对象在底层通过使用java.net包下的实现创建HTTP 请求， 可以通过使用ClientHttpRequestFactory指定不同的HTTP请求方式。 ClientHttpRequestFactory接口主要提供了三种实现方式 1、SimpleClientHttpRequestFactory方式，此处生成SimpleBufferingClientHttpRequest，使用HttpURLConnection创建底层的Http请求连接 2、HttpComponentsClientHttpRequestFactory方式，此处生成HttpComponentsClientHttpRequest，使用http client来实现网络请求 3、OkHttp3ClientHttpRequestFactory方式，此处生成OkHttp3ClientHttpRequest，使用okhttp来实现网络请求 优点 并没有重写底层的HTTP请求技术，而是提供配置，可选用OkHttp/HttpClient等 在OkHttp/HttpClient之上，封装了请求操作，可以定义Convertor来实现对象到请求body的转换方法，以及返回body到对象的转换方法。 配置 @Configuration public class RestTemplateConfig { @Bean public RestTemplate restTemplate(ClientHttpRequestFactory factory) { return new RestTemplate(factory); } @Bean public ClientHttpRequestFactory simpleClientHttpRequestFactory() { SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory(); //单位为ms factory.setReadTimeout(5000); //单位为ms factory.setConnectTimeout(5000); return factory; } @Primary @Bean public ClientHttpRequestFactory httpComponentsClientHttpRequestFactory() { HttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory(); //单位为ms factory.setReadTimeout(6000); //单位为ms factory.setConnectTimeout(6000); return factory; } /*@Bean public ClientHttpRequestFactory okHttp3ClientHttpRequestFactory() { OkHttp3ClientHttpRequestFactory factory = new OkHttp3ClientHttpRequestFactory(); //单位为ms factory.setReadTimeout(7000); //单位为ms factory.setConnectTimeout(7000); return factory; }*/ } 使用 @RunWith(SpringRunner.class) @SpringBootTest public class TestGet { @Autowired private RestTemplate restTemplate; @Test public void testGet() { String url = \"http://localhost:8080/welcome?message={1}\"; //可以使用map来封装请求参数 Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"1\", \"world\"); String jsonResult = restTemplate.getForObject(url, String.class, map); System.out.println(\"result:\" + jsonResult); } } @RunWith(SpringRunner.class) @SpringBootTest public class TestPost { @Autowired private RestTemplate restTemplate; @Test public void testPost() { RequestObj requestObj = RequestObj.builder().id(1) .age(20) .name(\"鹧鸪哨\").build(); String url = \"http://localhost:8080/test\"; //发起请求 String jsonResult = restTemplate.postForObject(url, requestObj, String.class); System.out.println(\"result:\" + jsonResult); } } #关于Feign 介绍 Feign 的英文表意为“假装，伪装，变形”， 是一个http请求调用的轻量级框架，可以以Java接口注解的方式调用Http请求，而不用像Java中通过封装HTTP请求报文的方式直接调用。Feign通过处理注解，将请求模板化，当实际调用的时候，传入参数，根据参数再应用到请求上，进而转化成真正的请求，这种请求相对而言比较直观。 Feign被广泛应用在Spring Cloud 的解决方案中，是学习基于Spring Cloud 微服务架构不可或缺的重要组件。 具体详情参考https://github.com/OpenFeign/feign pom配置 &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- feign底层采用的http请求方式 不加则默认使用JDK的HttpURLConnection --&gt; &lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Feign配置 @Configuration public class FeignConfig { @Bean Logger.Level feignLoggerLevel() { //记录请求和响应的标头，正文和元数据 return Logger.Level.FULL; } /** * 如果远程接口由于各种问题没有在响应中设置content-type， * 导致FeignClient接收的时候contentType为null，HttpMessageConverterExtractor将其设置为MediaType.APPLICATION_OCTET_STREAM * 此时MessageConverter需要增加MediaType.APPLICATION_OCTET_STREAM支持 */ @Bean public Decoder feignDecoder() { MappingJackson2HttpMessageConverter hmc = new MappingJackson2HttpMessageConverter(customObjectMapper()); List&lt;MediaType&gt; unModifiedMediaTypeList = hmc.getSupportedMediaTypes(); List&lt;MediaType&gt; mediaTypeList = new ArrayList&lt;&gt;(unModifiedMediaTypeList.size() + 1); mediaTypeList.addAll(unModifiedMediaTypeList); mediaTypeList.add(MediaType.APPLICATION_OCTET_STREAM); hmc.setSupportedMediaTypes(mediaTypeList); ObjectFactory&lt;HttpMessageConverters&gt; objectFactory = () -&gt; new HttpMessageConverters(hmc); return new ResponseEntityDecoder(new SpringDecoder(objectFactory)); } @Bean public ObjectMapper customObjectMapper() { //解决LocalDate、LocalDateTime反序列化问题 ObjectMapper objectMapper = new ObjectMapper(); objectMapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS); objectMapper.registerModule(new JavaTimeModule()); return new ObjectMapper(); } } 启用feign客户端 @SpringBootApplication @EnableFeignClients public class DemoHttpApplication { public static void main(String[] args) { SpringApplication.run(DemoHttpApplication.class, args); } } 定义feign客户端 @FeignClient(name = \"test-service\", path = \"/\", url = \"http://localhost:8080\") public interface TestClient { @GetMapping(\"/welcome\") String welcome(@RequestParam String message); @PostMapping(\"/test\") String test(@RequestBody RequestObj param); } 测试调用 @RunWith(SpringRunner.class) @SpringBootTest public class FeignTest { @Autowired private TestClient testClient; @Test public void testGet() { System.out.println(\"result:\" + testClient.welcome(\"world\")); } @Test public void testPost() { RequestObj requestObj = RequestObj.builder().id(1) .age(20) .name(\"鹧鸪哨\").build(); System.out.println(\"result:\" + testClient.test(requestObj)); } }","categories":[{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"}],"tags":[{"name":"远程调用","slug":"远程调用","permalink":"https://13592491893.github.io/tags/%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"feign","slug":"feign","permalink":"https://13592491893.github.io/tags/feign/"},{"name":"dubbo","slug":"dubbo","permalink":"https://13592491893.github.io/tags/dubbo/"}]}],"categories":[{"name":"mysql","slug":"mysql","permalink":"https://13592491893.github.io/categories/mysql/"},{"name":"docker","slug":"docker","permalink":"https://13592491893.github.io/categories/docker/"},{"name":"java","slug":"java","permalink":"https://13592491893.github.io/categories/java/"},{"name":"linux","slug":"linux","permalink":"https://13592491893.github.io/categories/linux/"},{"name":"thread","slug":"java/thread","permalink":"https://13592491893.github.io/categories/java/thread/"},{"name":"git","slug":"git","permalink":"https://13592491893.github.io/categories/git/"},{"name":"hexo博客","slug":"hexo博客","permalink":"https://13592491893.github.io/categories/hexo%E5%8D%9A%E5%AE%A2/"},{"name":"Markdown","slug":"hexo博客/Markdown","permalink":"https://13592491893.github.io/categories/hexo%E5%8D%9A%E5%AE%A2/Markdown/"},{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/categories/sql/"},{"name":"nginx","slug":"nginx","permalink":"https://13592491893.github.io/categories/nginx/"},{"name":"work","slug":"work","permalink":"https://13592491893.github.io/categories/work/"},{"name":"stream","slug":"java/stream","permalink":"https://13592491893.github.io/categories/java/stream/"},{"name":"collection","slug":"java/collection","permalink":"https://13592491893.github.io/categories/java/collection/"},{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/categories/vue/"},{"name":"redis","slug":"redis","permalink":"https://13592491893.github.io/categories/redis/"}],"tags":[{"name":"MySQL优化","slug":"MySQL优化","permalink":"https://13592491893.github.io/tags/MySQL%E4%BC%98%E5%8C%96/"},{"name":"教程","slug":"教程","permalink":"https://13592491893.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"优化","slug":"优化","permalink":"https://13592491893.github.io/tags/%E4%BC%98%E5%8C%96/"},{"name":"linux","slug":"linux","permalink":"https://13592491893.github.io/tags/linux/"},{"name":"docker","slug":"docker","permalink":"https://13592491893.github.io/tags/docker/"},{"name":"容器","slug":"容器","permalink":"https://13592491893.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"utils","slug":"utils","permalink":"https://13592491893.github.io/tags/utils/"},{"name":"object","slug":"object","permalink":"https://13592491893.github.io/tags/object/"},{"name":"tool","slug":"tool","permalink":"https://13592491893.github.io/tags/tool/"},{"name":"命令","slug":"命令","permalink":"https://13592491893.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"尚硅谷","slug":"尚硅谷","permalink":"https://13592491893.github.io/tags/%E5%B0%9A%E7%A1%85%E8%B0%B7/"},{"name":"thread","slug":"thread","permalink":"https://13592491893.github.io/tags/thread/"},{"name":"git","slug":"git","permalink":"https://13592491893.github.io/tags/git/"},{"name":"hexo博客","slug":"hexo博客","permalink":"https://13592491893.github.io/tags/hexo%E5%8D%9A%E5%AE%A2/"},{"name":"Markdown","slug":"Markdown","permalink":"https://13592491893.github.io/tags/Markdown/"},{"name":"java","slug":"java","permalink":"https://13592491893.github.io/tags/java/"},{"name":"session","slug":"session","permalink":"https://13592491893.github.io/tags/session/"},{"name":"面试","slug":"面试","permalink":"https://13592491893.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"rpc","slug":"rpc","permalink":"https://13592491893.github.io/tags/rpc/"},{"name":"http","slug":"http","permalink":"https://13592491893.github.io/tags/http/"},{"name":"远程调用","slug":"远程调用","permalink":"https://13592491893.github.io/tags/%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"redis","slug":"redis","permalink":"https://13592491893.github.io/tags/redis/"},{"name":"nosql","slug":"nosql","permalink":"https://13592491893.github.io/tags/nosql/"},{"name":"Quartz","slug":"Quartz","permalink":"https://13592491893.github.io/tags/Quartz/"},{"name":"定时任务","slug":"定时任务","permalink":"https://13592491893.github.io/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"job","slug":"job","permalink":"https://13592491893.github.io/tags/job/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://13592491893.github.io/tags/zookeeper/"},{"name":"eureka","slug":"eureka","permalink":"https://13592491893.github.io/tags/eureka/"},{"name":"cap","slug":"cap","permalink":"https://13592491893.github.io/tags/cap/"},{"name":"nginx","slug":"nginx","permalink":"https://13592491893.github.io/tags/nginx/"},{"name":"指令","slug":"指令","permalink":"https://13592491893.github.io/tags/%E6%8C%87%E4%BB%A4/"},{"name":"sql","slug":"sql","permalink":"https://13592491893.github.io/tags/sql/"},{"name":"mysql","slug":"mysql","permalink":"https://13592491893.github.io/tags/mysql/"},{"name":"web","slug":"web","permalink":"https://13592491893.github.io/tags/web/"},{"name":"servlet","slug":"servlet","permalink":"https://13592491893.github.io/tags/servlet/"},{"name":"语法","slug":"语法","permalink":"https://13592491893.github.io/tags/%E8%AF%AD%E6%B3%95/"},{"name":"spring","slug":"spring","permalink":"https://13592491893.github.io/tags/spring/"},{"name":"日志","slug":"日志","permalink":"https://13592491893.github.io/tags/%E6%97%A5%E5%BF%97/"},{"name":"log","slug":"log","permalink":"https://13592491893.github.io/tags/log/"},{"name":"gitee","slug":"gitee","permalink":"https://13592491893.github.io/tags/gitee/"},{"name":"config","slug":"config","permalink":"https://13592491893.github.io/tags/config/"},{"name":"feign","slug":"feign","permalink":"https://13592491893.github.io/tags/feign/"},{"name":"Executor","slug":"Executor","permalink":"https://13592491893.github.io/tags/Executor/"},{"name":"多线程","slug":"多线程","permalink":"https://13592491893.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"线程","slug":"线程","permalink":"https://13592491893.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"ap,cp","slug":"ap-cp","permalink":"https://13592491893.github.io/tags/ap-cp/"},{"name":"stream","slug":"stream","permalink":"https://13592491893.github.io/tags/stream/"},{"name":"list","slug":"list","permalink":"https://13592491893.github.io/tags/list/"},{"name":"set","slug":"set","permalink":"https://13592491893.github.io/tags/set/"},{"name":"map","slug":"map","permalink":"https://13592491893.github.io/tags/map/"},{"name":"vue","slug":"vue","permalink":"https://13592491893.github.io/tags/vue/"},{"name":"前端","slug":"前端","permalink":"https://13592491893.github.io/tags/%E5%89%8D%E7%AB%AF/"},{"name":"工作","slug":"工作","permalink":"https://13592491893.github.io/tags/%E5%B7%A5%E4%BD%9C/"},{"name":"校验","slug":"校验","permalink":"https://13592491893.github.io/tags/%E6%A0%A1%E9%AA%8C/"},{"name":"rules","slug":"rules","permalink":"https://13592491893.github.io/tags/rules/"},{"name":"跨域","slug":"跨域","permalink":"https://13592491893.github.io/tags/%E8%B7%A8%E5%9F%9F/"},{"name":"ztree","slug":"ztree","permalink":"https://13592491893.github.io/tags/ztree/"},{"name":"模糊搜索","slug":"模糊搜索","permalink":"https://13592491893.github.io/tags/%E6%A8%A1%E7%B3%8A%E6%90%9C%E7%B4%A2/"},{"name":"dubbo","slug":"dubbo","permalink":"https://13592491893.github.io/tags/dubbo/"}]}